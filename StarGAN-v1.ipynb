{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from random import randint, shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "#### Keras APIs\n",
    "from keras.models import Sequential, Model,load_model\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.layers import Layer,InputLayer, Input,Reshape, Conv2D, Conv2DTranspose,Embedding, CuDNNGRU,Bidirectional,\\\n",
    "Dense, Flatten,BatchNormalization, Activation, ZeroPadding2D, LeakyReLU, UpSampling2D,MaxPooling2D,Dropout,Concatenate,\\\n",
    "Lambda\n",
    "from keras import layers\n",
    "### pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization, InputSpec\n",
    "from keras.optimizers import Adam,RMSprop,Adadelta,SGD\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self,img_res,dataset_path):\n",
    "        self.img_res = img_res\n",
    "        self.dataset_path = dataset_path\n",
    "        self.df_labels= pd.read_table('list_attr_celeba.txt',skiprows=1,delim_whitespace=True)\n",
    "        self.df_labels.replace(-1,0,inplace=True)\n",
    "        self.path = glob.glob('%s\\\\*' % (self.dataset_path))\n",
    "    \n",
    "    #cut the path E:\\machine_learning_image_data\\CelebA\\Img\\img_align_celeba\\000001.jpg to 000001.jpg\n",
    "    def cut(self,path_list):\n",
    "        return path_list[-10:]\n",
    "    \n",
    "    #load 1 batch of images not iterate\n",
    "    def load_data(self,batch_size=1):\n",
    "\n",
    "                \n",
    "        # names of the batch of images\n",
    "        batch_images = np.random.choice(self.path,size=batch_size)\n",
    "        img_batch_names =[self.cut(p) for p in batch_images]\n",
    "        \n",
    "        # get original labels from labels dataframe\n",
    "        origin_path_batch = np.array(img_batch_names)        \n",
    "        origin_label = self.df_labels.loc[origin_path_batch]\n",
    "        origin_label = origin_label.values          \n",
    "        \n",
    "        imgs=[]\n",
    "#         imgs_batch = np.empty(shape = [0,128,128,3])\n",
    "        for img_path in batch_images:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                # crop the 178x218 image to 178x178 then resize to 128x128. crop the middle part.                \n",
    "                img = img.crop((0,20,178,198))\n",
    "                img = img.resize(self.img_res,Image.BILINEAR)\n",
    "                # 50% chance to flip the image for data enhancement\n",
    "                if np.random.random()>0.5: \n",
    "                    img = np.fliplr(img)\n",
    "                # convert 0-255 to -1 - +1 domain   \n",
    "                img = (np.array(img)/255 -0.5) *2\n",
    "                imgs.append(img)\n",
    "#                 imgs_batch=np.append(imgs_batch,img,axis = 0) \n",
    "            except Exception as e:\n",
    "                pass \n",
    "           \n",
    "        \n",
    "\n",
    "        imgs = np.array(imgs)\n",
    "        imgs =imgs.astype(np.float)\n",
    "        return imgs,origin_label\n",
    "    \n",
    "    # load batch of images and iterate\n",
    "    def load_batch(self, batch_size = 16):\n",
    "        \n",
    "               \n",
    "        # the number of batch in 1 epoch\n",
    "        self.num_batches = int(len(self.path) / batch_size)\n",
    "        self.total_samples = self.num_batches * batch_size\n",
    "        #set replace = False in order to sample the sample again. \n",
    "        path_all_images = np.random.choice(self.path,self.total_samples,replace = False)\n",
    "        \n",
    "        for i in range(self.num_batches - 1):\n",
    "            #path_batch are the image files names. need this value to get correct label info\n",
    "            path_batch = path_all_images[i * batch_size: (i+1) * batch_size]\n",
    "            img_batch_names =[self.cut(p) for p in path_batch]\n",
    "            \n",
    "            imgs_batch =[]\n",
    "#             imgs_batch = np.empty(shape = [0,128,128,3])\n",
    "            \n",
    "            for img_path in path_batch:\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    # crop the 178x218 image to 178x178 then resize to 128x128. crop the middle part.\n",
    "                    img = img.crop((0,20,178,198))\n",
    "                    img = img.resize(self.img_res,Image.BILINEAR)\n",
    "                    # 50% chance to flip the image for data enhancement\n",
    "                    if np.random.random()>0.5: \n",
    "                        img = np.fliplr(img)\n",
    "                    # convert 0-255 to -1 - +1 domain   \n",
    "                    img = (np.array(img)/255 -0.5) *2\n",
    "#                     imgs_batch=np.append(imgs_batch,img,axis = 0)\n",
    "                    imgs_batch.append(img)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                                  \n",
    "            imgs_batch = np.array(imgs_batch)\n",
    "            imgs_batch = imgs_batch.astype(np.float)\n",
    "\n",
    "            \n",
    "            \n",
    "            # get original labels from labels dataframe\n",
    "            origin_path_batch = np.array(img_batch_names)\n",
    "            origin_label = self.df_labels.loc[img_batch_names]\n",
    "            origin_label = origin_label.values\n",
    "            \n",
    "            # get target labels by permuation of original label\n",
    "            target_path_batch = np.random.permutation(origin_path_batch)\n",
    "            target_label = self.df_labels.loc[target_path_batch]\n",
    "            target_label = target_label.values\n",
    "            \n",
    "            yield imgs_batch, origin_label,target_label                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function for WGAN-GP to calculate the randomweighted average of two images(image_real and image_fake)\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "#     def __init__(self, batch_size=16):\n",
    "#         self.batch_size = batch_size\n",
    "    \n",
    "    def _merge_function(self,inputs):\n",
    "        alpha = K.random_uniform((16, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StarGAN():\n",
    "    \n",
    "    def __init__(self,continue_training=True,linear_decay=False):\n",
    "        \n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.gen_f =64\n",
    "        self.disc_f =64\n",
    "        self.linear_decay = linear_decay\n",
    "        \n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**6 )\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "        \n",
    "        self.learning_rate_initial = 0.0001\n",
    "        self.learning_rate = self.learning_rate_initial\n",
    "        \n",
    "        optimizer_gen = Adam(self.learning_rate,0.5,0.999)\n",
    "        optimizer_disc = Adam(self.learning_rate,0.5,0.999)\n",
    "        \n",
    "        dataset_path='E:\\\\machine_learning_image_data\\\\CelebA\\\\Img\\\\img_align_celeba'\n",
    "        #number of domain\n",
    "        self.label_dim = 40 \n",
    "        \n",
    "        ### Build and Compile Discriminator model\n",
    "        \n",
    "        \n",
    "        self.D = self.build_discriminator()\n",
    "\n",
    "        \n",
    "        if continue_training:            \n",
    "            self.D.load_weights(\"models\\\\stargan_discriminator_weights-v1.h5\")\n",
    "            \n",
    "            \n",
    "        img_real = Input(shape = self.img_shape)\n",
    "        img_generated = Input(shape = self.img_shape)\n",
    "        \n",
    "        valid,valid_origin_label = self.D(img_real)\n",
    "        fake, _ = self.D(img_generated)\n",
    "        \n",
    "        # Construct weighted average between real and fake images\n",
    "        random_weighted_average = RandomWeightedAverage(batch_size=16)\n",
    "        img_interpolated = random_weighted_average([img_real,img_generated])\n",
    "\n",
    "        \n",
    "        \n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated, _ = self.D(img_interpolated)\n",
    "\n",
    "        \n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          averaged_samples=img_interpolated)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        #turn the weights for label 5 times for investigation\n",
    "        self.D_model = Model([img_real,img_generated], [valid,fake,validity_interpolated,valid_origin_label])\n",
    "        self.D_model.compile(loss=[self.wasserstein_loss,self.wasserstein_loss,partial_gp_loss,self.classification_loss],\\\n",
    "                                        loss_weights = [1,1,10,5],\\\n",
    "                                        optimizer = optimizer_disc)\n",
    "        \n",
    "        \n",
    "        ### Build and Compile Generator model\n",
    "        \n",
    "        self.G = self.build_generator()\n",
    "\n",
    "        if continue_training:            \n",
    "            self.G.load_weights(\"models\\\\stargan_generator_weights-v1.h5\")\n",
    "            \n",
    "        origin_label = Input(shape=(self.label_dim,))\n",
    "        target_label = Input(shape=(self.label_dim,))\n",
    "        \n",
    "        img_generated_G = self.G([img_real,target_label])\n",
    "        img_reconstruct = self.G([img_generated_G,origin_label])\n",
    "        \n",
    "        valid_G,valid_target_label = self.D(img_generated_G)\n",
    "\n",
    "        self.D.trainable = False   \n",
    "        self.G.trainable = True\n",
    "        \n",
    "\n",
    "        \n",
    "        self.G_model = Model([img_real,target_label,origin_label], [valid_G,valid_target_label,img_reconstruct])\n",
    "        self.G_model.compile(loss = [self.wasserstein_loss,self.classification_loss,'mae'],\\\n",
    "                                       loss_weights =[1,5,10],optimizer = optimizer_gen)\n",
    "        \n",
    "        \n",
    "        self.data_loader = DataLoader(img_res=(self.img_rows,self.img_cols), dataset_path = dataset_path)\n",
    "\n",
    "    \n",
    "    def classification_loss(self,y,y_pred):\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=y_pred))\n",
    "        \n",
    "    def wasserstein_loss(self,y,y_pred):\n",
    "            return K.mean(y*y_pred)  \n",
    "        \n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "        \n",
    "    def build_generator(self):\n",
    "        \n",
    "        def depth_wise_concatenate(input_img, input_label):\n",
    "            # Replicate spatially and concatenate domain information\n",
    "            label = Lambda(lambda x: K.repeat(x, self.img_rows**2))(input_label)\n",
    "            label = Reshape((self.img_rows, self.img_cols, self.label_dim))(label)\n",
    "            x = Concatenate()([input_img, label])  \n",
    "            return x \n",
    "\n",
    "        \n",
    "        def conv2d(layer_input, filters, f_size=4,strides=2,padding = 'valid'):\n",
    "            \n",
    "            init = RandomNormal(stddev=0.02)\n",
    "            d = ZeroPadding2D((1,1))(layer_input)\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=strides, padding=padding,kernel_initializer=init)(d)            \n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = Activation('relu')(d)\n",
    "            return d\n",
    "        \n",
    "        def residual(layer_input,filters):\n",
    "            init = RandomNormal(stddev=0.02)\n",
    "            # first layer\n",
    "\n",
    "            x = ZeroPadding2D((1,1))(layer_input)\n",
    "            x = Conv2D(filters=filters, kernel_size=3, strides=1, padding='valid',kernel_initializer=init)(x)\n",
    "            x = InstanceNormalization(axis=-1)(x)\n",
    "            x = Activation('relu')(x)\n",
    "            # second layer\n",
    "\n",
    "            x = ZeroPadding2D((1,1))(x)\n",
    "            x = Conv2D(filters=filters, kernel_size=3, strides=1, padding='valid',kernel_initializer=init)(x)\n",
    "            x = InstanceNormalization(axis=-1)(x)\n",
    "            # merge\n",
    "            x = layers.add([x, layer_input])\n",
    "            return x\n",
    "\n",
    "        def deconv2d(layer_input, filters, f_size=4,padding = 'same'):\n",
    "            init = RandomNormal(stddev=0.02)\n",
    "\n",
    "            u = Conv2DTranspose(filters, kernel_size=f_size, strides=2, padding=padding,kernel_initializer=init)(layer_input)\n",
    "            u = InstanceNormalization(axis=-1)(u)\n",
    "            u = Activation('relu')(u)               \n",
    "            return u\n",
    "        \n",
    "        input_img = Input(shape=self.img_shape)\n",
    "        input_label = Input(shape=(self.label_dim,))\n",
    "        \n",
    "        model = depth_wise_concatenate(input_img, input_label)        \n",
    "\n",
    "        model = ZeroPadding2D((3,3))(model)\n",
    "        model = conv2d(model,self.gen_f,f_size=7,strides=1)\n",
    "        model = conv2d(model,self.gen_f*2)\n",
    "        model = conv2d(model,self.gen_f*4)\n",
    "        \n",
    "        #6 residual blocks\n",
    "        for _ in range(6):\n",
    "            model = residual(model,self.gen_f*4)\n",
    "        \n",
    "        model = deconv2d(model,self.gen_f*2)\n",
    "        model = deconv2d(model,self.gen_f)     \n",
    "\n",
    "        model = ZeroPadding2D((3,3))(model)\n",
    "        model = Conv2D(filters = 3,kernel_size=7,strides=1,padding='valid')(model)\n",
    "        \n",
    "        model =InstanceNormalization(axis=-1)(model)\n",
    "        \n",
    "        output_img = Activation('tanh')(model)\n",
    "        output_model = Model(inputs=[input_img,input_label],outputs=output_img)\n",
    "        output_model.summary()\n",
    "        \n",
    "        return output_model      \n",
    "        \n",
    "    def build_discriminator(self):\n",
    "        \n",
    "        def disc_layer(layer_input, filters, f_size=4, strides=2,normalization=False):\n",
    "            \n",
    "            init = RandomNormal(stddev=0.02)\n",
    "\n",
    "            d_layer = ZeroPadding2D((1,1))(layer_input)\n",
    "            d_layer = Conv2D(filters, kernel_size=f_size, strides=strides, padding='valid',kernel_initializer=init)(d_layer)\n",
    "            if normalization:                \n",
    "                d_layer = InstanceNormalization(axis=-1)(d_layer)\n",
    "            d_layer = LeakyReLU(alpha=0.01)(d_layer)\n",
    "            return d_layer\n",
    "        \n",
    "        input_img = Input(shape=self.img_shape)\n",
    "        \n",
    "        model = disc_layer(input_img, self.disc_f, normalization=False)\n",
    "        \n",
    "        model = disc_layer(model, self.disc_f*2)\n",
    "        model = disc_layer(model, self.disc_f*4)\n",
    "        model = disc_layer(model, self.disc_f*8)\n",
    "        model = disc_layer(model, self.disc_f*16)\n",
    "        model = disc_layer(model, self.disc_f*32)        \n",
    "        \n",
    "        \n",
    "\n",
    "        output_d_src = ZeroPadding2D((1,1))(model)\n",
    "        # D_src (2,2,1)\n",
    "        output_d_src = Conv2D(1, kernel_size=3, strides=1, padding='valid')(output_d_src)\n",
    "        \n",
    "        # D_cls (1,1,self.label_dim) -> (self.label_dim,)\n",
    "        model = Conv2D(filters = self.label_dim,kernel_size = int(self.img_rows/64), strides = 1,padding = 'valid')(model)      \n",
    "\n",
    "        \n",
    "        output_d_cls = Reshape((self.label_dim,))(model)  \n",
    "\n",
    "        output_model = Model(input_img,[output_d_src,output_d_cls])\n",
    "        output_model.summary()\n",
    "        \n",
    "        return output_model\n",
    "    \n",
    "    def train(self,epochs,batch_size=16,sample_interval=500,resume_epoch=0,resume_batch=0):\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        # Adversarial loss ground truths # (-1,2,2,1)\n",
    "        valid = - np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.ones((batch_size,) + self.disc_patch)\n",
    "        dummy = np.zeros((batch_size,) + self.disc_patch) # for gradient panalty of WGAN-GP\n",
    "        \n",
    "        \n",
    "        for epoch in range(resume_epoch,epochs):\n",
    "\n",
    "            # linear decay of learning rate for the last 100 epoch\n",
    "            if self.linear_decay ==True:\n",
    "                self.learning_rate = self.lr_linear_decay(epoch,epochs) \n",
    "            \n",
    "            for batch_i, (imgs, origin_label, target_label) in enumerate(self.data_loader.load_batch(batch_size),start=resume_batch):\n",
    "                \n",
    "                # in case of resume from resume_batch,the stop point needs to be set, otherwise it iterates over the max_batch_num.\n",
    "                if batch_i> self.data_loader.num_batches:   \n",
    "                    #reset the resume_batch parameter so that for the next epoch the iteration will start from 0\n",
    "                    resume_batch = 0\n",
    "                    break\n",
    "\n",
    "                                                           \n",
    "                # ----------------------\n",
    "                #  Train Discriminator\n",
    "                # ----------------------\n",
    "                \n",
    "                \n",
    "                imgs_generated = self.G.predict([imgs,target_label])\n",
    "                \n",
    "                d_loss = self.D_model.train_on_batch(x = [imgs,imgs_generated], y = [valid,fake,dummy,origin_label])\n",
    "                \n",
    "                \n",
    "                # ----------------------\n",
    "                #  Train Generator\n",
    "                # ----------------------                \n",
    "                \n",
    "                \n",
    "                imgs_reconstruct =self.G.predict([imgs_generated,origin_label])\n",
    "                g_loss = self.G_model.train_on_batch(x = [imgs,target_label,origin_label], y=[valid,target_label,imgs])\n",
    "                \n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                \n",
    "                print (\"[epoch %d/%d], [batch %d/%d], [Dloss: %f] [G loss: %05f] time: %s\"\\\n",
    "\n",
    "                       % (epoch, epochs, batch_i, self.data_loader.num_batches,d_loss[0], g_loss[0], elapsed_time))\n",
    "                print (\"current learning rate:%05f\" %self.learning_rate)\n",
    "                \n",
    "                if batch_i % sample_interval == 0:\n",
    "                    \n",
    "                    \n",
    "#                     print(f\"D_model metrics:{self.D_model.metrics_names}\")\n",
    "#                     print(f\"G_model metrics:{self.G_model.metrics_names}\")\n",
    "                    print (\"[epoch %d/%d], [batch %d/%d], [Dloss: %f] [G loss: %05f] time: %s\"\\\n",
    "\n",
    "                           % (epoch, epochs, batch_i, self.data_loader.num_batches,d_loss[0], g_loss[0], elapsed_time))\n",
    "                    print (\"current learning rate:%05f\" %self.learning_rate)\n",
    "                    \n",
    "                    self.sample_images(epoch, batch_i)\n",
    "#                     self.image_debug(imgs,origin_label,target_label)\n",
    "                    \n",
    "                    \n",
    "            # save the generator model each interval of 2000\n",
    "\n",
    "                if batch_i % 2000 == 0:\n",
    "\n",
    "                    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "                    #save models weights\n",
    "                    self.D.save_weights('models\\\\stargan_discriminator_weights-v1.h5')\n",
    "                    self.G.save_weights('models\\\\stargan_generator_weights-v1.h5')\n",
    "                    \n",
    "                    #save models\n",
    "#                     self.D.save('models\\\\stargan_discriminator-v1_CelebA_40dims.h5')\n",
    "#                     self.G.save('models\\\\stargan_generator-v1_CelebA_40dims.h5')              \n",
    "    \n",
    "    def sample_images(self,epoch,batch_i):\n",
    "        \n",
    "        os.makedirs('images/',exist_ok = True)\n",
    "        r, c = 5,6\n",
    "    \n",
    "#         target_label = np.zeros((5,40))\n",
    "        \n",
    "#         # 9: Blond_Hair;15:Eyeglass; 22: Mustache; 31:smiling; 39:Young;\n",
    "        \n",
    "\n",
    "#         # it seems if only one feature is enabled and others are unabled, the machine doesn't understand.\n",
    "#         # need further investication.\n",
    "\n",
    "#         str_0 = \"0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\"\n",
    "# #         str_0 = \"0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1\"\n",
    "#         target_label[0] = np.array(str_0.split())\n",
    "\n",
    "#         str_1 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\"\n",
    "# #         str_1 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0\"\n",
    "#         target_label[1] = np.array(str_1.split())\n",
    "\n",
    "#         str_2 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\" #mastache with male \n",
    "# #         str_2 = \"1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\"\n",
    "#         target_label[2] = np.array(str_2.split())\n",
    "        \n",
    "#         str_3 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\" \n",
    "# #         str_3 = \"0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1\"\n",
    "#         target_label[3] = np.array(str_3.split())\n",
    "\n",
    "#         str_4 = \"0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\" #young with charming\n",
    "# #         str_4 = \"0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\"\n",
    "#         target_label[4] = np.array(str_4.split())\n",
    "        \n",
    "\n",
    "                \n",
    "        #1st photo is original photo\n",
    "        \n",
    "        # 5 x 6 photos, first column is origin photo, other columns are generated photos\n",
    "\n",
    "        imgs_generated_illustration = np.empty(shape=[0,128,128,3])\n",
    "\n",
    "        #5 iterations to get 5 set of photos. imgs_generated_illustration_all is (30,128,128,3)\n",
    "        for k in range(5):\n",
    "            \n",
    "            \n",
    "            # load 1 images\n",
    "            imgs,imgs_origin_label = self.data_loader.load_data(batch_size=1)\n",
    "            \n",
    "            # repeat the origin_label x 5, (40,) -> (5,40)\n",
    "            imgs_origin_label = np.repeat(imgs_origin_label,5,axis=0)\n",
    "            \n",
    "            #try to tune 1 parameter of the original label and make it as target label\n",
    "            target_label = imgs_origin_label\n",
    "            \n",
    "            target_label[0,9] = 1 \n",
    "            target_label[0,8] = 0 #blond without black\n",
    "            target_label[0,10] = 0 #blond without brawn\n",
    "            target_label[0,17] = 0 #blond without gray\n",
    "            target_label[1,15] = 1\n",
    "            target_label[2,22] = 1  \n",
    "            target_label[2,20] = 1 #mastache with male\n",
    "            target_label[2,24] = 0 #mastache without no-beard\n",
    "            target_label[3,31] = 1\n",
    "            target_label[4,39] = 1\n",
    "            target_label[4,2] = 1 #young with attractive\n",
    "            target_label[4,3] = 0 #young without eyebag\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            # 1 image generate 5 different faces by 5 target labels   \n",
    "            imgs_gen = self.G.predict([np.repeat(imgs,5,axis=0),target_label])\n",
    "            # 6 images\n",
    "            imgs_concat = np.concatenate([imgs,imgs_gen])\n",
    "            # 6 images x 5\n",
    "            imgs_generated_illustration =np.append(imgs_generated_illustration,imgs_concat,axis=0)\n",
    "                    \n",
    "        \n",
    "        # Rescale images 0 - 1\n",
    "        imgs_generated_illustration = 0.5 * imgs_generated_illustration + 0.5\n",
    "        \n",
    "        label_title = ['Input','Blond_Hair','Eyeglasses','Mustache','Smiling','Young']\n",
    "        \n",
    "        fig, axs = plt.subplots(r,c)\n",
    "        # 20 x 15 is about 1440 x 1080 pixels\n",
    "        fig.set_size_inches(20, 15, forward=True)\n",
    "        \n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(imgs_generated_illustration[cnt,:,:,:])\n",
    "                if i == 0:                    \n",
    "                    axs[i,j].set_title(label_title[j],fontsize = 25)\n",
    "                    \n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1 \n",
    "        \n",
    "        fig.savefig(f\"images/Sample_Epoch_No_{epoch}_Batch_No_{batch_i}.png\")\n",
    "        plt.close()        \n",
    "    \n",
    "    \n",
    "    def image_debug(self,imgs,origin_label,target_label):\n",
    "        \n",
    "        generated_imgs = self.G.predict([imgs,target_label])\n",
    "        imgs = 0.5 * imgs + 0.5\n",
    "        generated_imgs = 0.5 * generated_imgs + 0.5\n",
    "        \n",
    "        plt.imshow(imgs[0])\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        plt.imshow(generated_imgs[0])\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "#         print('origin_label: ',origin_label[0])\n",
    "#         print('target_label: ',target_label[0])\n",
    "\n",
    "    # Generate image by personal image\n",
    "    def generate_image(self):\n",
    "        \n",
    "        \n",
    "        print('sample generation started...')\n",
    "        imgs_folder = 'samples'\n",
    "        #make dir named imgs_folder\n",
    "        os.makedirs(imgs_folder, exist_ok=True)\n",
    "        # read images into np array\n",
    "        imgs_names = self.load_data('{}/*.jpeg'.format\\\n",
    "                                     (imgs_folder))\n",
    "        imgs = [self.read_image(imgs_names[j]) for j in range(len(imgs_names))]\n",
    "        imgs = np.array(imgs)\n",
    "\n",
    "        r, c = 5,6\n",
    "    \n",
    "        target_label = np.zeros((5,40))\n",
    "        \n",
    "        str_0 = \"0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\"\n",
    "#         str_0 = \"0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1\"\n",
    "        target_label[0] = np.array(str_0.split())\n",
    "\n",
    "        str_1 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\"\n",
    "#         str_1 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0\"\n",
    "        target_label[1] = np.array(str_1.split())\n",
    "\n",
    "        str_2 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\" #mastache with male \n",
    "#         str_2 = \"1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\"\n",
    "        target_label[2] = np.array(str_2.split())\n",
    "        \n",
    "        str_3 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\" \n",
    "#         str_3 = \"0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1\"\n",
    "        target_label[3] = np.array(str_3.split())\n",
    "\n",
    "        str_4 = \"0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\" #young with charming\n",
    "#         str_4 = \"0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\"\n",
    "        target_label[4] = np.array(str_4.split())        \n",
    "\n",
    "        # 5 x 6 photos, first column is origin photo, other columns are generated photos\n",
    "\n",
    "        imgs_generated_illustration = np.empty(shape=[0,128,128,3])\n",
    "                        \n",
    "        for k in range(5):\n",
    "            \n",
    "            \n",
    "            imgs_gen =  self.G.predict([np.repeat(imgs[k],5,axis=0),target_label])\n",
    "            \n",
    "                        \n",
    "            # 6 images\n",
    "            imgs_concat = np.concatenate([imgs[k],imgs_gen])\n",
    "            # 6 images x 5\n",
    "            imgs_generated_illustration =np.append(imgs_generated_illustration,imgs_concat,axis=0)\n",
    "\n",
    "            \n",
    "         # Rescale images 0 - 1\n",
    "        imgs_generated_illustration = 0.5 * imgs_generated_illustration + 0.5\n",
    "        imgs_generated_illustration = np.clip(imgs_generated_illustration, 0.0, 1.0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        label_title = ['Input','Blond_Hair','Eyeglasses','Mustache','Smiling','Young']\n",
    "        \n",
    "        fig, axs = plt.subplots(r,c)\n",
    "        # 20 x 15 is about 1440 x 1080 pixels\n",
    "        fig.set_size_inches(20, 15, forward=True)\n",
    "        \n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(imgs_generated_illustration[cnt,:,:,:])\n",
    "                if i == 0:                    \n",
    "                    axs[i,j].set_title(label_title[j],fontsize = 25)\n",
    "                    \n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1 \n",
    "\n",
    "        fig.savefig(f\"samples/generated_image.png\")\n",
    "        plt.close()    \n",
    "        print('...sample generation finished')        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #calculate the lr based on the current epoch number. It's near linear but not pure linear\n",
    "    def lr_linear_decay(self,epoch,epochs):\n",
    "        \n",
    "        lr = self.learning_rate_initial\n",
    "        \n",
    "        remaining_epoch = epochs - epoch\n",
    "#         remaining_batch = batches - batch_i\n",
    "\n",
    "        lr = self.learning_rate_initial * (remaining_epoch /epochs ) \n",
    "        \n",
    "        return lr\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ =='__main__':    \n",
    "    #set continue_training to True to load the weights,otherwise it will trains from scratch\n",
    "    stargan = StarGAN(continue_training=True,linear_decay=True)\n",
    "    #resume_epoch & resume_batch: set in case of resume of the training stopped\n",
    "    stargan.train(epochs=11,resume_epoch=6,resume_batch=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4,10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - 40 labels\n",
    "\n",
    "# 5_o_Clock_Shadow  Arched_Eyebrows Attractive      Bags_Under_Eyes    Bald\n",
    "# Bangs             Big_Lips        Big_Nose        Black_Hair         Blond_Hair\n",
    "# Blurry            Brown_Hair      Bushy_Eyebrows  Chubby             Double_Chin\n",
    "# Eyeglasses        Goatee          Gray_Hair       Heavy_Makeup       High_Cheekbones\n",
    "# Male              Mouth_Slightly_Open Mustache    Narrow_Eyes        No_Beard \n",
    "# Oval_Face         Pale_Skin       Pointy_Nose     Receding_Hairline  Rosy_Cheeks\n",
    "# Sideburns         Smiling         Straight_Hair   Wavy_Hair          Wearing_Earrings\n",
    "# Wearing_Hat       Wearing_Lipstick Wearing_Necklace Wearing_Necktie Young "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_labels.columns.get_loc('Young')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1] blond\n",
    "# [0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1] blond\n",
    "\n",
    "# [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0] eyeglass\n",
    "\n",
    "# [1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0] mustache\n",
    "\n",
    "#[0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1] smile\n",
    "\n",
    "# [0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1] young"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blondy: 476  [0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1]\n",
    "#glass: 676   [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0]\n",
    "#mustache:689 [1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1]\n",
    "#smile:718    [0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1]\n",
    "# young :459  [0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
