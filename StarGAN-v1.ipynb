{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from random import randint, shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "#### Keras APIs\n",
    "from keras.models import Sequential, Model,load_model\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.layers import Layer,InputLayer, Input,Reshape, Conv2D, Conv2DTranspose,Embedding, CuDNNGRU,Bidirectional,\\\n",
    "Dense, Flatten,BatchNormalization, Activation, ZeroPadding2D, LeakyReLU, UpSampling2D,MaxPooling2D,Dropout,Concatenate,\\\n",
    "Lambda\n",
    "from keras import layers\n",
    "### pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization, InputSpec\n",
    "from keras.optimizers import Adam,RMSprop,Adadelta,SGD\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self,img_res,dataset_path):\n",
    "        self.img_res = img_res\n",
    "        self.dataset_path = dataset_path\n",
    "        self.df_labels= pd.read_table('list_attr_celeba.txt',skiprows=1,delim_whitespace=True)\n",
    "        self.df_labels.replace(-1,0,inplace=True)\n",
    "        self.path = glob.glob('%s\\\\*' % (self.dataset_path))\n",
    "    \n",
    "    #cut the path E:\\machine_learning_image_data\\CelebA\\Img\\img_align_celeba\\000001.jpg to 000001.jpg\n",
    "    def cut(self,path_list):\n",
    "        return path_list[-10:]\n",
    "    \n",
    "    #load 1 batch of images not iterate\n",
    "    def load_data(self,batch_size=1):\n",
    "\n",
    "                \n",
    "        # names of the batch of images\n",
    "        batch_images = np.random.choice(self.path,size=batch_size)\n",
    "        img_batch_names =[self.cut(p) for p in batch_images]\n",
    "        \n",
    "        # get original labels from labels dataframe\n",
    "        origin_path_batch = np.array(img_batch_names)        \n",
    "        origin_label = self.df_labels.loc[origin_path_batch]\n",
    "        origin_label = origin_label.values          \n",
    "        \n",
    "        imgs=[]\n",
    "#         imgs_batch = np.empty(shape = [0,128,128,3])\n",
    "        for img_path in batch_images:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                # crop the 178x218 image to 178x178 then resize to 128x128. crop the middle part.                \n",
    "                img = img.crop((0,20,178,198))\n",
    "                img = img.resize(self.img_res,Image.BILINEAR)\n",
    "                # 50% chance to flip the image for data enhancement\n",
    "                if np.random.random()>0.5: \n",
    "                    img = np.fliplr(img)\n",
    "                # convert 0-255 to -1 - +1 domain   \n",
    "                img = (np.array(img)/255 -0.5) *2\n",
    "                imgs.append(img)\n",
    "#                 imgs_batch=np.append(imgs_batch,img,axis = 0) \n",
    "            except Exception as e:\n",
    "                pass \n",
    "           \n",
    "        \n",
    "\n",
    "        imgs = np.array(imgs)\n",
    "        imgs =imgs.astype(np.float)\n",
    "        return imgs,origin_label\n",
    "    \n",
    "    # load batch of images and iterate\n",
    "    def load_batch(self, batch_size = 16):\n",
    "        \n",
    "               \n",
    "        # the number of batch in 1 epoch\n",
    "        self.num_batches = int(len(self.path) / batch_size)\n",
    "        self.total_samples = self.num_batches * batch_size\n",
    "        #set replace = False in order to sample the sample again. \n",
    "        path_all_images = np.random.choice(self.path,self.total_samples,replace = False)\n",
    "        \n",
    "        for i in range(self.num_batches - 1):\n",
    "            #path_batch are the image files names. need this value to get correct label info\n",
    "            path_batch = path_all_images[i * batch_size: (i+1) * batch_size]\n",
    "            img_batch_names =[self.cut(p) for p in path_batch]\n",
    "            \n",
    "            imgs_batch =[]\n",
    "#             imgs_batch = np.empty(shape = [0,128,128,3])\n",
    "            \n",
    "            for img_path in path_batch:\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    # crop the 178x218 image to 178x178 then resize to 128x128. crop the middle part.\n",
    "                    img = img.crop((0,20,178,198))\n",
    "                    img = img.resize(self.img_res,Image.BILINEAR)\n",
    "                    # 50% chance to flip the image for data enhancement\n",
    "                    if np.random.random()>0.5: \n",
    "                        img = np.fliplr(img)\n",
    "                    # convert 0-255 to -1 - +1 domain   \n",
    "                    img = (np.array(img)/255 -0.5) *2\n",
    "#                     imgs_batch=np.append(imgs_batch,img,axis = 0)\n",
    "                    imgs_batch.append(img)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                                  \n",
    "            imgs_batch = np.array(imgs_batch)\n",
    "            imgs_batch = imgs_batch.astype(np.float)\n",
    "\n",
    "            \n",
    "            \n",
    "            # get original labels from labels dataframe\n",
    "            origin_path_batch = np.array(img_batch_names)\n",
    "            origin_label = self.df_labels.loc[img_batch_names]\n",
    "            origin_label = origin_label.values\n",
    "            \n",
    "            # get target labels by permuation of original label\n",
    "            target_path_batch = np.random.permutation(origin_path_batch)\n",
    "            target_label = self.df_labels.loc[target_path_batch]\n",
    "            target_label = target_label.values\n",
    "            \n",
    "            yield imgs_batch, origin_label,target_label                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function for WGAN-GP to calculate the randomweighted average of two images(image_real and image_fake)\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "#     def __init__(self, batch_size=16):\n",
    "#         self.batch_size = batch_size\n",
    "    \n",
    "    def _merge_function(self,inputs):\n",
    "        alpha = K.random_uniform((16, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StarGAN():\n",
    "    \n",
    "    def __init__(self,continue_training=True,linear_decay=False):\n",
    "        \n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.gen_f =64\n",
    "        self.disc_f =64\n",
    "        self.linear_decay = linear_decay\n",
    "        \n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**6 )\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "        \n",
    "        self.learning_rate_initial = 0.0001\n",
    "        self.learning_rate = self.learning_rate_initial\n",
    "        \n",
    "        optimizer_gen = Adam(self.learning_rate,0.5,0.999)\n",
    "        optimizer_disc = Adam(self.learning_rate,0.5,0.999)\n",
    "        \n",
    "        dataset_path='E:\\\\machine_learning_image_data\\\\CelebA\\\\Img\\\\img_align_celeba'\n",
    "        #number of domain\n",
    "        self.label_dim = 40 \n",
    "        \n",
    "        ### Build and Compile Discriminator model\n",
    "        \n",
    "        \n",
    "        self.D = self.build_discriminator()\n",
    "\n",
    "        \n",
    "        if continue_training:            \n",
    "            self.D.load_weights(\"models\\\\stargan_discriminator_weights-v1.h5\")\n",
    "            \n",
    "            \n",
    "        img_real = Input(shape = self.img_shape)\n",
    "        img_generated = Input(shape = self.img_shape)\n",
    "        \n",
    "        valid,valid_origin_label = self.D(img_real)\n",
    "        fake, _ = self.D(img_generated)\n",
    "        \n",
    "        # Construct weighted average between real and fake images\n",
    "        random_weighted_average = RandomWeightedAverage(batch_size=16)\n",
    "        img_interpolated = random_weighted_average([img_real,img_generated])\n",
    "\n",
    "        \n",
    "        \n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated, _ = self.D(img_interpolated)\n",
    "\n",
    "        \n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          averaged_samples=img_interpolated)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        #turn the weights for label 5 times for investigation\n",
    "        self.D_model = Model([img_real,img_generated], [valid,fake,validity_interpolated,valid_origin_label])\n",
    "        self.D_model.compile(loss=[self.wasserstein_loss,self.wasserstein_loss,partial_gp_loss,self.classification_loss],\\\n",
    "                                        loss_weights = [1,1,10,5],\\\n",
    "                                        optimizer = optimizer_disc)\n",
    "        \n",
    "        \n",
    "        ### Build and Compile Generator model\n",
    "        \n",
    "        self.G = self.build_generator()\n",
    "\n",
    "        if continue_training:            \n",
    "            self.G.load_weights(\"models\\\\stargan_generator_weights-v1.h5\")\n",
    "            \n",
    "        origin_label = Input(shape=(self.label_dim,))\n",
    "        target_label = Input(shape=(self.label_dim,))\n",
    "        \n",
    "        img_generated_G = self.G([img_real,target_label])\n",
    "        img_reconstruct = self.G([img_generated_G,origin_label])\n",
    "        \n",
    "        valid_G,valid_target_label = self.D(img_generated_G)\n",
    "\n",
    "        self.D.trainable = False   \n",
    "        self.G.trainable = True\n",
    "        \n",
    "\n",
    "        \n",
    "        self.G_model = Model([img_real,target_label,origin_label], [valid_G,valid_target_label,img_reconstruct])\n",
    "        self.G_model.compile(loss = [self.wasserstein_loss,self.classification_loss,'mae'],\\\n",
    "                                       loss_weights =[1,5,10],optimizer = optimizer_gen)\n",
    "        \n",
    "        \n",
    "        self.data_loader = DataLoader(img_res=(self.img_rows,self.img_cols), dataset_path = dataset_path)\n",
    "\n",
    "    \n",
    "    def classification_loss(self,y,y_pred):\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=y_pred))\n",
    "        \n",
    "    def wasserstein_loss(self,y,y_pred):\n",
    "            return K.mean(y*y_pred)  \n",
    "        \n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "        \n",
    "    def build_generator(self):\n",
    "        \n",
    "        def depth_wise_concatenate(input_img, input_label):\n",
    "            # Replicate spatially and concatenate domain information\n",
    "            label = Lambda(lambda x: K.repeat(x, self.img_rows**2))(input_label)\n",
    "            label = Reshape((self.img_rows, self.img_cols, self.label_dim))(label)\n",
    "            x = Concatenate()([input_img, label])  \n",
    "            return x \n",
    "\n",
    "        \n",
    "        def conv2d(layer_input, filters, f_size=4,strides=2,padding = 'valid'):\n",
    "            \n",
    "            init = RandomNormal(stddev=0.02)\n",
    "            d = ZeroPadding2D((1,1))(layer_input)\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=strides, padding=padding,kernel_initializer=init)(d)            \n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = Activation('relu')(d)\n",
    "            return d\n",
    "        \n",
    "        def residual(layer_input,filters):\n",
    "            init = RandomNormal(stddev=0.02)\n",
    "            # first layer\n",
    "\n",
    "            x = ZeroPadding2D((1,1))(layer_input)\n",
    "            x = Conv2D(filters=filters, kernel_size=3, strides=1, padding='valid',kernel_initializer=init)(x)\n",
    "            x = InstanceNormalization(axis=-1)(x)\n",
    "            x = Activation('relu')(x)\n",
    "            # second layer\n",
    "\n",
    "            x = ZeroPadding2D((1,1))(x)\n",
    "            x = Conv2D(filters=filters, kernel_size=3, strides=1, padding='valid',kernel_initializer=init)(x)\n",
    "            x = InstanceNormalization(axis=-1)(x)\n",
    "            # merge\n",
    "            x = layers.add([x, layer_input])\n",
    "            return x\n",
    "\n",
    "        def deconv2d(layer_input, filters, f_size=4,padding = 'same'):\n",
    "            init = RandomNormal(stddev=0.02)\n",
    "\n",
    "            u = Conv2DTranspose(filters, kernel_size=f_size, strides=2, padding=padding,kernel_initializer=init)(layer_input)\n",
    "            u = InstanceNormalization(axis=-1)(u)\n",
    "            u = Activation('relu')(u)               \n",
    "            return u\n",
    "        \n",
    "        input_img = Input(shape=self.img_shape)\n",
    "        input_label = Input(shape=(self.label_dim,))\n",
    "        \n",
    "        model = depth_wise_concatenate(input_img, input_label)        \n",
    "\n",
    "        model = ZeroPadding2D((3,3))(model)\n",
    "        model = conv2d(model,self.gen_f,f_size=7,strides=1)\n",
    "        model = conv2d(model,self.gen_f*2)\n",
    "        model = conv2d(model,self.gen_f*4)\n",
    "        \n",
    "        #6 residual blocks\n",
    "        for _ in range(6):\n",
    "            model = residual(model,self.gen_f*4)\n",
    "        \n",
    "        model = deconv2d(model,self.gen_f*2)\n",
    "        model = deconv2d(model,self.gen_f)     \n",
    "\n",
    "        model = ZeroPadding2D((3,3))(model)\n",
    "        model = Conv2D(filters = 3,kernel_size=7,strides=1,padding='valid')(model)\n",
    "        \n",
    "        model =InstanceNormalization(axis=-1)(model)\n",
    "        \n",
    "        output_img = Activation('tanh')(model)\n",
    "        output_model = Model(inputs=[input_img,input_label],outputs=output_img)\n",
    "        output_model.summary()\n",
    "        \n",
    "        return output_model      \n",
    "        \n",
    "    def build_discriminator(self):\n",
    "        \n",
    "        def disc_layer(layer_input, filters, f_size=4, strides=2,normalization=False):\n",
    "            \n",
    "            init = RandomNormal(stddev=0.02)\n",
    "\n",
    "            d_layer = ZeroPadding2D((1,1))(layer_input)\n",
    "            d_layer = Conv2D(filters, kernel_size=f_size, strides=strides, padding='valid',kernel_initializer=init)(d_layer)\n",
    "            if normalization:                \n",
    "                d_layer = InstanceNormalization(axis=-1)(d_layer)\n",
    "            d_layer = LeakyReLU(alpha=0.01)(d_layer)\n",
    "            return d_layer\n",
    "        \n",
    "        input_img = Input(shape=self.img_shape)\n",
    "        \n",
    "        model = disc_layer(input_img, self.disc_f, normalization=False)\n",
    "        \n",
    "        model = disc_layer(model, self.disc_f*2)\n",
    "        model = disc_layer(model, self.disc_f*4)\n",
    "        model = disc_layer(model, self.disc_f*8)\n",
    "        model = disc_layer(model, self.disc_f*16)\n",
    "        model = disc_layer(model, self.disc_f*32)        \n",
    "        \n",
    "        \n",
    "\n",
    "        output_d_src = ZeroPadding2D((1,1))(model)\n",
    "        # D_src (2,2,1)\n",
    "        output_d_src = Conv2D(1, kernel_size=3, strides=1, padding='valid')(output_d_src)\n",
    "        \n",
    "        # D_cls (1,1,self.label_dim) -> (self.label_dim,)\n",
    "        model = Conv2D(filters = self.label_dim,kernel_size = int(self.img_rows/64), strides = 1,padding = 'valid')(model)      \n",
    "\n",
    "        \n",
    "        output_d_cls = Reshape((self.label_dim,))(model)  \n",
    "\n",
    "        output_model = Model(input_img,[output_d_src,output_d_cls])\n",
    "        output_model.summary()\n",
    "        \n",
    "        return output_model\n",
    "    \n",
    "    def train(self,epochs,batch_size=16,sample_interval=500,resume_epoch=0,resume_batch=0):\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        # Adversarial loss ground truths # (-1,2,2,1)\n",
    "        valid = - np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.ones((batch_size,) + self.disc_patch)\n",
    "        dummy = np.zeros((batch_size,) + self.disc_patch) # for gradient panalty of WGAN-GP\n",
    "        \n",
    "        \n",
    "        for epoch in range(resume_epoch,epochs):\n",
    "\n",
    "            # linear decay of learning rate for the last 100 epoch\n",
    "            if self.linear_decay ==True:\n",
    "                self.learning_rate = self.lr_linear_decay(epoch,epochs) \n",
    "            \n",
    "            for batch_i, (imgs, origin_label, target_label) in enumerate(self.data_loader.load_batch(batch_size),start=resume_batch):\n",
    "                \n",
    "                # in case of resume from resume_batch,the stop point needs to be set, otherwise it iterates over the max_batch_num.\n",
    "                if batch_i> self.data_loader.num_batches:   \n",
    "                    #reset the resume_batch parameter so that for the next epoch the iteration will start from 0\n",
    "                    resume_batch = 0\n",
    "                    break\n",
    "\n",
    "                                                           \n",
    "                # ----------------------\n",
    "                #  Train Discriminator\n",
    "                # ----------------------\n",
    "                \n",
    "                \n",
    "                imgs_generated = self.G.predict([imgs,target_label])\n",
    "                \n",
    "                d_loss = self.D_model.train_on_batch(x = [imgs,imgs_generated], y = [valid,fake,dummy,origin_label])\n",
    "                \n",
    "                \n",
    "                # ----------------------\n",
    "                #  Train Generator\n",
    "                # ----------------------                \n",
    "                \n",
    "                \n",
    "                imgs_reconstruct =self.G.predict([imgs_generated,origin_label])\n",
    "                g_loss = self.G_model.train_on_batch(x = [imgs,target_label,origin_label], y=[valid,target_label,imgs])\n",
    "                \n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                \n",
    "                print (\"[epoch %d/%d], [batch %d/%d], [Dloss: %f] [G loss: %05f] time: %s\"\\\n",
    "\n",
    "                       % (epoch, epochs, batch_i, self.data_loader.num_batches,d_loss[0], g_loss[0], elapsed_time))\n",
    "                print (\"current learning rate:%05f\" %self.learning_rate)\n",
    "                \n",
    "                if batch_i % sample_interval == 0:\n",
    "                    \n",
    "                    \n",
    "#                     print(f\"D_model metrics:{self.D_model.metrics_names}\")\n",
    "#                     print(f\"G_model metrics:{self.G_model.metrics_names}\")\n",
    "                    print (\"[epoch %d/%d], [batch %d/%d], [Dloss: %f] [G loss: %05f] time: %s\"\\\n",
    "\n",
    "                           % (epoch, epochs, batch_i, self.data_loader.num_batches,d_loss[0], g_loss[0], elapsed_time))\n",
    "                    print (\"current learning rate:%05f\" %self.learning_rate)\n",
    "                    \n",
    "                    self.sample_images(epoch, batch_i)\n",
    "#                     self.image_debug(imgs,origin_label,target_label)\n",
    "                    \n",
    "                    \n",
    "            # save the generator model each interval of 2000\n",
    "\n",
    "                if batch_i % 2000 == 0:\n",
    "\n",
    "                    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "                    #save models weights\n",
    "                    self.D.save_weights('models\\\\stargan_discriminator_weights-v1.h5')\n",
    "                    self.G.save_weights('models\\\\stargan_generator_weights-v1.h5')\n",
    "                    \n",
    "                    #save models\n",
    "#                     self.D.save('models\\\\stargan_discriminator-v1_CelebA_40dims.h5')\n",
    "#                     self.G.save('models\\\\stargan_generator-v1_CelebA_40dims.h5')              \n",
    "    \n",
    "    def sample_images(self,epoch,batch_i):\n",
    "        \n",
    "        os.makedirs('images/',exist_ok = True)\n",
    "        r, c = 5,6\n",
    "    \n",
    "#         target_label = np.zeros((5,40))\n",
    "        \n",
    "#         # 9: Blond_Hair;15:Eyeglass; 22: Mustache; 31:smiling; 39:Young;\n",
    "        \n",
    "\n",
    "#         # it seems if only one feature is enabled and others are unabled, the machine doesn't understand.\n",
    "#         # need further investication.\n",
    "\n",
    "#         str_0 = \"0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\"\n",
    "# #         str_0 = \"0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1\"\n",
    "#         target_label[0] = np.array(str_0.split())\n",
    "\n",
    "#         str_1 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\"\n",
    "# #         str_1 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0\"\n",
    "#         target_label[1] = np.array(str_1.split())\n",
    "\n",
    "#         str_2 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\" #mastache with male \n",
    "# #         str_2 = \"1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\"\n",
    "#         target_label[2] = np.array(str_2.split())\n",
    "        \n",
    "#         str_3 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\" \n",
    "# #         str_3 = \"0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1\"\n",
    "#         target_label[3] = np.array(str_3.split())\n",
    "\n",
    "#         str_4 = \"0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\" #young with charming\n",
    "# #         str_4 = \"0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\"\n",
    "#         target_label[4] = np.array(str_4.split())\n",
    "        \n",
    "\n",
    "                \n",
    "        #1st photo is original photo\n",
    "        \n",
    "        # 5 x 6 photos, first column is origin photo, other columns are generated photos\n",
    "\n",
    "        imgs_generated_illustration = np.empty(shape=[0,128,128,3])\n",
    "\n",
    "        #5 iterations to get 5 set of photos. imgs_generated_illustration_all is (30,128,128,3)\n",
    "        for k in range(5):\n",
    "            \n",
    "            \n",
    "            # load 1 images\n",
    "            imgs,imgs_origin_label = self.data_loader.load_data(batch_size=1)\n",
    "            \n",
    "            # repeat the origin_label x 5, (40,) -> (5,40)\n",
    "            imgs_origin_label = np.repeat(imgs_origin_label,5,axis=0)\n",
    "            \n",
    "            #try to tune 1 parameter of the original label and make it as target label\n",
    "            target_label = imgs_origin_label\n",
    "            \n",
    "            target_label[0,9] = 1 \n",
    "            target_label[0,8] = 0 #blond without black\n",
    "            target_label[0,10] = 0 #blond without brawn\n",
    "            target_label[0,17] = 0 #blond without gray\n",
    "            target_label[1,15] = 1\n",
    "            target_label[2,22] = 1  \n",
    "            target_label[2,20] = 1 #mastache with male\n",
    "            target_label[2,24] = 0 #mastache without no-beard\n",
    "            target_label[3,31] = 1\n",
    "            target_label[4,39] = 1\n",
    "            target_label[4,2] = 1 #young with attractive\n",
    "            target_label[4,3] = 0 #young without eyebag\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            # 1 image generate 5 different faces by 5 target labels   \n",
    "            imgs_gen = self.G.predict([np.repeat(imgs,5,axis=0),target_label])\n",
    "            # 6 images\n",
    "            imgs_concat = np.concatenate([imgs,imgs_gen])\n",
    "            # 6 images x 5\n",
    "            imgs_generated_illustration =np.append(imgs_generated_illustration,imgs_concat,axis=0)\n",
    "                    \n",
    "        \n",
    "        # Rescale images 0 - 1\n",
    "        imgs_generated_illustration = 0.5 * imgs_generated_illustration + 0.5\n",
    "        \n",
    "        label_title = ['Input','Blond_Hair','Eyeglasses','Mustache','Smiling','Young']\n",
    "        \n",
    "        fig, axs = plt.subplots(r,c)\n",
    "        # 20 x 15 is about 1440 x 1080 pixels\n",
    "        fig.set_size_inches(20, 15, forward=True)\n",
    "        \n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(imgs_generated_illustration[cnt,:,:,:])\n",
    "                if i == 0:                    \n",
    "                    axs[i,j].set_title(label_title[j],fontsize = 25)\n",
    "                    \n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1 \n",
    "        \n",
    "        fig.savefig(f\"images/Sample_Epoch_No_{epoch}_Batch_No_{batch_i}.png\")\n",
    "        plt.close()        \n",
    "    \n",
    "    \n",
    "    def image_debug(self,imgs,origin_label,target_label):\n",
    "        \n",
    "        generated_imgs = self.G.predict([imgs,target_label])\n",
    "        imgs = 0.5 * imgs + 0.5\n",
    "        generated_imgs = 0.5 * generated_imgs + 0.5\n",
    "        \n",
    "        plt.imshow(imgs[0])\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        plt.imshow(generated_imgs[0])\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "#         print('origin_label: ',origin_label[0])\n",
    "#         print('target_label: ',target_label[0])\n",
    "\n",
    "    def generate_image(self):\n",
    "        \n",
    "        \n",
    "        print('sample generation started...')\n",
    "        imgs_folder = 'samples'\n",
    "        #make dir named imgs_folder\n",
    "        os.makedirs(imgs_folder, exist_ok=True)\n",
    "        # read images into np array\n",
    "        imgs_names = self.load_data('{}/*.jpeg'.format\\\n",
    "                                     (imgs_folder))\n",
    "        imgs = [self.read_image(imgs_names[j]) for j in range(len(imgs_names))]\n",
    "        imgs = np.array(imgs)\n",
    "\n",
    "        r, c = 5,6\n",
    "    \n",
    "        target_label = np.zeros((5,40))\n",
    "        \n",
    "        str_0 = \"0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\"\n",
    "#         str_0 = \"0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1\"\n",
    "        target_label[0] = np.array(str_0.split())\n",
    "\n",
    "        str_1 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\"\n",
    "#         str_1 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0\"\n",
    "        target_label[1] = np.array(str_1.split())\n",
    "\n",
    "        str_2 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\" #mastache with male \n",
    "#         str_2 = \"1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\"\n",
    "        target_label[2] = np.array(str_2.split())\n",
    "        \n",
    "        str_3 = \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\" \n",
    "#         str_3 = \"0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1\"\n",
    "        target_label[3] = np.array(str_3.split())\n",
    "\n",
    "        str_4 = \"0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\" #young with charming\n",
    "#         str_4 = \"0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\"\n",
    "        target_label[4] = np.array(str_4.split())        \n",
    "\n",
    "        # 5 x 6 photos, first column is origin photo, other columns are generated photos\n",
    "\n",
    "        imgs_generated_illustration = np.empty(shape=[0,128,128,3])\n",
    "                        \n",
    "        for k in range(5):\n",
    "            \n",
    "            \n",
    "            imgs_gen =  self.G.predict([np.repeat(imgs[k],5,axis=0),target_label])\n",
    "            \n",
    "                        \n",
    "            # 6 images\n",
    "            imgs_concat = np.concatenate([imgs[k],imgs_gen])\n",
    "            # 6 images x 5\n",
    "            imgs_generated_illustration =np.append(imgs_generated_illustration,imgs_concat,axis=0)\n",
    "\n",
    "            \n",
    "         # Rescale images 0 - 1\n",
    "        imgs_generated_illustration = 0.5 * imgs_generated_illustration + 0.5\n",
    "        imgs_generated_illustration = np.clip(imgs_generated_illustration, 0.0, 1.0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        label_title = ['Input','Blond_Hair','Eyeglasses','Mustache','Smiling','Young']\n",
    "        \n",
    "        fig, axs = plt.subplots(r,c)\n",
    "        # 20 x 15 is about 1440 x 1080 pixels\n",
    "        fig.set_size_inches(20, 15, forward=True)\n",
    "        \n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(imgs_generated_illustration[cnt,:,:,:])\n",
    "                if i == 0:                    \n",
    "                    axs[i,j].set_title(label_title[j],fontsize = 25)\n",
    "                    \n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1 \n",
    "\n",
    "        fig.savefig(f\"samples/generated_image.png\")\n",
    "        plt.close()    \n",
    "        print('...sample generation finished')        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #calculate the lr based on the current epoch number. It's near linear but not pure linear\n",
    "    def lr_linear_decay(self,epoch,epochs):\n",
    "        \n",
    "        lr = self.learning_rate_initial\n",
    "        \n",
    "        remaining_epoch = epochs - epoch\n",
    "#         remaining_batch = batches - batch_i\n",
    "\n",
    "        lr = self.learning_rate_initial * (remaining_epoch /epochs ) \n",
    "        \n",
    "        return lr\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0906 10:15:08.442160  1600 deprecation_wrapper.py:119] From C:\\Users\\WIN10\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0906 10:15:08.443158  1600 deprecation_wrapper.py:119] From C:\\Users\\WIN10\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0906 10:15:08.445153  1600 deprecation_wrapper.py:119] From C:\\Users\\WIN10\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W0906 10:15:08.488038  1600 deprecation_wrapper.py:119] From C:\\Users\\WIN10\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0906 10:15:08.590763  1600 deprecation_wrapper.py:119] From C:\\Users\\WIN10\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 130, 130, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 64)   3136        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 64, 64, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 66, 66, 64)   0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 128)  131200      zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 32, 32, 128)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 34, 34, 128)  0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 256)  524544      zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 16, 16, 256)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 18, 18, 256)  0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 512)    2097664     zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 8, 8, 512)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 10, 10, 512)  0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 4, 4, 1024)   8389632     zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 4, 4, 1024)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 6, 6, 1024)   0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 2, 2, 2048)   33556480    zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 2, 2, 2048)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 4, 4, 2048)   0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 1, 1, 40)     327720      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 2, 2, 1)      18433       zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 40)           0           conv2d_8[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 45,048,809\n",
      "Trainable params: 45,048,809\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0906 10:15:10.005978  1600 deprecation_wrapper.py:119] From C:\\Users\\WIN10\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0906 10:15:10.076789  1600 deprecation.py:323] From C:\\Users\\WIN10\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16384, 40)    0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 128, 128, 40) 0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 128, 43) 0           input_4[0][0]                    \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 134, 134, 43) 0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 136, 136, 43) 0           zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 130, 130, 64) 134912      zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_1 (Insta (None, 130, 130, 64) 128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 130, 130, 64) 0           instance_normalization_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPadding2 (None, 132, 132, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 65, 65, 128)  131200      zero_padding2d_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_2 (Insta (None, 65, 65, 128)  256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 65, 65, 128)  0           instance_normalization_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPadding2 (None, 67, 67, 128)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 256)  524544      zero_padding2d_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_3 (Insta (None, 32, 32, 256)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 256)  0           instance_normalization_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPadding2 (None, 34, 34, 256)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  590080      zero_padding2d_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_4 (Insta (None, 32, 32, 256)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 256)  0           instance_normalization_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPadding2 (None, 34, 34, 256)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 256)  590080      zero_padding2d_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_5 (Insta (None, 32, 32, 256)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 256)  0           instance_normalization_5[0][0]   \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPadding2 (None, 34, 34, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 256)  590080      zero_padding2d_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_6 (Insta (None, 32, 32, 256)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 256)  0           instance_normalization_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPadding2 (None, 34, 34, 256)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 256)  590080      zero_padding2d_15[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_7 (Insta (None, 32, 32, 256)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 256)  0           instance_normalization_7[0][0]   \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPadding2 (None, 34, 34, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 256)  590080      zero_padding2d_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_8 (Insta (None, 32, 32, 256)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 256)  0           instance_normalization_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPadding2 (None, 34, 34, 256)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 256)  590080      zero_padding2d_17[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_9 (Insta (None, 32, 32, 256)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 256)  0           instance_normalization_9[0][0]   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_18 (ZeroPadding2 (None, 34, 34, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 256)  590080      zero_padding2d_18[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_10 (Inst (None, 32, 32, 256)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 256)  0           instance_normalization_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_19 (ZeroPadding2 (None, 34, 34, 256)  0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 256)  590080      zero_padding2d_19[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_11 (Inst (None, 32, 32, 256)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 256)  0           instance_normalization_11[0][0]  \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_20 (ZeroPadding2 (None, 34, 34, 256)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 256)  590080      zero_padding2d_20[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_12 (Inst (None, 32, 32, 256)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 256)  0           instance_normalization_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_21 (ZeroPadding2 (None, 34, 34, 256)  0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 256)  590080      zero_padding2d_21[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_13 (Inst (None, 32, 32, 256)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 256)  0           instance_normalization_13[0][0]  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_22 (ZeroPadding2 (None, 34, 34, 256)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 256)  590080      zero_padding2d_22[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_14 (Inst (None, 32, 32, 256)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 256)  0           instance_normalization_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_23 (ZeroPadding2 (None, 34, 34, 256)  0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 256)  590080      zero_padding2d_23[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_15 (Inst (None, 32, 32, 256)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 256)  0           instance_normalization_15[0][0]  \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 64, 64, 128)  524416      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_16 (Inst (None, 64, 64, 128)  256         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 64, 64, 128)  0           instance_normalization_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 128, 128, 64) 131136      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_17 (Inst (None, 128, 128, 64) 128         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 128, 128, 64) 0           instance_normalization_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_24 (ZeroPadding2 (None, 134, 134, 64) 0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 128, 128, 3)  9411        zero_padding2d_24[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_18 (Inst (None, 128, 128, 3)  6           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 3)  0           instance_normalization_18[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 8,544,009\n",
      "Trainable params: 8,544,009\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 4200/12662], [Dloss: 0.355243] [G loss: 0.179131] time: 0:00:14.452348\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4201/12662], [Dloss: 0.235577] [G loss: 0.199248] time: 0:00:15.859584\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4202/12662], [Dloss: 0.263194] [G loss: 0.461397] time: 0:00:17.245877\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4203/12662], [Dloss: 0.399184] [G loss: 0.575096] time: 0:00:18.688020\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4204/12662], [Dloss: 0.349227] [G loss: 0.366452] time: 0:00:20.090270\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4205/12662], [Dloss: 1.001994] [G loss: 0.282465] time: 0:00:21.481549\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4206/12662], [Dloss: 0.179557] [G loss: 0.184209] time: 0:00:22.888786\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4207/12662], [Dloss: 0.552266] [G loss: -0.183752] time: 0:00:24.228204\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4208/12662], [Dloss: 0.478669] [G loss: -0.096204] time: 0:00:25.563632\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4209/12662], [Dloss: 0.259746] [G loss: 0.125375] time: 0:00:27.033702\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4210/12662], [Dloss: 0.254606] [G loss: 0.055484] time: 0:00:28.424980\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4211/12662], [Dloss: 0.303069] [G loss: -0.057258] time: 0:00:29.848174\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4212/12662], [Dloss: 0.190132] [G loss: 0.100066] time: 0:00:31.361128\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4213/12662], [Dloss: 0.109585] [G loss: -0.005695] time: 0:00:32.836183\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4214/12662], [Dloss: 0.454846] [G loss: -0.058518] time: 0:00:34.169617\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4215/12662], [Dloss: 0.030763] [G loss: -0.073980] time: 0:00:35.559899\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4216/12662], [Dloss: 0.500124] [G loss: -0.059144] time: 0:00:37.010021\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4217/12662], [Dloss: 0.441309] [G loss: -0.088736] time: 0:00:38.401300\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4218/12662], [Dloss: 0.482401] [G loss: -0.358517] time: 0:00:39.760664\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4219/12662], [Dloss: 0.134695] [G loss: -0.111620] time: 0:00:41.140974\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4220/12662], [Dloss: 0.377254] [G loss: -0.380877] time: 0:00:42.627996\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4221/12662], [Dloss: -0.097510] [G loss: -0.297273] time: 0:00:44.067147\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4222/12662], [Dloss: 0.078406] [G loss: -0.589117] time: 0:00:45.412549\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4223/12662], [Dloss: 0.431660] [G loss: -0.030599] time: 0:00:46.776900\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4224/12662], [Dloss: 0.484518] [G loss: 0.199453] time: 0:00:48.207076\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4225/12662], [Dloss: 0.132553] [G loss: 0.366513] time: 0:00:49.602344\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4226/12662], [Dloss: 0.681663] [G loss: 0.666760] time: 0:00:50.963703\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4227/12662], [Dloss: 0.452207] [G loss: 0.319106] time: 0:00:52.327057\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4228/12662], [Dloss: 0.105794] [G loss: 0.143708] time: 0:00:53.664480\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4229/12662], [Dloss: 0.596385] [G loss: -0.285815] time: 0:00:55.030826\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4230/12662], [Dloss: 0.501120] [G loss: -0.229880] time: 0:00:56.579685\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4231/12662], [Dloss: 0.536950] [G loss: 0.041333] time: 0:00:58.038782\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4232/12662], [Dloss: 0.172977] [G loss: -0.142139] time: 0:00:59.450009\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4233/12662], [Dloss: 0.329977] [G loss: 0.194778] time: 0:01:00.824333\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4234/12662], [Dloss: 0.383219] [G loss: 0.008643] time: 0:01:02.188684\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4235/12662], [Dloss: 0.475941] [G loss: -0.148658] time: 0:01:03.590934\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4236/12662], [Dloss: 0.150156] [G loss: -0.417546] time: 0:01:05.002160\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4237/12662], [Dloss: -0.015830] [G loss: -0.351535] time: 0:01:06.433332\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4238/12662], [Dloss: 0.385800] [G loss: -0.103741] time: 0:01:07.862510\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4239/12662], [Dloss: 0.270087] [G loss: 0.006072] time: 0:01:09.302658\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4240/12662], [Dloss: 0.334285] [G loss: 0.380069] time: 0:01:10.673991\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4241/12662], [Dloss: 0.496502] [G loss: 0.520027] time: 0:01:12.027372\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4242/12662], [Dloss: 0.456203] [G loss: 0.348097] time: 0:01:13.371776\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4243/12662], [Dloss: 0.196624] [G loss: 0.401694] time: 0:01:14.689253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4244/12662], [Dloss: 0.169874] [G loss: 0.230665] time: 0:01:16.003737\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4245/12662], [Dloss: 0.066516] [G loss: -0.000660] time: 0:01:17.516691\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4246/12662], [Dloss: 0.474729] [G loss: -0.101682] time: 0:01:18.966813\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4247/12662], [Dloss: 0.340835] [G loss: -0.019850] time: 0:01:20.420924\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4248/12662], [Dloss: 0.200523] [G loss: -0.313003] time: 0:01:21.832150\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4249/12662], [Dloss: 0.224739] [G loss: -0.214908] time: 0:01:23.228416\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4250/12662], [Dloss: 0.349811] [G loss: -0.243535] time: 0:01:24.661583\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4251/12662], [Dloss: 0.201149] [G loss: 0.026974] time: 0:01:25.973076\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4252/12662], [Dloss: 0.005693] [G loss: 0.045913] time: 0:01:27.479049\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4253/12662], [Dloss: 0.226692] [G loss: 0.385066] time: 0:01:29.008957\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4254/12662], [Dloss: 0.287274] [G loss: 0.279065] time: 0:01:30.392257\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4255/12662], [Dloss: -0.119472] [G loss: 0.230506] time: 0:01:31.800491\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4256/12662], [Dloss: 0.216131] [G loss: 0.526677] time: 0:01:33.171824\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4257/12662], [Dloss: 0.457104] [G loss: 0.280861] time: 0:01:34.640895\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4258/12662], [Dloss: 0.340969] [G loss: 0.438335] time: 0:01:36.020206\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4259/12662], [Dloss: 0.143678] [G loss: 0.339698] time: 0:01:37.376579\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4260/12662], [Dloss: 0.228608] [G loss: 0.070113] time: 0:01:38.767858\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4261/12662], [Dloss: 0.068151] [G loss: 0.200138] time: 0:01:40.159137\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4262/12662], [Dloss: 0.105906] [G loss: 0.342088] time: 0:01:41.577344\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4263/12662], [Dloss: 0.437892] [G loss: 0.463548] time: 0:01:42.912783\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4264/12662], [Dloss: 0.550918] [G loss: 0.345608] time: 0:01:44.290851\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4265/12662], [Dloss: 0.144667] [G loss: 0.065750] time: 0:01:45.681163\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4266/12662], [Dloss: 0.129807] [G loss: 0.147540] time: 0:01:47.053493\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 4267/12662], [Dloss: 0.555336] [G loss: -0.079844] time: 0:01:48.435797\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4268/12662], [Dloss: 0.617201] [G loss: 0.080980] time: 0:01:49.875945\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4269/12662], [Dloss: 0.479207] [G loss: 0.312664] time: 0:01:51.236307\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4270/12662], [Dloss: 0.167089] [G loss: 0.111039] time: 0:01:52.640552\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4271/12662], [Dloss: 0.280735] [G loss: 0.113490] time: 0:01:53.997922\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4272/12662], [Dloss: 0.341468] [G loss: -0.066586] time: 0:01:55.415132\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4273/12662], [Dloss: 0.565325] [G loss: 0.264798] time: 0:01:56.809403\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4274/12662], [Dloss: -0.192526] [G loss: 0.233674] time: 0:01:58.155802\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4275/12662], [Dloss: -0.085817] [G loss: 0.185831] time: 0:01:59.560046\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4276/12662], [Dloss: -0.031730] [G loss: 0.139602] time: 0:02:00.941352\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4277/12662], [Dloss: 0.142914] [G loss: 0.287500] time: 0:02:02.240877\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4278/12662], [Dloss: 0.188933] [G loss: 0.340937] time: 0:02:03.594258\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4279/12662], [Dloss: 0.467673] [G loss: 0.082930] time: 0:02:04.944646\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4280/12662], [Dloss: 0.313308] [G loss: 0.340041] time: 0:02:06.380805\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4281/12662], [Dloss: 0.292745] [G loss: 0.036142] time: 0:02:07.723215\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4282/12662], [Dloss: 0.393224] [G loss: 0.193160] time: 0:02:09.065625\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4283/12662], [Dloss: 0.098781] [G loss: 0.108168] time: 0:02:10.456904\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4284/12662], [Dloss: 0.408548] [G loss: -0.139277] time: 0:02:11.814274\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4285/12662], [Dloss: 0.492234] [G loss: 0.055470] time: 0:02:13.206551\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4286/12662], [Dloss: 0.536259] [G loss: -0.290861] time: 0:02:14.538987\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4287/12662], [Dloss: -0.038755] [G loss: -0.005863] time: 0:02:15.898352\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4288/12662], [Dloss: 0.139727] [G loss: 0.115459] time: 0:02:17.243754\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4289/12662], [Dloss: 0.160855] [G loss: -0.026816] time: 0:02:18.635033\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4290/12662], [Dloss: 0.177344] [G loss: 0.145838] time: 0:02:19.992403\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4291/12662], [Dloss: 0.389234] [G loss: 0.019424] time: 0:02:21.387672\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4292/12662], [Dloss: 0.159793] [G loss: -0.095209] time: 0:02:22.743047\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4293/12662], [Dloss: 0.092073] [G loss: 0.134120] time: 0:02:24.189179\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4294/12662], [Dloss: 0.464304] [G loss: -0.336844] time: 0:02:25.599408\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4295/12662], [Dloss: 0.499780] [G loss: 0.062475] time: 0:02:26.970740\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4296/12662], [Dloss: 0.401170] [G loss: -0.215716] time: 0:02:28.370170\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4297/12662], [Dloss: -0.005065] [G loss: 0.259663] time: 0:02:29.785214\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4298/12662], [Dloss: -0.001828] [G loss: -0.346238] time: 0:02:31.148568\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4299/12662], [Dloss: 0.259881] [G loss: -0.494848] time: 0:02:32.459063\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4300/12662], [Dloss: 0.232419] [G loss: -0.747290] time: 0:02:33.835382\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4301/12662], [Dloss: 0.061051] [G loss: -0.526653] time: 0:02:35.193749\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4302/12662], [Dloss: 0.325618] [G loss: -0.535839] time: 0:02:36.597994\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4303/12662], [Dloss: 0.583113] [G loss: -0.376553] time: 0:02:38.029166\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4304/12662], [Dloss: -0.029615] [G loss: 0.286001] time: 0:02:39.395512\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4305/12662], [Dloss: 0.300610] [G loss: 0.212747] time: 0:02:40.766845\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4306/12662], [Dloss: 0.292085] [G loss: 0.016265] time: 0:02:42.118231\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4307/12662], [Dloss: 0.206216] [G loss: 0.262799] time: 0:02:43.411771\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4308/12662], [Dloss: 0.825865] [G loss: -0.102231] time: 0:02:44.779115\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4309/12662], [Dloss: 0.637697] [G loss: -0.154471] time: 0:02:46.162415\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4310/12662], [Dloss: 0.326329] [G loss: -0.209648] time: 0:02:47.545715\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4311/12662], [Dloss: 0.280714] [G loss: -0.258639] time: 0:02:48.954947\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4312/12662], [Dloss: 0.046799] [G loss: -0.327640] time: 0:02:50.324285\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4313/12662], [Dloss: 0.389593] [G loss: -0.304284] time: 0:02:51.772412\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4314/12662], [Dloss: 0.224466] [G loss: -0.215646] time: 0:02:53.091883\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4315/12662], [Dloss: -0.062300] [G loss: 0.201934] time: 0:02:54.563946\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4316/12662], [Dloss: 0.356896] [G loss: 0.218935] time: 0:02:56.014068\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4317/12662], [Dloss: 0.383816] [G loss: 0.519280] time: 0:02:57.362462\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4318/12662], [Dloss: 0.508148] [G loss: -0.104547] time: 0:02:58.785656\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4319/12662], [Dloss: 0.325961] [G loss: -0.002108] time: 0:03:00.201868\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4320/12662], [Dloss: 0.242777] [G loss: 0.058660] time: 0:03:01.553254\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4321/12662], [Dloss: 0.200004] [G loss: -0.156475] time: 0:03:02.907632\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4322/12662], [Dloss: 0.247596] [G loss: -0.055089] time: 0:03:04.227104\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4323/12662], [Dloss: 0.230498] [G loss: -0.175427] time: 0:03:05.588463\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4324/12662], [Dloss: 0.234357] [G loss: 0.118983] time: 0:03:07.070499\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4325/12662], [Dloss: 0.472540] [G loss: -0.256430] time: 0:03:08.503667\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4326/12662], [Dloss: 0.109275] [G loss: 0.000710] time: 0:03:09.915890\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4327/12662], [Dloss: 0.261705] [G loss: -0.012028] time: 0:03:11.223393\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4328/12662], [Dloss: 0.252081] [G loss: 0.229385] time: 0:03:12.620656\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4329/12662], [Dloss: 0.617975] [G loss: 0.215721] time: 0:03:13.977029\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4330/12662], [Dloss: 0.192840] [G loss: 0.052056] time: 0:03:15.500953\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4331/12662], [Dloss: 0.478419] [G loss: -0.035557] time: 0:03:16.891235\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4332/12662], [Dloss: 0.484012] [G loss: -0.363689] time: 0:03:18.289496\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4333/12662], [Dloss: 0.035309] [G loss: 0.035316] time: 0:03:19.796466\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 4334/12662], [Dloss: 0.172650] [G loss: 0.169088] time: 0:03:21.254566\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4335/12662], [Dloss: 0.331801] [G loss: 0.298202] time: 0:03:22.654821\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4336/12662], [Dloss: -0.174765] [G loss: 0.018192] time: 0:03:24.101951\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4337/12662], [Dloss: 0.597968] [G loss: 0.128964] time: 0:03:25.705662\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4338/12662], [Dloss: 0.229496] [G loss: 0.229600] time: 0:03:27.154787\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4339/12662], [Dloss: 0.297261] [G loss: 0.347680] time: 0:03:28.577981\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4340/12662], [Dloss: 0.014804] [G loss: 0.395491] time: 0:03:29.965271\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4341/12662], [Dloss: 0.598702] [G loss: 0.061844] time: 0:03:31.359542\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4342/12662], [Dloss: 0.083242] [G loss: 0.066782] time: 0:03:32.707936\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4343/12662], [Dloss: 0.464534] [G loss: 0.322394] time: 0:03:34.105199\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4344/12662], [Dloss: 0.165126] [G loss: -0.103564] time: 0:03:35.485508\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4345/12662], [Dloss: 0.364203] [G loss: 0.028494] time: 0:03:36.822931\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4346/12662], [Dloss: 0.371352] [G loss: 0.131251] time: 0:03:38.299981\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4347/12662], [Dloss: 0.016756] [G loss: 0.254301] time: 0:03:39.736140\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4348/12662], [Dloss: 0.363436] [G loss: -0.294600] time: 0:03:41.177286\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4349/12662], [Dloss: 0.409846] [G loss: -0.123620] time: 0:03:42.655333\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4350/12662], [Dloss: -0.158407] [G loss: 0.146032] time: 0:03:44.038654\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4351/12662], [Dloss: 0.046503] [G loss: 0.068823] time: 0:03:45.489797\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4352/12662], [Dloss: 0.205509] [G loss: -0.178503] time: 0:03:47.001753\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4353/12662], [Dloss: 0.625422] [G loss: -0.031992] time: 0:03:48.428937\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4354/12662], [Dloss: 0.426526] [G loss: -0.082991] time: 0:03:49.978792\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4355/12662], [Dloss: 0.256622] [G loss: 0.024816] time: 0:03:51.467810\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4356/12662], [Dloss: 0.234753] [G loss: 0.667002] time: 0:03:52.983755\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4357/12662], [Dloss: 0.466209] [G loss: 0.349421] time: 0:03:54.426896\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4358/12662], [Dloss: 0.523467] [G loss: 0.238392] time: 0:03:55.832138\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4359/12662], [Dloss: 0.243258] [G loss: 0.123470] time: 0:03:57.309188\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4360/12662], [Dloss: 0.446964] [G loss: 0.148263] time: 0:03:58.654590\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4361/12662], [Dloss: 0.369747] [G loss: 0.112300] time: 0:04:00.091746\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4362/12662], [Dloss: -0.093715] [G loss: 0.350182] time: 0:04:01.416204\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4363/12662], [Dloss: 0.460418] [G loss: 0.183144] time: 0:04:02.866326\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4364/12662], [Dloss: 0.369376] [G loss: -0.002268] time: 0:04:04.265584\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4365/12662], [Dloss: 0.395937] [G loss: -0.106674] time: 0:04:05.647887\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4366/12662], [Dloss: 0.199527] [G loss: -0.121181] time: 0:04:07.017225\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4367/12662], [Dloss: 0.416089] [G loss: -0.203357] time: 0:04:08.367613\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4368/12662], [Dloss: 0.430895] [G loss: -0.043867] time: 0:04:09.847655\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4369/12662], [Dloss: 0.435637] [G loss: -0.076039] time: 0:04:11.277830\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4370/12662], [Dloss: 0.317214] [G loss: 0.101717] time: 0:04:12.789787\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4371/12662], [Dloss: 0.203671] [G loss: 0.255628] time: 0:04:14.146160\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4372/12662], [Dloss: 0.330393] [G loss: 0.596772] time: 0:04:15.541428\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4373/12662], [Dloss: 0.163901] [G loss: 0.511049] time: 0:04:16.941683\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4374/12662], [Dloss: 0.308755] [G loss: 0.427557] time: 0:04:18.347923\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4375/12662], [Dloss: 0.424554] [G loss: 0.217961] time: 0:04:19.781090\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4376/12662], [Dloss: 0.351230] [G loss: 0.123975] time: 0:04:21.092582\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4377/12662], [Dloss: 0.465302] [G loss: -0.055725] time: 0:04:22.569632\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4378/12662], [Dloss: 0.150465] [G loss: -0.128274] time: 0:04:23.867162\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4379/12662], [Dloss: 0.209922] [G loss: -0.096292] time: 0:04:25.252458\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4380/12662], [Dloss: 0.569633] [G loss: -0.063934] time: 0:04:26.747459\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4381/12662], [Dloss: 0.371898] [G loss: 0.290828] time: 0:04:28.147714\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4382/12662], [Dloss: 0.365885] [G loss: 0.265747] time: 0:04:29.606812\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4383/12662], [Dloss: -0.006446] [G loss: 0.477549] time: 0:04:31.074887\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4384/12662], [Dloss: 0.274469] [G loss: 0.167987] time: 0:04:32.474144\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4385/12662], [Dloss: 0.696997] [G loss: 0.211476] time: 0:04:33.995076\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4386/12662], [Dloss: 0.375763] [G loss: 0.350482] time: 0:04:35.400318\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4387/12662], [Dloss: 0.443878] [G loss: 0.169357] time: 0:04:36.750707\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4388/12662], [Dloss: 0.524370] [G loss: 0.115505] time: 0:04:38.263661\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4389/12662], [Dloss: -0.043508] [G loss: 0.646487] time: 0:04:39.702812\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4390/12662], [Dloss: 0.345115] [G loss: 0.280329] time: 0:04:41.169888\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4391/12662], [Dloss: 0.282443] [G loss: 0.120860] time: 0:04:42.624000\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4392/12662], [Dloss: 0.076357] [G loss: 0.268677] time: 0:04:43.957433\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4393/12662], [Dloss: 0.368934] [G loss: 0.177641] time: 0:04:45.340734\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4394/12662], [Dloss: 0.250151] [G loss: 0.732264] time: 0:04:46.725032\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4395/12662], [Dloss: 0.151263] [G loss: 0.410319] time: 0:04:48.170167\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4396/12662], [Dloss: 0.189853] [G loss: 0.267312] time: 0:04:49.532524\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4397/12662], [Dloss: 0.087648] [G loss: -0.032709] time: 0:04:50.871941\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4398/12662], [Dloss: 0.238738] [G loss: 0.302707] time: 0:04:52.319071\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4399/12662], [Dloss: 0.503300] [G loss: 0.438558] time: 0:04:53.696388\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4400/12662], [Dloss: 0.320869] [G loss: 0.375018] time: 0:04:55.040793\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 4401/12662], [Dloss: 0.083767] [G loss: 0.423542] time: 0:04:56.418109\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4402/12662], [Dloss: 0.140966] [G loss: 0.165650] time: 0:04:57.812380\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4403/12662], [Dloss: 0.352841] [G loss: 0.190412] time: 0:04:59.197675\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4404/12662], [Dloss: 0.485402] [G loss: -0.042686] time: 0:05:00.569008\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4405/12662], [Dloss: 0.165215] [G loss: -0.005689] time: 0:05:01.965274\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4406/12662], [Dloss: 0.272044] [G loss: -0.241595] time: 0:05:03.337604\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4407/12662], [Dloss: 0.415791] [G loss: 0.067335] time: 0:05:04.789721\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4408/12662], [Dloss: 0.429332] [G loss: -0.242407] time: 0:05:06.201944\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4409/12662], [Dloss: 0.330602] [G loss: -0.530132] time: 0:05:07.573276\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4410/12662], [Dloss: 0.303483] [G loss: -0.276656] time: 0:05:08.897734\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4411/12662], [Dloss: -0.054542] [G loss: -0.182377] time: 0:05:10.289013\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4412/12662], [Dloss: 0.099495] [G loss: 0.002404] time: 0:05:11.633418\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4413/12662], [Dloss: 0.172569] [G loss: 0.047963] time: 0:05:13.019711\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4414/12662], [Dloss: 0.326540] [G loss: -0.107413] time: 0:05:14.435923\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4415/12662], [Dloss: 0.502865] [G loss: 0.163899] time: 0:05:15.706525\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4416/12662], [Dloss: 0.378525] [G loss: 0.452362] time: 0:05:17.140689\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4417/12662], [Dloss: 0.323933] [G loss: 0.496658] time: 0:05:18.510027\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4418/12662], [Dloss: 0.199296] [G loss: 0.126042] time: 0:05:19.918261\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4419/12662], [Dloss: 0.582951] [G loss: -0.091127] time: 0:05:21.398303\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4420/12662], [Dloss: 0.431694] [G loss: -0.133105] time: 0:05:22.805541\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4421/12662], [Dloss: 0.475753] [G loss: -0.298945] time: 0:05:24.206792\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4422/12662], [Dloss: 0.202760] [G loss: 0.123814] time: 0:05:25.663896\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4423/12662], [Dloss: 0.178111] [G loss: 0.148077] time: 0:05:27.010295\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4424/12662], [Dloss: 0.396380] [G loss: 0.134023] time: 0:05:28.433488\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4425/12662], [Dloss: 0.308748] [G loss: 0.496754] time: 0:05:29.875632\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4426/12662], [Dloss: 0.580863] [G loss: 0.404264] time: 0:05:31.233999\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4427/12662], [Dloss: 0.353083] [G loss: 0.897503] time: 0:05:32.700078\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4428/12662], [Dloss: 0.512944] [G loss: 0.901263] time: 0:05:34.101331\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4429/12662], [Dloss: 0.058508] [G loss: 0.940406] time: 0:05:35.473661\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4430/12662], [Dloss: 0.374908] [G loss: 0.598411] time: 0:05:36.979633\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4431/12662], [Dloss: 0.286736] [G loss: 0.441767] time: 0:05:38.359942\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4432/12662], [Dloss: 0.276532] [G loss: 0.252152] time: 0:05:39.800090\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4433/12662], [Dloss: 0.582506] [G loss: 0.200424] time: 0:05:41.271156\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4434/12662], [Dloss: -0.270135] [G loss: -0.705603] time: 0:05:42.613566\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4435/12662], [Dloss: 0.421231] [G loss: -0.131191] time: 0:05:44.014850\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4436/12662], [Dloss: 0.451898] [G loss: -0.258310] time: 0:05:45.379706\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4437/12662], [Dloss: 0.605804] [G loss: -0.651916] time: 0:05:46.784947\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4438/12662], [Dloss: 0.040708] [G loss: -0.010675] time: 0:05:48.232077\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4439/12662], [Dloss: 0.148522] [G loss: -0.196460] time: 0:05:49.772957\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4440/12662], [Dloss: 0.322387] [G loss: 0.137168] time: 0:05:51.236044\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4441/12662], [Dloss: 0.146736] [G loss: 0.392894] time: 0:05:52.603387\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4442/12662], [Dloss: 0.401119] [G loss: 0.563698] time: 0:05:53.942805\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4443/12662], [Dloss: 0.402509] [G loss: 0.545677] time: 0:05:55.240335\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4444/12662], [Dloss: 0.121231] [G loss: 0.358056] time: 0:05:56.616654\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4445/12662], [Dloss: 0.247082] [G loss: -0.153500] time: 0:05:57.920168\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4446/12662], [Dloss: 0.371290] [G loss: 0.276362] time: 0:05:59.369293\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4447/12662], [Dloss: 0.220505] [G loss: 0.116346] time: 0:06:00.719681\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4448/12662], [Dloss: 0.124951] [G loss: 0.113462] time: 0:06:02.127915\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4449/12662], [Dloss: 0.258524] [G loss: 0.270474] time: 0:06:03.576042\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4450/12662], [Dloss: 0.450036] [G loss: -0.303898] time: 0:06:04.934409\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4451/12662], [Dloss: 0.454683] [G loss: -0.450069] time: 0:06:06.375555\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4452/12662], [Dloss: 0.156559] [G loss: -0.272478] time: 0:06:07.869560\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4453/12662], [Dloss: -0.099479] [G loss: -0.606580] time: 0:06:09.266823\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4454/12662], [Dloss: 0.423041] [G loss: -0.526657] time: 0:06:10.713953\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4455/12662], [Dloss: 0.186443] [G loss: -0.366968] time: 0:06:12.074315\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4456/12662], [Dloss: 0.306778] [G loss: -0.176466] time: 0:06:13.533413\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4457/12662], [Dloss: 0.306248] [G loss: 0.272988] time: 0:06:14.927684\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4458/12662], [Dloss: 0.454418] [G loss: 0.263041] time: 0:06:16.222222\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4459/12662], [Dloss: 0.181032] [G loss: 0.618718] time: 0:06:17.651400\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4460/12662], [Dloss: 0.356275] [G loss: 0.498057] time: 0:06:19.006775\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4461/12662], [Dloss: 0.155174] [G loss: 0.624720] time: 0:06:20.410022\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4462/12662], [Dloss: 0.199511] [G loss: 0.534468] time: 0:06:21.753429\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4463/12662], [Dloss: 0.095432] [G loss: 0.203991] time: 0:06:23.184602\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4464/12662], [Dloss: 0.215012] [G loss: 0.485972] time: 0:06:24.572889\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4465/12662], [Dloss: 0.218782] [G loss: 0.104692] time: 0:06:25.972147\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4466/12662], [Dloss: 0.159725] [G loss: 0.119389] time: 0:06:27.405314\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4467/12662], [Dloss: 0.249997] [G loss: 0.025903] time: 0:06:28.878374\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 4468/12662], [Dloss: 0.114424] [G loss: 0.429497] time: 0:06:30.277632\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4469/12662], [Dloss: 0.222211] [G loss: -0.163251] time: 0:06:31.664922\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4470/12662], [Dloss: 0.344273] [G loss: 0.404792] time: 0:06:33.116042\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4471/12662], [Dloss: 0.228504] [G loss: 0.267899] time: 0:06:34.470420\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4472/12662], [Dloss: 0.098307] [G loss: -0.143234] time: 0:06:35.908574\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4473/12662], [Dloss: 0.157060] [G loss: -0.118169] time: 0:06:37.314813\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4474/12662], [Dloss: 0.411655] [G loss: -0.062438] time: 0:06:38.672182\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4475/12662], [Dloss: 0.367433] [G loss: -0.048045] time: 0:06:40.101360\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4476/12662], [Dloss: 0.340363] [G loss: 0.150493] time: 0:06:41.526550\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4477/12662], [Dloss: -0.025673] [G loss: 0.239718] time: 0:06:42.970686\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4478/12662], [Dloss: -0.033809] [G loss: -0.036942] time: 0:06:44.527524\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4479/12662], [Dloss: 0.460037] [G loss: -0.193677] time: 0:06:46.055437\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4480/12662], [Dloss: 0.544884] [G loss: 0.073698] time: 0:06:47.455692\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4481/12662], [Dloss: 0.473476] [G loss: -0.179212] time: 0:06:48.830017\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4482/12662], [Dloss: 0.297740] [G loss: 0.061765] time: 0:06:50.237253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4483/12662], [Dloss: 0.128932] [G loss: -0.017762] time: 0:06:54.416078\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4484/12662], [Dloss: 0.365741] [G loss: -0.416651] time: 0:06:56.788732\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4485/12662], [Dloss: -0.498270] [G loss: 0.330937] time: 0:06:58.316646\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4486/12662], [Dloss: 0.150842] [G loss: -0.024647] time: 0:06:59.710917\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4487/12662], [Dloss: 0.291264] [G loss: 0.010759] time: 0:07:01.080255\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4488/12662], [Dloss: 0.491658] [G loss: 0.009755] time: 0:07:02.601188\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4489/12662], [Dloss: 0.269527] [G loss: 0.536321] time: 0:07:03.954568\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4490/12662], [Dloss: 0.236251] [G loss: 0.478296] time: 0:07:05.366791\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4491/12662], [Dloss: 0.383728] [G loss: 0.580679] time: 0:07:06.784999\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4492/12662], [Dloss: 0.262785] [G loss: 0.315670] time: 0:07:08.463510\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4493/12662], [Dloss: 0.593441] [G loss: 0.416306] time: 0:07:10.211834\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4494/12662], [Dloss: 0.494782] [G loss: 0.085650] time: 0:07:11.655972\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4495/12662], [Dloss: 0.370952] [G loss: 0.262534] time: 0:07:13.128035\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4496/12662], [Dloss: 0.507010] [G loss: -0.247547] time: 0:07:14.549234\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4497/12662], [Dloss: 0.267719] [G loss: -0.274863] time: 0:07:16.032268\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4498/12662], [Dloss: 0.209628] [G loss: -0.222309] time: 0:07:17.367697\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4499/12662], [Dloss: 0.377605] [G loss: -0.700314] time: 0:07:18.697141\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4500/12662], [Dloss: 0.359902] [G loss: -0.579095] time: 0:07:19.996666\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4500/12662], [Dloss: 0.359902] [G loss: -0.579095] time: 0:07:19.996666\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4501/12662], [Dloss: 0.370866] [G loss: -0.272873] time: 0:07:22.783214\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4502/12662], [Dloss: 0.201770] [G loss: 0.045655] time: 0:07:24.084733\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4503/12662], [Dloss: 0.128428] [G loss: 0.244798] time: 0:07:25.404204\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4504/12662], [Dloss: 0.226659] [G loss: 0.519561] time: 0:07:26.937619\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4505/12662], [Dloss: 0.534468] [G loss: 0.520604] time: 0:07:28.324939\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4506/12662], [Dloss: 0.184542] [G loss: 0.274599] time: 0:07:29.706245\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4507/12662], [Dloss: 0.361351] [G loss: 0.381676] time: 0:07:31.207231\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4508/12662], [Dloss: 0.220609] [G loss: 0.540840] time: 0:07:32.683283\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4509/12662], [Dloss: 0.435363] [G loss: 0.110280] time: 0:07:34.033672\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4510/12662], [Dloss: 0.311081] [G loss: 0.006866] time: 0:07:35.395031\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4511/12662], [Dloss: 0.223613] [G loss: 0.010253] time: 0:07:36.896017\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4512/12662], [Dloss: 0.160199] [G loss: -0.049212] time: 0:07:38.335168\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4513/12662], [Dloss: 0.470279] [G loss: -0.400689] time: 0:07:39.678575\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4514/12662], [Dloss: 0.401182] [G loss: -0.341056] time: 0:07:41.102767\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4515/12662], [Dloss: 0.251305] [G loss: -0.043265] time: 0:07:42.531944\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4516/12662], [Dloss: 0.366217] [G loss: -0.101099] time: 0:07:44.006507\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4517/12662], [Dloss: 0.245478] [G loss: -0.166532] time: 0:07:45.424230\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4518/12662], [Dloss: 0.065419] [G loss: -0.097859] time: 0:07:46.894299\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4519/12662], [Dloss: 0.271428] [G loss: -0.103156] time: 0:07:48.225738\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4520/12662], [Dloss: -0.242249] [G loss: 0.019999] time: 0:07:49.848399\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4521/12662], [Dloss: 0.424254] [G loss: -0.367394] time: 0:07:51.203774\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4522/12662], [Dloss: -0.118765] [G loss: 0.231491] time: 0:07:52.529229\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4523/12662], [Dloss: 0.404708] [G loss: 0.092232] time: 0:07:53.988327\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4524/12662], [Dloss: 0.454385] [G loss: -0.355849] time: 0:07:55.382598\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4525/12662], [Dloss: 0.654958] [G loss: -0.027815] time: 0:07:56.815766\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4526/12662], [Dloss: 0.408404] [G loss: -0.134979] time: 0:07:58.150197\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4527/12662], [Dloss: -0.112727] [G loss: -0.250743] time: 0:07:59.507567\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4528/12662], [Dloss: 0.404507] [G loss: -0.171520] time: 0:08:00.857955\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4529/12662], [Dloss: 0.600090] [G loss: -0.745347] time: 0:08:02.238264\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4530/12662], [Dloss: 0.195640] [G loss: -0.176737] time: 0:08:03.618573\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4531/12662], [Dloss: 0.435973] [G loss: -0.410947] time: 0:08:04.949014\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4532/12662], [Dloss: 0.541729] [G loss: -0.340616] time: 0:08:06.314363\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4533/12662], [Dloss: 0.293125] [G loss: 0.327736] time: 0:08:07.628848\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 4534/12662], [Dloss: 0.053840] [G loss: 0.481167] time: 0:08:09.030100\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4535/12662], [Dloss: 0.364751] [G loss: 0.632604] time: 0:08:10.337603\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4536/12662], [Dloss: 0.442246] [G loss: 0.594317] time: 0:08:11.673032\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4537/12662], [Dloss: 0.209401] [G loss: 0.389487] time: 0:08:12.988514\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4538/12662], [Dloss: 0.048383] [G loss: 0.553761] time: 0:08:14.305990\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4539/12662], [Dloss: 0.034000] [G loss: 0.632764] time: 0:08:15.689291\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4540/12662], [Dloss: 0.328031] [G loss: 0.483161] time: 0:08:17.204240\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4541/12662], [Dloss: 0.238670] [G loss: 0.717213] time: 0:08:18.692260\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4542/12662], [Dloss: 0.645259] [G loss: 0.164333] time: 0:08:20.187262\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4543/12662], [Dloss: 0.214405] [G loss: 0.048656] time: 0:08:21.597490\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4544/12662], [Dloss: 0.018009] [G loss: 0.159395] time: 0:08:23.018690\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4545/12662], [Dloss: 0.275405] [G loss: -0.294357] time: 0:08:24.549595\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4546/12662], [Dloss: 0.114262] [G loss: 0.158726] time: 0:08:25.949858\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4547/12662], [Dloss: 0.293645] [G loss: -0.071471] time: 0:08:27.305226\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4548/12662], [Dloss: 0.538548] [G loss: 0.049990] time: 0:08:28.724430\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4549/12662], [Dloss: 0.511210] [G loss: 0.331668] time: 0:08:30.125683\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4550/12662], [Dloss: 0.342875] [G loss: 0.372991] time: 0:08:31.405261\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4551/12662], [Dloss: 0.125308] [G loss: 0.324331] time: 0:08:32.770609\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4552/12662], [Dloss: -0.031021] [G loss: 0.283697] time: 0:08:34.112022\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4553/12662], [Dloss: 0.432700] [G loss: 0.230527] time: 0:08:35.514272\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4554/12662], [Dloss: 0.176012] [G loss: 0.246873] time: 0:08:36.892586\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4555/12662], [Dloss: 0.313160] [G loss: 0.388448] time: 0:08:38.266910\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4556/12662], [Dloss: 0.162081] [G loss: 0.427902] time: 0:08:39.688109\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4557/12662], [Dloss: 0.258902] [G loss: 0.126202] time: 0:08:41.077394\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4558/12662], [Dloss: 0.474124] [G loss: 0.268155] time: 0:08:42.360961\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4559/12662], [Dloss: 0.135918] [G loss: 0.127311] time: 0:08:43.837014\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4560/12662], [Dloss: 0.192108] [G loss: 0.094635] time: 0:08:45.288133\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4561/12662], [Dloss: -0.019537] [G loss: 0.084996] time: 0:08:46.609599\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4562/12662], [Dloss: 0.051055] [G loss: 0.190829] time: 0:08:48.087646\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4563/12662], [Dloss: 0.209932] [G loss: 0.085763] time: 0:08:49.466958\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4564/12662], [Dloss: 0.466636] [G loss: -0.057694] time: 0:08:50.871202\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4565/12662], [Dloss: 0.012522] [G loss: 0.379750] time: 0:08:52.277441\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4566/12662], [Dloss: 0.209959] [G loss: -0.174814] time: 0:08:53.628827\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4567/12662], [Dloss: 0.409045] [G loss: 0.055431] time: 0:08:55.010133\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4568/12662], [Dloss: 0.358903] [G loss: -0.328547] time: 0:08:56.376479\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4569/12662], [Dloss: 0.182339] [G loss: 0.055996] time: 0:08:57.794686\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4570/12662], [Dloss: 0.420975] [G loss: -0.079360] time: 0:08:59.190952\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4571/12662], [Dloss: 0.053611] [G loss: -0.028578] time: 0:09:00.579239\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4572/12662], [Dloss: 0.454991] [G loss: 0.100727] time: 0:09:01.966530\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4573/12662], [Dloss: -0.030012] [G loss: -0.127529] time: 0:09:03.340854\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4574/12662], [Dloss: 0.189174] [G loss: -0.004788] time: 0:09:04.691242\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4575/12662], [Dloss: 0.516196] [G loss: 0.022282] time: 0:09:06.040633\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4576/12662], [Dloss: 0.177504] [G loss: -0.255482] time: 0:09:07.434905\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4577/12662], [Dloss: 0.446007] [G loss: 0.305363] time: 0:09:08.869070\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4578/12662], [Dloss: 0.162038] [G loss: 0.173433] time: 0:09:10.242396\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4579/12662], [Dloss: -0.187916] [G loss: 0.178163] time: 0:09:11.567852\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4580/12662], [Dloss: 0.172250] [G loss: 0.226343] time: 0:09:12.980075\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4581/12662], [Dloss: 0.534273] [G loss: 0.165769] time: 0:09:14.453135\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4582/12662], [Dloss: 0.311765] [G loss: 0.616177] time: 0:09:15.860372\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4583/12662], [Dloss: 0.431430] [G loss: 0.337779] time: 0:09:17.251651\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4584/12662], [Dloss: 0.519022] [G loss: 0.320346] time: 0:09:18.638941\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4585/12662], [Dloss: 0.108024] [G loss: 0.338350] time: 0:09:20.049170\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4586/12662], [Dloss: 0.123203] [G loss: 0.520216] time: 0:09:21.460395\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4587/12662], [Dloss: 0.123599] [G loss: 0.347680] time: 0:09:22.747952\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4588/12662], [Dloss: 0.464099] [G loss: 0.097150] time: 0:09:24.116293\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4589/12662], [Dloss: 0.133473] [G loss: 0.249756] time: 0:09:25.434767\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4590/12662], [Dloss: 0.489585] [G loss: 0.043374] time: 0:09:26.816072\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4591/12662], [Dloss: 0.176136] [G loss: 0.371758] time: 0:09:28.232285\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4592/12662], [Dloss: 0.291495] [G loss: 0.144109] time: 0:09:29.606610\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4593/12662], [Dloss: 0.316347] [G loss: 0.159426] time: 0:09:30.999883\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4594/12662], [Dloss: 0.170262] [G loss: 0.316525] time: 0:09:32.338304\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4595/12662], [Dloss: 0.135649] [G loss: 0.494563] time: 0:09:33.684704\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4596/12662], [Dloss: 0.203263] [G loss: 0.271521] time: 0:09:35.044068\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4597/12662], [Dloss: 0.229840] [G loss: 0.509273] time: 0:09:36.363539\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4598/12662], [Dloss: 0.240606] [G loss: 0.276649] time: 0:09:37.754818\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4599/12662], [Dloss: 0.290664] [G loss: 0.030925] time: 0:09:39.133132\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4600/12662], [Dloss: 0.285385] [G loss: -0.037685] time: 0:09:40.505462\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 4601/12662], [Dloss: 0.452809] [G loss: 0.194029] time: 0:09:41.909707\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4602/12662], [Dloss: 0.657663] [G loss: 0.054210] time: 0:09:43.295002\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4603/12662], [Dloss: -0.139797] [G loss: -0.040429] time: 0:09:44.615984\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4604/12662], [Dloss: 0.423050] [G loss: -0.252758] time: 0:09:46.029719\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4605/12662], [Dloss: -0.038899] [G loss: 0.165502] time: 0:09:47.427980\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4606/12662], [Dloss: 0.026822] [G loss: -0.343616] time: 0:09:48.803302\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4607/12662], [Dloss: 0.178697] [G loss: 0.224903] time: 0:09:50.198571\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4608/12662], [Dloss: 0.351736] [G loss: 0.394517] time: 0:09:51.553946\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4609/12662], [Dloss: 0.554252] [G loss: 0.355522] time: 0:09:52.933257\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4610/12662], [Dloss: 0.321569] [G loss: 0.407620] time: 0:09:54.257715\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4611/12662], [Dloss: 0.317133] [G loss: 0.097267] time: 0:09:55.559234\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4612/12662], [Dloss: 0.512915] [G loss: 0.026639] time: 0:09:56.876711\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4613/12662], [Dloss: 0.164747] [G loss: 0.251629] time: 0:09:58.230091\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4614/12662], [Dloss: 0.327694] [G loss: 0.253289] time: 0:09:59.527621\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4615/12662], [Dloss: 0.464612] [G loss: 0.109993] time: 0:10:00.904938\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4616/12662], [Dloss: 0.424654] [G loss: 0.492250] time: 0:10:02.253332\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4617/12662], [Dloss: 0.001147] [G loss: 0.172837] time: 0:10:03.637629\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4618/12662], [Dloss: 0.242010] [G loss: 0.189842] time: 0:10:04.948125\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4619/12662], [Dloss: 0.210100] [G loss: 0.308109] time: 0:10:06.305495\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4620/12662], [Dloss: 0.586266] [G loss: -0.623864] time: 0:10:07.636934\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4621/12662], [Dloss: 0.145187] [G loss: -0.234947] time: 0:10:08.992309\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4622/12662], [Dloss: 0.374115] [G loss: -0.177080] time: 0:10:10.317765\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4623/12662], [Dloss: 0.219087] [G loss: -0.060679] time: 0:10:11.608313\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4624/12662], [Dloss: 0.304003] [G loss: -0.241180] time: 0:10:12.945736\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4625/12662], [Dloss: 0.468535] [G loss: -0.001313] time: 0:10:14.277176\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4626/12662], [Dloss: 0.500876] [G loss: 0.097506] time: 0:10:15.845980\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4627/12662], [Dloss: 0.356155] [G loss: 0.489531] time: 0:10:17.203350\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4628/12662], [Dloss: 0.095825] [G loss: 0.303474] time: 0:10:18.571691\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4629/12662], [Dloss: 0.337585] [G loss: 0.154158] time: 0:10:20.027797\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4630/12662], [Dloss: 0.357848] [G loss: 0.218813] time: 0:10:21.379182\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4631/12662], [Dloss: 0.374655] [G loss: 0.209972] time: 0:10:22.799384\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4632/12662], [Dloss: 0.454958] [G loss: -0.290196] time: 0:10:24.208616\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4633/12662], [Dloss: 0.331783] [G loss: -0.126868] time: 0:10:25.725559\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4634/12662], [Dloss: 0.278778] [G loss: -0.300301] time: 0:10:27.092902\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4635/12662], [Dloss: -0.016361] [G loss: -0.270265] time: 0:10:28.496149\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4636/12662], [Dloss: 0.064949] [G loss: 0.107287] time: 0:10:29.904383\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4637/12662], [Dloss: 0.059346] [G loss: -0.130774] time: 0:10:31.253775\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4638/12662], [Dloss: 0.379907] [G loss: -0.256654] time: 0:10:32.618125\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4639/12662], [Dloss: 0.374237] [G loss: -0.101784] time: 0:10:33.952557\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4640/12662], [Dloss: 0.319441] [G loss: 0.007061] time: 0:10:35.394700\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4641/12662], [Dloss: 0.391635] [G loss: 0.042479] time: 0:10:36.727136\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4642/12662], [Dloss: -0.132046] [G loss: 0.143110] time: 0:10:38.140357\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4643/12662], [Dloss: 0.220812] [G loss: 0.064530] time: 0:10:39.586490\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4644/12662], [Dloss: 0.336458] [G loss: 0.113285] time: 0:10:41.039603\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4645/12662], [Dloss: 0.055291] [G loss: 0.167256] time: 0:10:42.402957\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4646/12662], [Dloss: 0.253386] [G loss: 0.030946] time: 0:10:43.855074\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4647/12662], [Dloss: 0.136981] [G loss: 0.300223] time: 0:10:45.196486\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4648/12662], [Dloss: 0.453171] [G loss: 0.490658] time: 0:10:46.529920\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4649/12662], [Dloss: 0.075787] [G loss: 0.231340] time: 0:10:47.928181\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4650/12662], [Dloss: 0.375646] [G loss: -0.098650] time: 0:10:49.352372\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4651/12662], [Dloss: 0.368256] [G loss: -0.162845] time: 0:10:50.687801\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4652/12662], [Dloss: 0.066224] [G loss: -0.020048] time: 0:10:52.093042\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4653/12662], [Dloss: 0.130687] [G loss: -0.023641] time: 0:10:53.518231\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4654/12662], [Dloss: -0.019316] [G loss: 0.145568] time: 0:10:54.899537\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4655/12662], [Dloss: -0.102290] [G loss: 0.139953] time: 0:10:56.226987\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4656/12662], [Dloss: 0.275723] [G loss: 0.288474] time: 0:10:57.552442\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4657/12662], [Dloss: 0.507826] [G loss: 0.261763] time: 0:10:58.908814\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4658/12662], [Dloss: 0.150298] [G loss: 0.375063] time: 0:11:00.299096\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4659/12662], [Dloss: -0.267153] [G loss: 0.103369] time: 0:11:01.706333\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4660/12662], [Dloss: 0.354838] [G loss: -0.074973] time: 0:11:03.070684\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4661/12662], [Dloss: 0.214394] [G loss: -0.121545] time: 0:11:04.466950\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4662/12662], [Dloss: 0.522295] [G loss: 0.193186] time: 0:11:05.852246\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4663/12662], [Dloss: -0.073318] [G loss: 0.255141] time: 0:11:07.158751\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4664/12662], [Dloss: 0.619564] [G loss: 0.127512] time: 0:11:08.438329\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4665/12662], [Dloss: 0.470860] [G loss: -0.135215] time: 0:11:09.824622\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4666/12662], [Dloss: 0.492947] [G loss: 0.157265] time: 0:11:11.242829\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4667/12662], [Dloss: 0.533883] [G loss: -0.038298] time: 0:11:12.613164\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 4668/12662], [Dloss: 0.559711] [G loss: -0.216231] time: 0:11:13.990481\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4669/12662], [Dloss: 0.407958] [G loss: -0.119224] time: 0:11:15.315936\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4670/12662], [Dloss: 0.385300] [G loss: -0.191666] time: 0:11:16.640394\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4671/12662], [Dloss: 0.490353] [G loss: -0.564996] time: 0:11:18.115449\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4672/12662], [Dloss: 0.357948] [G loss: -0.249402] time: 0:11:19.401011\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4673/12662], [Dloss: 0.615813] [G loss: -0.061075] time: 0:11:20.803261\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4674/12662], [Dloss: 0.449077] [G loss: 0.355442] time: 0:11:22.193543\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4675/12662], [Dloss: 0.342799] [G loss: 0.246545] time: 0:11:23.560886\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4676/12662], [Dloss: 0.403864] [G loss: 0.011109] time: 0:11:24.870384\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4677/12662], [Dloss: 0.329682] [G loss: -0.046075] time: 0:11:26.260666\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4678/12662], [Dloss: 0.538384] [G loss: 0.291983] time: 0:11:27.703807\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4679/12662], [Dloss: 0.385867] [G loss: 0.344850] time: 0:11:29.074142\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4680/12662], [Dloss: 0.337795] [G loss: 0.223667] time: 0:11:30.419544\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4681/12662], [Dloss: -0.022912] [G loss: 0.335075] time: 0:11:31.856700\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4682/12662], [Dloss: 0.376621] [G loss: 0.084743] time: 0:11:33.177169\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4683/12662], [Dloss: 0.424439] [G loss: -0.178725] time: 0:11:34.557477\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4684/12662], [Dloss: 0.244439] [G loss: -0.255310] time: 0:11:35.946762\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4685/12662], [Dloss: 0.031806] [G loss: -0.249604] time: 0:11:37.284185\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4686/12662], [Dloss: 0.309219] [G loss: -0.156742] time: 0:11:38.672472\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4687/12662], [Dloss: 0.422915] [G loss: -0.313553] time: 0:11:39.960029\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4688/12662], [Dloss: 0.512193] [G loss: -0.654571] time: 0:11:41.308423\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4689/12662], [Dloss: 0.482889] [G loss: -0.406270] time: 0:11:42.656323\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4690/12662], [Dloss: 0.694931] [G loss: 0.049944] time: 0:11:44.005726\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4691/12662], [Dloss: 0.269221] [G loss: 0.324021] time: 0:11:45.357134\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4692/12662], [Dloss: 0.344458] [G loss: 0.746076] time: 0:11:46.735448\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4693/12662], [Dloss: 0.787122] [G loss: 0.336722] time: 0:11:48.053922\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4694/12662], [Dloss: 0.440867] [G loss: 0.547157] time: 0:11:49.416278\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4695/12662], [Dloss: 0.277533] [G loss: 0.294235] time: 0:11:50.781627\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4696/12662], [Dloss: 0.410665] [G loss: 0.350685] time: 0:11:52.034277\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4697/12662], [Dloss: 0.218921] [G loss: 0.502670] time: 0:11:53.397631\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4698/12662], [Dloss: 0.241020] [G loss: 0.082520] time: 0:11:54.760985\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4699/12662], [Dloss: 0.191638] [G loss: 0.247539] time: 0:11:56.156253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4700/12662], [Dloss: 0.254483] [G loss: -0.404291] time: 0:11:57.491682\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4701/12662], [Dloss: 0.050989] [G loss: -0.198657] time: 0:11:58.866006\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4702/12662], [Dloss: 0.396605] [G loss: -0.128998] time: 0:12:00.201435\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4703/12662], [Dloss: 0.356020] [G loss: -0.124975] time: 0:12:01.521904\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4704/12662], [Dloss: 0.533283] [G loss: -0.366716] time: 0:12:02.826415\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4705/12662], [Dloss: 0.299798] [G loss: -0.173238] time: 0:12:04.209715\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4706/12662], [Dloss: 0.639969] [G loss: 0.217781] time: 0:12:05.544147\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4707/12662], [Dloss: -0.014566] [G loss: 0.104288] time: 0:12:06.951383\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4708/12662], [Dloss: 0.566749] [G loss: 0.291221] time: 0:12:08.281825\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4709/12662], [Dloss: 0.233819] [G loss: 0.234481] time: 0:12:09.555419\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4710/12662], [Dloss: 0.329267] [G loss: -0.000956] time: 0:12:10.933733\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4711/12662], [Dloss: 0.214754] [G loss: 0.289921] time: 0:12:12.250212\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4712/12662], [Dloss: 0.117707] [G loss: 0.472033] time: 0:12:13.587635\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4713/12662], [Dloss: 0.480409] [G loss: -0.333587] time: 0:12:14.909102\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4714/12662], [Dloss: 0.271037] [G loss: 0.311580] time: 0:12:16.247522\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4715/12662], [Dloss: 0.520823] [G loss: -0.051100] time: 0:12:17.557020\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4716/12662], [Dloss: 0.492705] [G loss: -0.068120] time: 0:12:18.959270\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4717/12662], [Dloss: 0.309180] [G loss: -0.208773] time: 0:12:20.518101\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4718/12662], [Dloss: 0.357395] [G loss: -0.125057] time: 0:12:21.905391\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4719/12662], [Dloss: 0.525679] [G loss: 0.046977] time: 0:12:23.274729\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4720/12662], [Dloss: -0.024564] [G loss: -0.257479] time: 0:12:24.568270\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4721/12662], [Dloss: 0.332645] [G loss: 0.271137] time: 0:12:25.886743\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4722/12662], [Dloss: 0.288417] [G loss: -0.047375] time: 0:12:27.302956\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4723/12662], [Dloss: 0.081869] [G loss: -0.089988] time: 0:12:28.714182\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4724/12662], [Dloss: 0.451174] [G loss: -0.027322] time: 0:12:30.097483\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4725/12662], [Dloss: 0.372641] [G loss: 0.142522] time: 0:12:31.454852\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4726/12662], [Dloss: 0.350623] [G loss: -0.265874] time: 0:12:32.793273\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4727/12662], [Dloss: 0.095496] [G loss: -0.245934] time: 0:12:34.095790\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4728/12662], [Dloss: 0.336095] [G loss: 0.119836] time: 0:12:35.413266\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4729/12662], [Dloss: 0.927131] [G loss: 0.230297] time: 0:12:36.785596\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4730/12662], [Dloss: 0.248792] [G loss: -0.031568] time: 0:12:38.160918\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4731/12662], [Dloss: 0.263173] [G loss: -0.160602] time: 0:12:39.565163\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4732/12662], [Dloss: 0.400432] [G loss: -0.099565] time: 0:12:40.862692\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4733/12662], [Dloss: 0.526911] [G loss: -0.251641] time: 0:12:42.237017\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4734/12662], [Dloss: 0.432082] [G loss: 0.258363] time: 0:12:43.570451\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 4735/12662], [Dloss: 0.273101] [G loss: 0.317190] time: 0:12:44.922834\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4736/12662], [Dloss: 0.248592] [G loss: -0.082190] time: 0:12:46.223356\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4737/12662], [Dloss: 0.450229] [G loss: 0.031888] time: 0:12:47.577734\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4738/12662], [Dloss: 0.204565] [G loss: 0.125487] time: 0:12:48.941088\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4739/12662], [Dloss: 0.426868] [G loss: -0.315121] time: 0:12:50.249588\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4740/12662], [Dloss: 0.478620] [G loss: 0.026002] time: 0:12:51.653833\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4741/12662], [Dloss: 0.540658] [G loss: 0.332856] time: 0:12:53.037133\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4742/12662], [Dloss: 0.024321] [G loss: 0.354328] time: 0:12:54.434397\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4743/12662], [Dloss: 0.401677] [G loss: 0.245831] time: 0:12:55.758855\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4744/12662], [Dloss: 0.368689] [G loss: 0.514458] time: 0:12:57.137169\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4745/12662], [Dloss: 0.283760] [G loss: 0.244336] time: 0:12:58.481574\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4746/12662], [Dloss: 0.425540] [G loss: 0.185258] time: 0:12:59.886815\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4747/12662], [Dloss: 0.210293] [G loss: 0.034956] time: 0:13:01.229225\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4748/12662], [Dloss: 0.186853] [G loss: 0.270695] time: 0:13:02.691315\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4749/12662], [Dloss: 0.240975] [G loss: -0.227905] time: 0:13:04.066637\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4750/12662], [Dloss: 0.452011] [G loss: -0.437688] time: 0:13:05.540695\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4751/12662], [Dloss: -0.067585] [G loss: 0.234092] time: 0:13:07.063622\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4752/12662], [Dloss: 0.294374] [G loss: -0.253081] time: 0:13:08.471856\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4753/12662], [Dloss: 0.216186] [G loss: 0.206402] time: 0:13:09.829226\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4754/12662], [Dloss: 0.041455] [G loss: 0.332885] time: 0:13:11.243443\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4755/12662], [Dloss: 0.161005] [G loss: 0.651092] time: 0:13:12.696558\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4756/12662], [Dloss: 0.071641] [G loss: 0.525438] time: 0:13:14.382050\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4757/12662], [Dloss: 0.470780] [G loss: 0.504564] time: 0:13:16.112422\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4758/12662], [Dloss: 0.366319] [G loss: 0.463948] time: 0:13:17.635349\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4759/12662], [Dloss: 0.504808] [G loss: 0.178403] time: 0:13:19.187199\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4760/12662], [Dloss: 0.359857] [G loss: 0.200905] time: 0:13:20.700154\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4761/12662], [Dloss: 0.324887] [G loss: 0.117925] time: 0:13:21.985715\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4762/12662], [Dloss: -0.078939] [G loss: 0.442175] time: 0:13:23.380984\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4763/12662], [Dloss: 0.397912] [G loss: 0.049147] time: 0:13:24.875985\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4764/12662], [Dloss: 0.299364] [G loss: 0.060802] time: 0:13:26.341067\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4765/12662], [Dloss: 0.185251] [G loss: 0.031501] time: 0:13:27.814127\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4766/12662], [Dloss: 0.195105] [G loss: 0.031416] time: 0:13:29.195433\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4767/12662], [Dloss: 0.029246] [G loss: 0.108488] time: 0:13:30.570755\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4768/12662], [Dloss: 0.347162] [G loss: -0.257244] time: 0:13:31.924136\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4769/12662], [Dloss: 0.063365] [G loss: 0.258911] time: 0:13:33.283500\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4770/12662], [Dloss: 0.515657] [G loss: 0.037356] time: 0:13:34.693729\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4771/12662], [Dloss: 0.505874] [G loss: 0.173207] time: 0:13:36.013200\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4772/12662], [Dloss: -0.054795] [G loss: 0.067666] time: 0:13:37.266847\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4773/12662], [Dloss: -0.169368] [G loss: 0.142095] time: 0:13:38.585321\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4774/12662], [Dloss: -0.015935] [G loss: -0.013070] time: 0:13:39.921747\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4775/12662], [Dloss: 0.220988] [G loss: -0.297977] time: 0:13:41.274131\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4776/12662], [Dloss: 0.371279] [G loss: -0.531343] time: 0:13:42.766140\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4777/12662], [Dloss: 0.694142] [G loss: -0.801513] time: 0:13:44.134996\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4778/12662], [Dloss: 0.090894] [G loss: -0.065812] time: 0:13:45.543245\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4779/12662], [Dloss: 0.422388] [G loss: 0.251146] time: 0:13:46.887167\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4780/12662], [Dloss: 0.279237] [G loss: 0.057880] time: 0:13:48.211625\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4781/12662], [Dloss: 0.464805] [G loss: 0.362763] time: 0:13:49.537080\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4782/12662], [Dloss: 0.300048] [G loss: 0.803005] time: 0:13:50.802696\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4783/12662], [Dloss: 0.704755] [G loss: 0.604106] time: 0:13:52.182007\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4784/12662], [Dloss: 0.464650] [G loss: -0.124694] time: 0:13:53.685985\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4785/12662], [Dloss: 0.071334] [G loss: 0.164235] time: 0:13:55.196944\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4786/12662], [Dloss: 0.260623] [G loss: 0.333544] time: 0:13:56.538357\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4787/12662], [Dloss: 0.659372] [G loss: 0.276184] time: 0:13:57.857828\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4788/12662], [Dloss: 0.329318] [G loss: 0.179802] time: 0:13:59.240131\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4789/12662], [Dloss: 0.543855] [G loss: 0.092993] time: 0:14:00.612462\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4790/12662], [Dloss: 0.473080] [G loss: -0.084907] time: 0:14:01.999751\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4791/12662], [Dloss: 0.508292] [G loss: -0.138911] time: 0:14:03.346164\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4792/12662], [Dloss: 0.055372] [G loss: -0.062752] time: 0:14:04.747402\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4793/12662], [Dloss: 0.190590] [G loss: 0.255514] time: 0:14:06.097791\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4794/12662], [Dloss: 0.312778] [G loss: 0.272725] time: 0:14:07.449177\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4795/12662], [Dloss: 0.171230] [G loss: 0.721685] time: 0:14:08.793581\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4796/12662], [Dloss: 0.063634] [G loss: 0.278525] time: 0:14:10.166909\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4797/12662], [Dloss: 0.243155] [G loss: 0.156634] time: 0:14:11.556194\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4798/12662], [Dloss: 0.154675] [G loss: 0.264139] time: 0:14:12.793883\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4799/12662], [Dloss: 0.324864] [G loss: 0.267965] time: 0:14:14.147264\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4800/12662], [Dloss: 0.415782] [G loss: 0.296077] time: 0:14:15.548516\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4801/12662], [Dloss: 0.443513] [G loss: -0.025418] time: 0:14:16.814132\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 4802/12662], [Dloss: 0.590195] [G loss: -0.234481] time: 0:14:18.259267\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4803/12662], [Dloss: 0.182810] [G loss: 0.191508] time: 0:14:19.627607\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4804/12662], [Dloss: 0.374313] [G loss: -0.136473] time: 0:14:21.004924\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4805/12662], [Dloss: 0.322477] [G loss: -0.032263] time: 0:14:22.348331\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4806/12662], [Dloss: 0.329093] [G loss: 0.168913] time: 0:14:23.703707\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4807/12662], [Dloss: 0.260488] [G loss: 0.109385] time: 0:14:25.070053\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4808/12662], [Dloss: 0.524506] [G loss: 0.556286] time: 0:14:26.404484\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4809/12662], [Dloss: -0.210494] [G loss: 0.134071] time: 0:14:27.762851\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4810/12662], [Dloss: 0.416749] [G loss: 0.373176] time: 0:14:29.137176\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4811/12662], [Dloss: 0.468596] [G loss: 0.383236] time: 0:14:30.541420\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4812/12662], [Dloss: 0.182294] [G loss: 0.289063] time: 0:14:31.869867\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4813/12662], [Dloss: 0.123918] [G loss: 0.304088] time: 0:14:33.175376\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4814/12662], [Dloss: 0.322631] [G loss: 0.113590] time: 0:14:34.502826\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4815/12662], [Dloss: 0.357076] [G loss: -0.114773] time: 0:14:35.903081\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4816/12662], [Dloss: 0.146343] [G loss: 0.280230] time: 0:14:37.282392\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4817/12662], [Dloss: 0.295084] [G loss: -0.050682] time: 0:14:38.722541\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4818/12662], [Dloss: 0.644741] [G loss: 0.021906] time: 0:14:40.121812\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4819/12662], [Dloss: 0.459732] [G loss: 0.365093] time: 0:14:41.610366\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4820/12662], [Dloss: 0.398042] [G loss: 0.348528] time: 0:14:43.016124\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4821/12662], [Dloss: 0.309284] [G loss: 0.450152] time: 0:14:44.507137\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4822/12662], [Dloss: 0.255452] [G loss: 0.424880] time: 0:14:45.761781\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4823/12662], [Dloss: 0.321634] [G loss: 0.165015] time: 0:14:47.176997\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4824/12662], [Dloss: 0.045201] [G loss: 0.241508] time: 0:14:48.530377\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4825/12662], [Dloss: 0.086831] [G loss: 0.581189] time: 0:14:49.887747\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4826/12662], [Dloss: 0.318423] [G loss: 0.383393] time: 0:14:51.292989\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4827/12662], [Dloss: 0.433185] [G loss: -0.021219] time: 0:14:52.574561\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4828/12662], [Dloss: 0.382838] [G loss: 0.153043] time: 0:14:53.936918\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4829/12662], [Dloss: -0.044329] [G loss: 0.074359] time: 0:14:55.271349\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4830/12662], [Dloss: 0.521726] [G loss: 0.314487] time: 0:14:56.627722\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4831/12662], [Dloss: 0.301427] [G loss: 0.081558] time: 0:14:57.927246\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4832/12662], [Dloss: 0.112288] [G loss: 0.287892] time: 0:14:59.235747\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4833/12662], [Dloss: 0.298225] [G loss: 0.311042] time: 0:15:00.615058\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4834/12662], [Dloss: 0.362870] [G loss: 0.356565] time: 0:15:01.954476\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4835/12662], [Dloss: 0.113233] [G loss: 0.194185] time: 0:15:03.342763\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4836/12662], [Dloss: 0.484346] [G loss: 0.206717] time: 0:15:04.667221\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4837/12662], [Dloss: -0.141995] [G loss: 0.305368] time: 0:15:05.992677\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4838/12662], [Dloss: 0.602420] [G loss: 0.664890] time: 0:15:07.363012\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4839/12662], [Dloss: 0.204154] [G loss: 0.224677] time: 0:15:08.669518\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4840/12662], [Dloss: 0.017673] [G loss: 0.601129] time: 0:15:10.097698\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4841/12662], [Dloss: 0.350456] [G loss: 0.112352] time: 0:15:11.439111\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4842/12662], [Dloss: 0.214434] [G loss: 0.065020] time: 0:15:12.769553\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4843/12662], [Dloss: 0.069746] [G loss: 0.092673] time: 0:15:14.095008\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4844/12662], [Dloss: 0.337037] [G loss: -0.530296] time: 0:15:15.475317\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4845/12662], [Dloss: 0.424927] [G loss: -0.562265] time: 0:15:16.847646\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4846/12662], [Dloss: 0.236348] [G loss: 0.074834] time: 0:15:18.239923\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4847/12662], [Dloss: 0.358340] [G loss: -0.111018] time: 0:15:19.617239\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4848/12662], [Dloss: 0.404695] [G loss: 0.050790] time: 0:15:20.952668\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4849/12662], [Dloss: 0.513921] [G loss: -0.171451] time: 0:15:22.358908\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4850/12662], [Dloss: 0.090836] [G loss: 0.267690] time: 0:15:23.746197\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4851/12662], [Dloss: 0.184242] [G loss: 0.330621] time: 0:15:25.154431\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4852/12662], [Dloss: 0.046258] [G loss: 0.290892] time: 0:15:26.636469\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4853/12662], [Dloss: 0.203714] [G loss: 0.687061] time: 0:15:28.010792\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4854/12662], [Dloss: 0.088868] [G loss: 0.185596] time: 0:15:29.387111\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4855/12662], [Dloss: 0.078818] [G loss: 0.234267] time: 0:15:30.828257\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4856/12662], [Dloss: 0.435859] [G loss: 0.111049] time: 0:15:32.223526\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4857/12662], [Dloss: 0.179401] [G loss: -0.018520] time: 0:15:33.549978\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4858/12662], [Dloss: 0.142675] [G loss: -0.095949] time: 0:15:34.918319\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4859/12662], [Dloss: 0.479988] [G loss: 0.121503] time: 0:15:36.459199\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4860/12662], [Dloss: 0.449447] [G loss: -0.302958] time: 0:15:38.943554\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4861/12662], [Dloss: 0.423301] [G loss: -0.263625] time: 0:15:40.287959\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4862/12662], [Dloss: 0.531968] [G loss: 0.148771] time: 0:15:41.876710\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4863/12662], [Dloss: 0.286294] [G loss: 0.404369] time: 0:15:50.627348\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4864/12662], [Dloss: 0.169968] [G loss: 0.422212] time: 0:15:56.289206\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4865/12662], [Dloss: 0.342840] [G loss: 0.476033] time: 0:15:58.761594\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4866/12662], [Dloss: 0.603454] [G loss: 0.560137] time: 0:16:00.157860\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4867/12662], [Dloss: 0.436985] [G loss: 0.468220] time: 0:16:01.445416\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4868/12662], [Dloss: 0.263666] [G loss: 0.326774] time: 0:16:02.830712\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 4869/12662], [Dloss: 0.083736] [G loss: -0.091302] time: 0:16:04.173121\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4870/12662], [Dloss: 0.570206] [G loss: -0.020268] time: 0:16:05.459681\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4871/12662], [Dloss: 0.430092] [G loss: -0.193888] time: 0:16:06.828022\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4872/12662], [Dloss: -0.029322] [G loss: 0.001969] time: 0:16:08.134527\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4873/12662], [Dloss: 0.008761] [G loss: -0.252891] time: 0:16:09.432057\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4874/12662], [Dloss: 0.197327] [G loss: -0.279719] time: 0:16:10.757512\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4875/12662], [Dloss: 0.781513] [G loss: -0.134339] time: 0:16:12.105907\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4876/12662], [Dloss: 0.610174] [G loss: 0.168268] time: 0:16:13.470258\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4877/12662], [Dloss: 0.327015] [G loss: 0.095085] time: 0:16:14.742854\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4878/12662], [Dloss: 0.206344] [G loss: 0.413033] time: 0:16:16.104214\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4879/12662], [Dloss: 0.105791] [G loss: 1.024160] time: 0:16:17.525413\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4880/12662], [Dloss: 0.407691] [G loss: 0.515748] time: 0:16:19.012436\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4881/12662], [Dloss: 0.443515] [G loss: 0.305490] time: 0:16:20.364819\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4882/12662], [Dloss: 0.214858] [G loss: 0.645148] time: 0:16:21.770061\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4883/12662], [Dloss: 0.306611] [G loss: 0.613313] time: 0:16:23.023708\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4884/12662], [Dloss: 0.119550] [G loss: 0.326804] time: 0:16:24.419974\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4885/12662], [Dloss: 0.451982] [G loss: 0.054871] time: 0:16:25.784325\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4886/12662], [Dloss: 0.433540] [G loss: 0.150097] time: 0:16:27.148677\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4887/12662], [Dloss: 0.125972] [G loss: 0.308000] time: 0:16:28.597803\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4888/12662], [Dloss: 0.433517] [G loss: 0.276806] time: 0:16:30.248387\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4889/12662], [Dloss: 0.045865] [G loss: 0.253875] time: 0:16:31.579826\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4890/12662], [Dloss: 0.057681] [G loss: 0.158074] time: 0:16:32.947169\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4891/12662], [Dloss: 0.127343] [G loss: 0.117095] time: 0:16:34.313515\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4892/12662], [Dloss: 0.503775] [G loss: -0.246203] time: 0:16:35.617030\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4893/12662], [Dloss: -0.075375] [G loss: -0.430965] time: 0:16:37.048202\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4894/12662], [Dloss: 0.229540] [G loss: -0.489234] time: 0:16:38.451449\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4895/12662], [Dloss: 0.432677] [G loss: -0.415386] time: 0:16:39.819790\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4896/12662], [Dloss: 0.126495] [G loss: 0.140824] time: 0:16:41.208077\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4897/12662], [Dloss: 0.211000] [G loss: 0.379084] time: 0:16:42.657201\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4898/12662], [Dloss: -0.102991] [G loss: 0.527465] time: 0:16:43.983654\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4899/12662], [Dloss: 0.159492] [G loss: 0.476903] time: 0:16:45.332048\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4900/12662], [Dloss: 0.202478] [G loss: 0.113325] time: 0:16:46.674458\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4901/12662], [Dloss: 0.381637] [G loss: 0.261599] time: 0:16:48.025843\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4902/12662], [Dloss: 0.137495] [G loss: -0.139635] time: 0:16:49.336339\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4903/12662], [Dloss: 0.322842] [G loss: 0.099125] time: 0:16:50.674759\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4904/12662], [Dloss: 0.227255] [G loss: -0.448531] time: 0:16:52.015175\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4905/12662], [Dloss: 0.301733] [G loss: -0.263198] time: 0:16:53.329659\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4906/12662], [Dloss: 0.383894] [G loss: -0.653670] time: 0:16:54.634171\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4907/12662], [Dloss: 0.655664] [G loss: -0.390096] time: 0:16:55.909759\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4908/12662], [Dloss: 0.664965] [G loss: -0.331862] time: 0:16:57.147449\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4909/12662], [Dloss: 0.198575] [G loss: 0.275039] time: 0:16:58.476894\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4910/12662], [Dloss: 0.290084] [G loss: 0.557120] time: 0:16:59.781405\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4911/12662], [Dloss: 0.057360] [G loss: 0.462469] time: 0:17:01.118828\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4912/12662], [Dloss: 0.693028] [G loss: 0.495284] time: 0:17:02.477196\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4913/12662], [Dloss: 0.764725] [G loss: 0.305855] time: 0:17:03.997132\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4914/12662], [Dloss: 0.485533] [G loss: 0.561004] time: 0:17:05.743460\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4915/12662], [Dloss: 0.496758] [G loss: 0.499027] time: 0:17:07.107319\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4916/12662], [Dloss: 0.443599] [G loss: 0.089220] time: 0:17:08.402854\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4917/12662], [Dloss: 0.647764] [G loss: 0.050385] time: 0:17:09.760729\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4918/12662], [Dloss: 0.169235] [G loss: 0.257410] time: 0:17:11.074215\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4919/12662], [Dloss: 0.539710] [G loss: 0.012827] time: 0:17:12.464539\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4920/12662], [Dloss: 0.490352] [G loss: 0.146414] time: 0:17:13.806949\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4921/12662], [Dloss: 0.591403] [G loss: -0.141082] time: 0:17:15.211697\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4922/12662], [Dloss: 0.197334] [G loss: -0.063705] time: 0:17:16.577046\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4923/12662], [Dloss: 0.509699] [G loss: -0.212869] time: 0:17:17.910480\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4924/12662], [Dloss: 0.573531] [G loss: 0.219007] time: 0:17:19.283807\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4925/12662], [Dloss: 0.350677] [G loss: -0.099401] time: 0:17:20.560393\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4926/12662], [Dloss: 0.343349] [G loss: -0.415025] time: 0:17:22.058891\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4927/12662], [Dloss: 0.329909] [G loss: 0.199980] time: 0:17:23.388335\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4928/12662], [Dloss: 0.595813] [G loss: 0.216453] time: 0:17:24.703749\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4929/12662], [Dloss: 0.312625] [G loss: 0.232688] time: 0:17:26.091039\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4930/12662], [Dloss: 0.540379] [G loss: 0.148513] time: 0:17:27.398543\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4931/12662], [Dloss: 0.113662] [G loss: -0.141306] time: 0:17:28.723001\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4932/12662], [Dloss: -0.073105] [G loss: 0.526163] time: 0:17:30.095340\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4933/12662], [Dloss: 0.390397] [G loss: 0.506611] time: 0:17:31.390866\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4934/12662], [Dloss: 0.186818] [G loss: 0.192116] time: 0:17:32.818049\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4935/12662], [Dloss: 0.297060] [G loss: -0.066460] time: 0:17:34.148491\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 4936/12662], [Dloss: -0.005801] [G loss: 0.306602] time: 0:17:35.517829\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4937/12662], [Dloss: 0.615669] [G loss: -0.049396] time: 0:17:36.863231\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4938/12662], [Dloss: 0.097055] [G loss: 0.306002] time: 0:17:38.232569\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4939/12662], [Dloss: 0.104650] [G loss: 0.291151] time: 0:17:39.553037\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4940/12662], [Dloss: 0.399195] [G loss: -0.414776] time: 0:17:40.922386\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4941/12662], [Dloss: 0.167328] [G loss: -0.246505] time: 0:17:42.182007\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4942/12662], [Dloss: 0.248731] [G loss: 0.344605] time: 0:17:43.512468\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4943/12662], [Dloss: 0.394791] [G loss: -0.069332] time: 0:17:44.780584\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4944/12662], [Dloss: -0.027251] [G loss: 0.158107] time: 0:17:46.117523\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4945/12662], [Dloss: 0.469844] [G loss: 0.325724] time: 0:17:47.376157\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4946/12662], [Dloss: -0.054742] [G loss: 0.480807] time: 0:17:48.716572\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4947/12662], [Dloss: 0.406036] [G loss: 0.658963] time: 0:17:50.032558\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4948/12662], [Dloss: -0.119752] [G loss: 0.717304] time: 0:17:51.350035\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4949/12662], [Dloss: 0.244926] [G loss: 0.624074] time: 0:17:52.651555\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4950/12662], [Dloss: 0.245793] [G loss: 0.385292] time: 0:17:53.996956\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4951/12662], [Dloss: 0.099289] [G loss: 0.403889] time: 0:17:55.269553\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4952/12662], [Dloss: 0.052291] [G loss: 0.324602] time: 0:17:56.658837\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4953/12662], [Dloss: 0.122084] [G loss: 0.395613] time: 0:17:57.947391\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4954/12662], [Dloss: -0.062933] [G loss: 0.628336] time: 0:17:59.334681\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4955/12662], [Dloss: 0.267911] [G loss: 0.452478] time: 0:18:00.661134\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4956/12662], [Dloss: -0.101815] [G loss: 0.296784] time: 0:18:02.012520\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4957/12662], [Dloss: 0.596121] [G loss: 0.242603] time: 0:18:03.277138\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4958/12662], [Dloss: 0.603793] [G loss: -0.042368] time: 0:18:04.592619\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4959/12662], [Dloss: 0.281710] [G loss: 0.322518] time: 0:18:05.853248\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4960/12662], [Dloss: -0.116768] [G loss: 0.072049] time: 0:18:07.185685\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4961/12662], [Dloss: 0.169752] [G loss: 0.113248] time: 0:18:08.435343\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4962/12662], [Dloss: 0.378667] [G loss: -0.162851] time: 0:18:09.795705\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4963/12662], [Dloss: 0.492807] [G loss: -0.375018] time: 0:18:11.104710\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4964/12662], [Dloss: 0.024072] [G loss: 0.063801] time: 0:18:12.464074\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4965/12662], [Dloss: 0.230479] [G loss: -0.182001] time: 0:18:13.729690\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4966/12662], [Dloss: 0.236440] [G loss: 0.075275] time: 0:18:15.058137\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4967/12662], [Dloss: 0.351516] [G loss: -0.267942] time: 0:18:16.408525\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4968/12662], [Dloss: 0.474448] [G loss: -0.011771] time: 0:18:17.761906\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4969/12662], [Dloss: 0.372399] [G loss: -0.101977] time: 0:18:19.063425\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4970/12662], [Dloss: 0.339423] [G loss: -0.007502] time: 0:18:20.424784\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4971/12662], [Dloss: 0.605861] [G loss: 0.153011] time: 0:18:21.782154\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4972/12662], [Dloss: 0.578480] [G loss: 0.188387] time: 0:18:23.046772\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4973/12662], [Dloss: 0.233742] [G loss: 0.216240] time: 0:18:24.279550\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4974/12662], [Dloss: 0.143376] [G loss: 0.389400] time: 0:18:25.604008\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4975/12662], [Dloss: 0.131627] [G loss: -0.007523] time: 0:18:26.958386\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4976/12662], [Dloss: 0.033427] [G loss: 0.264033] time: 0:18:28.299798\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4977/12662], [Dloss: 0.216610] [G loss: 0.303457] time: 0:18:29.604309\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4978/12662], [Dloss: 0.035057] [G loss: 0.021704] time: 0:18:30.990602\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4979/12662], [Dloss: 0.751134] [G loss: -0.061505] time: 0:18:32.268185\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4980/12662], [Dloss: 0.082431] [G loss: -0.002781] time: 0:18:33.611592\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4981/12662], [Dloss: 0.344016] [G loss: 0.087830] time: 0:18:35.010851\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4982/12662], [Dloss: 0.271740] [G loss: -0.301539] time: 0:18:36.388167\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4983/12662], [Dloss: 0.219472] [G loss: 0.070234] time: 0:18:37.735563\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4984/12662], [Dloss: 0.373800] [G loss: 0.251418] time: 0:18:39.087947\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4985/12662], [Dloss: 0.302957] [G loss: 0.265667] time: 0:18:40.422378\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4986/12662], [Dloss: 0.335217] [G loss: 0.250838] time: 0:18:41.744841\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4987/12662], [Dloss: 0.106140] [G loss: 0.543786] time: 0:18:43.152077\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4988/12662], [Dloss: -0.003313] [G loss: 0.700760] time: 0:18:44.624152\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4989/12662], [Dloss: 0.501340] [G loss: 0.349146] time: 0:18:46.107694\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4990/12662], [Dloss: 0.494859] [G loss: 0.414856] time: 0:18:47.569784\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4991/12662], [Dloss: -0.066483] [G loss: 0.025378] time: 0:18:48.957074\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4992/12662], [Dloss: 0.063546] [G loss: 0.203910] time: 0:18:50.356332\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4993/12662], [Dloss: 0.317063] [G loss: 0.137771] time: 0:18:51.695750\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4994/12662], [Dloss: 0.388000] [G loss: -0.022748] time: 0:18:53.094010\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4995/12662], [Dloss: 0.381210] [G loss: -0.306605] time: 0:18:54.458362\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4996/12662], [Dloss: 0.690870] [G loss: -0.386333] time: 0:18:55.851635\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4997/12662], [Dloss: 0.304560] [G loss: 0.150861] time: 0:18:57.171106\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4998/12662], [Dloss: 0.244107] [G loss: 0.020052] time: 0:18:58.639182\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 4999/12662], [Dloss: 0.388623] [G loss: 0.066579] time: 0:18:59.971617\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5000/12662], [Dloss: 0.559227] [G loss: 0.449224] time: 0:19:01.342950\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5000/12662], [Dloss: 0.559227] [G loss: 0.449224] time: 0:19:01.342950\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5001/12662], [Dloss: -0.073333] [G loss: 0.544405] time: 0:19:03.529103\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5002/12662], [Dloss: 0.050136] [G loss: 0.473311] time: 0:19:04.907417\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5003/12662], [Dloss: 0.497568] [G loss: -0.013998] time: 0:19:06.233869\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5004/12662], [Dloss: 0.290754] [G loss: 0.027957] time: 0:19:07.573791\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5005/12662], [Dloss: 0.789815] [G loss: 0.287167] time: 0:19:08.942729\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5006/12662], [Dloss: 0.209755] [G loss: 0.569645] time: 0:19:10.261233\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5007/12662], [Dloss: 0.026147] [G loss: 0.362408] time: 0:19:11.710368\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5008/12662], [Dloss: 0.462260] [G loss: -0.126311] time: 0:19:13.067738\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5009/12662], [Dloss: 0.673163] [G loss: -0.275581] time: 0:19:14.450041\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5010/12662], [Dloss: 0.364314] [G loss: -0.329248] time: 0:19:15.812398\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5011/12662], [Dloss: -0.043796] [G loss: 0.372597] time: 0:19:17.112920\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5012/12662], [Dloss: 0.449996] [G loss: -0.124646] time: 0:19:18.449346\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5013/12662], [Dloss: 0.280810] [G loss: -0.005264] time: 0:19:19.863563\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5014/12662], [Dloss: 0.340023] [G loss: 0.165032] time: 0:19:21.266811\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5015/12662], [Dloss: 0.538473] [G loss: 0.219848] time: 0:19:22.724911\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5016/12662], [Dloss: 0.335652] [G loss: 0.138477] time: 0:19:24.166058\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5017/12662], [Dloss: -0.014255] [G loss: 0.206950] time: 0:19:25.489518\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5018/12662], [Dloss: 0.052339] [G loss: 0.316037] time: 0:19:26.830930\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5019/12662], [Dloss: 0.184535] [G loss: 0.303886] time: 0:19:28.173340\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5020/12662], [Dloss: 0.170436] [G loss: 0.004341] time: 0:19:29.571601\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5021/12662], [Dloss: 0.453829] [G loss: 0.501555] time: 0:19:30.864144\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5022/12662], [Dloss: 0.360337] [G loss: 0.456428] time: 0:19:32.164666\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5023/12662], [Dloss: -0.224222] [G loss: 0.396096] time: 0:19:33.445241\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5024/12662], [Dloss: 0.310740] [G loss: 0.301297] time: 0:19:34.815576\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5025/12662], [Dloss: 0.401373] [G loss: -0.025681] time: 0:19:36.271682\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5026/12662], [Dloss: 0.847521] [G loss: 0.129317] time: 0:19:37.630049\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5027/12662], [Dloss: 0.151578] [G loss: 0.267492] time: 0:19:38.923590\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5028/12662], [Dloss: 0.422042] [G loss: 0.881820] time: 0:19:40.326837\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5029/12662], [Dloss: 0.583727] [G loss: 0.657377] time: 0:19:41.706148\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5030/12662], [Dloss: 0.230217] [G loss: -0.232301] time: 0:19:42.986725\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5031/12662], [Dloss: 0.510061] [G loss: -0.165933] time: 0:19:44.338614\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5032/12662], [Dloss: 0.743307] [G loss: -0.482402] time: 0:19:45.738389\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5033/12662], [Dloss: 0.196283] [G loss: -0.254843] time: 0:19:47.036948\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5034/12662], [Dloss: 0.049018] [G loss: -0.145144] time: 0:19:48.359422\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5035/12662], [Dloss: -0.081037] [G loss: 0.208098] time: 0:19:49.642989\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5036/12662], [Dloss: 0.230451] [G loss: -0.038039] time: 0:19:50.962460\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5037/12662], [Dloss: 0.572128] [G loss: -0.012756] time: 0:19:52.258500\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5038/12662], [Dloss: 0.427204] [G loss: 0.216955] time: 0:19:53.547053\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5039/12662], [Dloss: -0.206462] [G loss: 0.145113] time: 0:19:54.946311\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5040/12662], [Dloss: 0.503353] [G loss: 0.219872] time: 0:19:56.350556\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5041/12662], [Dloss: 0.621611] [G loss: 0.649237] time: 0:19:57.759787\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5042/12662], [Dloss: 0.420172] [G loss: 0.675809] time: 0:19:59.195946\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5043/12662], [Dloss: 0.405050] [G loss: 0.648930] time: 0:20:00.564287\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5044/12662], [Dloss: 0.170732] [G loss: 0.641866] time: 0:20:01.916670\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5045/12662], [Dloss: 0.399105] [G loss: 0.327770] time: 0:20:03.174306\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5046/12662], [Dloss: 0.065241] [G loss: 0.347812] time: 0:20:04.545639\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5047/12662], [Dloss: 0.288243] [G loss: 0.364521] time: 0:20:05.886054\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5048/12662], [Dloss: 0.401031] [G loss: 0.037477] time: 0:20:07.190566\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5049/12662], [Dloss: 0.113324] [G loss: 0.047859] time: 0:20:08.559904\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5050/12662], [Dloss: 0.265383] [G loss: 0.079104] time: 0:20:09.917273\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5051/12662], [Dloss: 0.412886] [G loss: 0.335728] time: 0:20:11.255694\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5052/12662], [Dloss: -0.047658] [G loss: 0.177690] time: 0:20:12.582147\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5053/12662], [Dloss: 0.128200] [G loss: 0.299265] time: 0:20:13.907601\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5054/12662], [Dloss: 0.606701] [G loss: 0.024180] time: 0:20:15.229068\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5055/12662], [Dloss: 0.575330] [G loss: -0.227201] time: 0:20:16.541558\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5056/12662], [Dloss: 0.507791] [G loss: 0.044680] time: 0:20:17.846069\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5057/12662], [Dloss: 0.332521] [G loss: 0.826621] time: 0:20:19.197408\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5058/12662], [Dloss: 0.608593] [G loss: 0.525552] time: 0:20:20.528847\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5059/12662], [Dloss: 0.083940] [G loss: 0.465839] time: 0:20:21.773519\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5060/12662], [Dloss: 0.218569] [G loss: 0.430494] time: 0:20:23.042127\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5061/12662], [Dloss: 0.494819] [G loss: 0.496997] time: 0:20:24.411464\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5062/12662], [Dloss: 0.085386] [G loss: 0.347156] time: 0:20:25.709991\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5063/12662], [Dloss: 0.573519] [G loss: 0.394295] time: 0:20:26.967628\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5064/12662], [Dloss: 0.420783] [G loss: 0.429880] time: 0:20:28.253190\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5065/12662], [Dloss: 0.356766] [G loss: 0.110410] time: 0:20:29.535760\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5066/12662], [Dloss: 0.149503] [G loss: 0.131381] time: 0:20:30.874180\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5067/12662], [Dloss: 0.260757] [G loss: 0.157375] time: 0:20:32.251475\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5068/12662], [Dloss: 0.465118] [G loss: -0.116580] time: 0:20:33.534044\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5069/12662], [Dloss: 0.310921] [G loss: -0.013954] time: 0:20:34.936294\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5070/12662], [Dloss: 0.072007] [G loss: 0.138680] time: 0:20:36.337547\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5071/12662], [Dloss: 0.353494] [G loss: 0.094537] time: 0:20:37.680954\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5072/12662], [Dloss: 0.060608] [G loss: 0.069139] time: 0:20:39.039321\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5073/12662], [Dloss: 0.384829] [G loss: 0.026981] time: 0:20:40.415641\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5074/12662], [Dloss: -0.104167] [G loss: 0.458176] time: 0:20:41.722652\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5075/12662], [Dloss: 0.235260] [G loss: 0.605692] time: 0:20:43.000235\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5076/12662], [Dloss: -0.527098] [G loss: 0.667849] time: 0:20:44.447365\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5077/12662], [Dloss: 0.368959] [G loss: 0.451847] time: 0:20:45.759854\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5078/12662], [Dloss: 0.216416] [G loss: 0.273562] time: 0:20:47.107251\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5079/12662], [Dloss: 0.561424] [G loss: 0.158561] time: 0:20:48.458588\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5080/12662], [Dloss: 0.526127] [G loss: -0.013254] time: 0:20:49.822938\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5081/12662], [Dloss: 0.686114] [G loss: 0.105349] time: 0:20:51.153380\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5082/12662], [Dloss: 0.480313] [G loss: -0.243342] time: 0:20:52.478836\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5083/12662], [Dloss: 0.351383] [G loss: -0.211561] time: 0:20:53.808785\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5084/12662], [Dloss: 0.145633] [G loss: -0.088369] time: 0:20:55.068416\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5085/12662], [Dloss: 0.202165] [G loss: -0.037776] time: 0:20:56.397860\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5086/12662], [Dloss: -0.246905] [G loss: -0.139857] time: 0:20:57.721321\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5087/12662], [Dloss: 0.187467] [G loss: 0.018982] time: 0:20:59.118584\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5088/12662], [Dloss: 0.060171] [G loss: 0.000685] time: 0:21:00.467975\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5089/12662], [Dloss: 0.479152] [G loss: -0.186516] time: 0:21:01.900370\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5090/12662], [Dloss: 0.208169] [G loss: -0.128685] time: 0:21:03.236796\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5091/12662], [Dloss: -0.407665] [G loss: 0.387793] time: 0:21:04.759724\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5092/12662], [Dloss: 0.404000] [G loss: 0.399667] time: 0:21:06.200388\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5093/12662], [Dloss: 0.048983] [G loss: 0.415796] time: 0:21:07.676441\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5094/12662], [Dloss: 0.292034] [G loss: -0.070934] time: 0:21:09.128557\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5095/12662], [Dloss: 0.411100] [G loss: 0.067325] time: 0:21:10.517842\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5096/12662], [Dloss: 0.142586] [G loss: 0.180809] time: 0:21:11.792433\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5097/12662], [Dloss: -0.133639] [G loss: 0.346428] time: 0:21:13.211638\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5098/12662], [Dloss: 0.306742] [G loss: 0.167547] time: 0:21:14.492213\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5099/12662], [Dloss: 0.579709] [G loss: 0.281266] time: 0:21:15.839609\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5100/12662], [Dloss: 0.454937] [G loss: 0.212721] time: 0:21:17.236873\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5101/12662], [Dloss: 0.267874] [G loss: -0.071470] time: 0:21:18.558339\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5102/12662], [Dloss: 0.175641] [G loss: 0.276433] time: 0:21:19.929671\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5103/12662], [Dloss: 0.167212] [G loss: 0.486566] time: 0:21:21.281057\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5104/12662], [Dloss: 0.569554] [G loss: 0.125632] time: 0:21:22.578587\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5105/12662], [Dloss: 0.434055] [G loss: 0.069741] time: 0:21:23.959893\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5106/12662], [Dloss: 0.090770] [G loss: 0.247259] time: 0:21:25.330228\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5107/12662], [Dloss: 0.271471] [G loss: 0.364770] time: 0:21:26.831214\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5108/12662], [Dloss: 0.191119] [G loss: 0.532959] time: 0:21:28.333702\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5109/12662], [Dloss: 0.419310] [G loss: 0.296483] time: 0:21:29.722986\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5110/12662], [Dloss: 0.363975] [G loss: 0.183072] time: 0:21:31.191060\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5111/12662], [Dloss: 0.379067] [G loss: 0.249686] time: 0:21:32.506542\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5112/12662], [Dloss: -0.214723] [G loss: 0.360645] time: 0:21:33.882861\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5113/12662], [Dloss: 0.485092] [G loss: 0.427660] time: 0:21:35.220284\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5114/12662], [Dloss: 0.400391] [G loss: 0.499407] time: 0:21:36.554715\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5115/12662], [Dloss: -0.021458] [G loss: 0.335588] time: 0:21:37.852246\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5116/12662], [Dloss: 0.348003] [G loss: 0.268749] time: 0:21:39.213605\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5117/12662], [Dloss: 0.136926] [G loss: -0.078462] time: 0:21:40.582943\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5118/12662], [Dloss: 0.592232] [G loss: -0.429784] time: 0:21:41.897427\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5119/12662], [Dloss: 0.568108] [G loss: -0.169259] time: 0:21:43.254303\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5120/12662], [Dloss: 0.323335] [G loss: -0.142514] time: 0:21:44.613185\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5121/12662], [Dloss: 0.266031] [G loss: 0.133141] time: 0:21:45.970585\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5122/12662], [Dloss: 0.403868] [G loss: 0.183057] time: 0:21:47.324963\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5123/12662], [Dloss: 0.613029] [G loss: 0.102755] time: 0:21:48.827944\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5124/12662], [Dloss: 0.480150] [G loss: 0.475591] time: 0:21:50.182322\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5125/12662], [Dloss: 0.194226] [G loss: 0.376020] time: 0:21:51.551660\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5126/12662], [Dloss: -0.048969] [G loss: 0.505492] time: 0:21:52.863152\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5127/12662], [Dloss: 0.502486] [G loss: 0.322638] time: 0:21:54.200576\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5128/12662], [Dloss: 0.002858] [G loss: 0.582675] time: 0:21:55.558943\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5129/12662], [Dloss: 0.210537] [G loss: 0.066571] time: 0:21:56.887390\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5130/12662], [Dloss: 0.020871] [G loss: -0.032495] time: 0:21:58.249747\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5131/12662], [Dloss: 0.473050] [G loss: 0.109918] time: 0:21:59.583181\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5132/12662], [Dloss: 0.012672] [G loss: -0.354554] time: 0:22:00.887692\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5133/12662], [Dloss: 0.280408] [G loss: 0.040243] time: 0:22:02.204171\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5134/12662], [Dloss: 0.073482] [G loss: -0.162055] time: 0:22:03.602432\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5135/12662], [Dloss: 0.299904] [G loss: 0.398946] time: 0:22:04.969775\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5136/12662], [Dloss: 0.577672] [G loss: 0.255189] time: 0:22:06.297225\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5137/12662], [Dloss: -0.327971] [G loss: 0.344175] time: 0:22:07.682520\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5138/12662], [Dloss: 0.388944] [G loss: 0.223678] time: 0:22:08.992018\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5139/12662], [Dloss: 0.439370] [G loss: -0.004158] time: 0:22:10.356369\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5140/12662], [Dloss: 0.281924] [G loss: -0.136806] time: 0:22:11.663872\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5141/12662], [Dloss: 0.514443] [G loss: -0.225046] time: 0:22:13.018251\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5142/12662], [Dloss: 0.299274] [G loss: 0.320276] time: 0:22:14.368639\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5143/12662], [Dloss: 0.516670] [G loss: 0.232337] time: 0:22:15.710052\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5144/12662], [Dloss: 0.081685] [G loss: 0.359586] time: 0:22:17.005587\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5145/12662], [Dloss: 0.150131] [G loss: 0.640509] time: 0:22:18.263254\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5146/12662], [Dloss: 0.257073] [G loss: 0.487501] time: 0:22:19.545825\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5147/12662], [Dloss: 0.018344] [G loss: 0.618274] time: 0:22:20.820415\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5148/12662], [Dloss: 0.220390] [G loss: 0.248292] time: 0:22:22.176788\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5149/12662], [Dloss: 0.632579] [G loss: 0.328616] time: 0:22:23.486286\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5150/12662], [Dloss: 0.107783] [G loss: 0.408795] time: 0:22:24.761876\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5151/12662], [Dloss: 0.450106] [G loss: 0.311699] time: 0:22:26.122236\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5152/12662], [Dloss: 0.326042] [G loss: 0.151811] time: 0:22:27.466641\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5153/12662], [Dloss: 0.301918] [G loss: -0.040739] time: 0:22:28.821020\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5154/12662], [Dloss: 0.148709] [G loss: 0.089957] time: 0:22:30.104587\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5155/12662], [Dloss: 0.135202] [G loss: 0.041798] time: 0:22:31.428047\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5156/12662], [Dloss: 0.470742] [G loss: -0.034389] time: 0:22:32.721588\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5157/12662], [Dloss: -0.050673] [G loss: 0.494444] time: 0:22:34.045048\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5158/12662], [Dloss: 0.523759] [G loss: 0.365332] time: 0:22:35.300690\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5159/12662], [Dloss: 0.271552] [G loss: 0.407778] time: 0:22:36.686983\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5160/12662], [Dloss: 0.433230] [G loss: 0.688917] time: 0:22:38.016427\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5161/12662], [Dloss: 0.377601] [G loss: 0.612445] time: 0:22:39.382773\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5162/12662], [Dloss: 0.197887] [G loss: 0.282990] time: 0:22:40.759093\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5163/12662], [Dloss: 0.241292] [G loss: 0.515295] time: 0:22:42.103497\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5164/12662], [Dloss: 0.574650] [G loss: 0.226552] time: 0:22:43.418979\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5165/12662], [Dloss: 0.374714] [G loss: 0.040588] time: 0:22:44.743437\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5166/12662], [Dloss: 0.083601] [G loss: 0.043678] time: 0:22:46.129730\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5167/12662], [Dloss: 0.233281] [G loss: 0.025824] time: 0:22:47.476129\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5168/12662], [Dloss: -0.005496] [G loss: -0.132451] time: 0:22:48.754709\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5169/12662], [Dloss: 0.593569] [G loss: 0.035185] time: 0:22:50.047253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5170/12662], [Dloss: 0.096797] [G loss: 0.321682] time: 0:22:51.413598\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5171/12662], [Dloss: 0.097600] [G loss: 0.179177] time: 0:22:52.752019\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5172/12662], [Dloss: 0.022128] [G loss: 0.407156] time: 0:22:54.083411\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5173/12662], [Dloss: 0.445524] [G loss: 0.522541] time: 0:22:55.369970\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5174/12662], [Dloss: 0.553153] [G loss: 0.758963] time: 0:22:56.738311\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5175/12662], [Dloss: 0.185339] [G loss: 0.539122] time: 0:22:58.077729\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5176/12662], [Dloss: 0.217471] [G loss: 0.411626] time: 0:22:59.331376\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5177/12662], [Dloss: 0.298257] [G loss: 0.342680] time: 0:23:00.663813\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5178/12662], [Dloss: 0.250232] [G loss: 0.194980] time: 0:23:02.025173\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5179/12662], [Dloss: 0.298030] [G loss: -0.024067] time: 0:23:03.366585\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5180/12662], [Dloss: 0.151001] [G loss: 0.197205] time: 0:23:04.634194\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5181/12662], [Dloss: 0.158568] [G loss: -0.420874] time: 0:23:06.009517\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5182/12662], [Dloss: 0.421197] [G loss: -0.355311] time: 0:23:07.481580\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5183/12662], [Dloss: 0.413019] [G loss: -0.440628] time: 0:23:09.025451\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5184/12662], [Dloss: 0.392234] [G loss: -0.218976] time: 0:23:10.481557\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5185/12662], [Dloss: 0.358250] [G loss: 0.113444] time: 0:23:11.844911\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5186/12662], [Dloss: 0.646377] [G loss: -0.211645] time: 0:23:13.267107\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5187/12662], [Dloss: 0.357882] [G loss: 0.352060] time: 0:23:14.565141\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5188/12662], [Dloss: 0.349441] [G loss: 0.390518] time: 0:23:15.846714\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5189/12662], [Dloss: 0.325205] [G loss: 0.808675] time: 0:23:17.224030\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5190/12662], [Dloss: 0.434730] [G loss: 0.464519] time: 0:23:18.499126\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5191/12662], [Dloss: 0.311609] [G loss: 0.778948] time: 0:23:19.903370\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5192/12662], [Dloss: 0.401640] [G loss: 0.161947] time: 0:23:21.175967\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5193/12662], [Dloss: 0.547608] [G loss: 0.143426] time: 0:23:22.522366\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5194/12662], [Dloss: 0.226962] [G loss: 0.092088] time: 0:23:23.776013\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5195/12662], [Dloss: 0.207166] [G loss: -0.114307] time: 0:23:25.097479\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5196/12662], [Dloss: 0.448912] [G loss: -0.471926] time: 0:23:26.474796\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5197/12662], [Dloss: 0.365258] [G loss: -0.403402] time: 0:23:27.782299\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5198/12662], [Dloss: 0.402256] [G loss: -0.621759] time: 0:23:29.132688\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5199/12662], [Dloss: 0.051407] [G loss: -0.451063] time: 0:23:30.413263\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5200/12662], [Dloss: 0.433571] [G loss: -0.476335] time: 0:23:31.738718\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5201/12662], [Dloss: 0.564746] [G loss: -0.259525] time: 0:23:33.037246\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5202/12662], [Dloss: 0.438711] [G loss: -0.110294] time: 0:23:34.333778\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5203/12662], [Dloss: 0.298617] [G loss: 0.143461] time: 0:23:35.684167\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5204/12662], [Dloss: 0.282003] [G loss: 0.351620] time: 0:23:36.948785\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5205/12662], [Dloss: 0.277625] [G loss: 0.585231] time: 0:23:38.219387\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5206/12662], [Dloss: 0.482331] [G loss: 0.577173] time: 0:23:39.586730\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5207/12662], [Dloss: 0.118417] [G loss: 0.775601] time: 0:23:40.904207\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5208/12662], [Dloss: 0.234533] [G loss: 0.621297] time: 0:23:42.194755\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5209/12662], [Dloss: 0.346405] [G loss: 0.317030] time: 0:23:43.605981\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5210/12662], [Dloss: 0.125384] [G loss: 0.200764] time: 0:23:44.949914\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5211/12662], [Dloss: 0.329483] [G loss: 0.360529] time: 0:23:46.220053\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5212/12662], [Dloss: 0.024901] [G loss: 0.490694] time: 0:23:47.601368\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5213/12662], [Dloss: 0.249171] [G loss: 0.650663] time: 0:23:48.914855\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5214/12662], [Dloss: -0.000964] [G loss: 0.170918] time: 0:23:50.172492\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5215/12662], [Dloss: 0.388088] [G loss: 0.182265] time: 0:23:51.574260\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5216/12662], [Dloss: 0.590413] [G loss: 0.032031] time: 0:23:52.842867\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5217/12662], [Dloss: 0.253072] [G loss: 0.184464] time: 0:23:54.217705\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5218/12662], [Dloss: 0.300404] [G loss: 0.607108] time: 0:23:55.470365\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5219/12662], [Dloss: 0.683725] [G loss: 0.144761] time: 0:23:56.792829\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5220/12662], [Dloss: 0.194104] [G loss: 0.443881] time: 0:23:58.167165\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5221/12662], [Dloss: 0.465705] [G loss: 0.754885] time: 0:23:59.501596\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5222/12662], [Dloss: 0.318863] [G loss: 0.573769] time: 0:24:00.897370\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5223/12662], [Dloss: 0.731354] [G loss: 0.534539] time: 0:24:02.277195\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5224/12662], [Dloss: 0.368817] [G loss: 0.564411] time: 0:24:03.599658\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5225/12662], [Dloss: 0.203662] [G loss: 0.357453] time: 0:24:04.912653\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5226/12662], [Dloss: 0.250843] [G loss: 0.469789] time: 0:24:06.292962\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5227/12662], [Dloss: 0.284069] [G loss: -0.071032] time: 0:24:07.614428\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5228/12662], [Dloss: 0.342180] [G loss: 0.554034] time: 0:24:08.908966\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5229/12662], [Dloss: 0.025122] [G loss: 0.389812] time: 0:24:10.236416\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5230/12662], [Dloss: 0.457975] [G loss: 0.144844] time: 0:24:11.548906\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5231/12662], [Dloss: 0.415017] [G loss: 0.679021] time: 0:24:12.916249\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5232/12662], [Dloss: -0.146937] [G loss: 0.855404] time: 0:24:14.238712\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5233/12662], [Dloss: 0.415643] [G loss: 0.512516] time: 0:24:15.541229\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5234/12662], [Dloss: 0.379937] [G loss: 0.474108] time: 0:24:16.914555\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5235/12662], [Dloss: 0.203777] [G loss: 0.555955] time: 0:24:18.311326\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5236/12662], [Dloss: 0.071401] [G loss: 0.171419] time: 0:24:19.614840\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5237/12662], [Dloss: 0.472418] [G loss: -0.005726] time: 0:24:21.067954\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5238/12662], [Dloss: 0.276709] [G loss: 0.066537] time: 0:24:22.599857\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5239/12662], [Dloss: 0.181642] [G loss: 0.001471] time: 0:24:24.005099\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5240/12662], [Dloss: 0.497177] [G loss: 0.040281] time: 0:24:25.386405\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5241/12662], [Dloss: 0.307429] [G loss: -0.007084] time: 0:24:26.864452\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5242/12662], [Dloss: 0.795169] [G loss: -0.144581] time: 0:24:28.319561\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5243/12662], [Dloss: 0.074569] [G loss: 0.223222] time: 0:24:29.753234\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5244/12662], [Dloss: 0.409986] [G loss: -0.188727] time: 0:24:31.184928\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5245/12662], [Dloss: 0.269830] [G loss: -0.025582] time: 0:24:32.533322\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5246/12662], [Dloss: 0.234371] [G loss: 0.210196] time: 0:24:33.905652\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5247/12662], [Dloss: 0.631154] [G loss: -0.096595] time: 0:24:35.322861\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5248/12662], [Dloss: 0.257049] [G loss: -0.062104] time: 0:24:36.585485\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5249/12662], [Dloss: 0.787800] [G loss: 0.149871] time: 0:24:37.947841\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5250/12662], [Dloss: 0.511502] [G loss: 0.267309] time: 0:24:39.378016\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5251/12662], [Dloss: 0.242203] [G loss: 0.325314] time: 0:24:40.675546\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5252/12662], [Dloss: 0.347336] [G loss: 0.535405] time: 0:24:42.029959\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5253/12662], [Dloss: 0.505663] [G loss: 0.492781] time: 0:24:43.383522\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5254/12662], [Dloss: 0.194144] [G loss: 0.696193] time: 0:24:44.794747\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5255/12662], [Dloss: 0.435393] [G loss: -0.029374] time: 0:24:46.115216\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5256/12662], [Dloss: 0.203482] [G loss: 0.384537] time: 0:24:47.456628\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5257/12662], [Dloss: 0.240219] [G loss: 0.802836] time: 0:24:48.892799\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5258/12662], [Dloss: 0.296549] [G loss: 0.289405] time: 0:24:50.311006\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5259/12662], [Dloss: 0.053371] [G loss: 0.454126] time: 0:24:51.656409\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5260/12662], [Dloss: 0.281169] [G loss: 0.551288] time: 0:24:52.961916\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5261/12662], [Dloss: 0.407750] [G loss: 0.699459] time: 0:24:54.330257\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5262/12662], [Dloss: -0.001899] [G loss: 0.541291] time: 0:24:55.710565\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5263/12662], [Dloss: 0.157256] [G loss: 0.665897] time: 0:24:57.063453\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5264/12662], [Dloss: 0.193081] [G loss: 0.330657] time: 0:24:58.409853\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5265/12662], [Dloss: 0.368825] [G loss: 0.365219] time: 0:24:59.736305\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5266/12662], [Dloss: 0.429583] [G loss: 0.492848] time: 0:25:01.067745\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5267/12662], [Dloss: 0.306527] [G loss: 0.564206] time: 0:25:02.387216\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5268/12662], [Dloss: 0.037990] [G loss: 0.431548] time: 0:25:03.765547\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5269/12662], [Dloss: 0.577690] [G loss: 0.213793] time: 0:25:05.125909\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5270/12662], [Dloss: 0.133770] [G loss: 0.016669] time: 0:25:06.425433\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5271/12662], [Dloss: 0.482313] [G loss: -0.040719] time: 0:25:07.745339\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5272/12662], [Dloss: 0.355250] [G loss: 0.072524] time: 0:25:09.047856\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5273/12662], [Dloss: 0.331413] [G loss: 0.159981] time: 0:25:10.383284\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5274/12662], [Dloss: 0.277216] [G loss: 0.274844] time: 0:25:11.706745\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5275/12662], [Dloss: 0.252829] [G loss: -0.084015] time: 0:25:13.018237\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5276/12662], [Dloss: 0.006470] [G loss: 0.344293] time: 0:25:14.376604\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5277/12662], [Dloss: -0.011963] [G loss: 0.436266] time: 0:25:15.689094\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5278/12662], [Dloss: 0.384766] [G loss: 0.387133] time: 0:25:17.026518\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5279/12662], [Dloss: 0.375320] [G loss: 0.323161] time: 0:25:18.412810\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5280/12662], [Dloss: -0.000245] [G loss: 0.202892] time: 0:25:19.764197\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5281/12662], [Dloss: 0.475675] [G loss: 0.256568] time: 0:25:21.030809\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5282/12662], [Dloss: 0.035704] [G loss: -0.070401] time: 0:25:22.348285\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5283/12662], [Dloss: 0.560016] [G loss: -0.359565] time: 0:25:23.653794\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5284/12662], [Dloss: 0.099666] [G loss: -0.423882] time: 0:25:24.985738\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5285/12662], [Dloss: 0.395798] [G loss: -0.100830] time: 0:25:26.304717\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5286/12662], [Dloss: 0.227732] [G loss: -0.129627] time: 0:25:27.629175\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5287/12662], [Dloss: 0.332868] [G loss: -0.101229] time: 0:25:28.861878\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5288/12662], [Dloss: 0.613515] [G loss: 0.290277] time: 0:25:30.363369\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5289/12662], [Dloss: 0.620570] [G loss: -0.053107] time: 0:25:31.686830\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5290/12662], [Dloss: 0.454693] [G loss: 0.185815] time: 0:25:33.050184\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5291/12662], [Dloss: 0.264892] [G loss: 0.341472] time: 0:25:34.424508\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5292/12662], [Dloss: 0.360728] [G loss: 0.301323] time: 0:25:35.812795\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5293/12662], [Dloss: -0.072593] [G loss: 0.574387] time: 0:25:37.139248\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5294/12662], [Dloss: 0.448132] [G loss: 0.084849] time: 0:25:38.510581\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5295/12662], [Dloss: 0.535774] [G loss: 0.329481] time: 0:25:39.784679\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5296/12662], [Dloss: 0.471866] [G loss: 0.580862] time: 0:25:41.171969\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5297/12662], [Dloss: -0.161293] [G loss: 0.449216] time: 0:25:42.544299\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5298/12662], [Dloss: 0.355960] [G loss: 0.325882] time: 0:25:43.899684\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5299/12662], [Dloss: 0.140273] [G loss: 0.493328] time: 0:25:45.167808\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5300/12662], [Dloss: 0.192615] [G loss: 0.497170] time: 0:25:46.457359\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5301/12662], [Dloss: 0.285312] [G loss: 0.547520] time: 0:25:47.767854\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5302/12662], [Dloss: 0.349377] [G loss: 0.345150] time: 0:25:49.172606\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5303/12662], [Dloss: 0.609229] [G loss: 0.657825] time: 0:25:50.515519\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5304/12662], [Dloss: 0.006255] [G loss: 0.254560] time: 0:25:51.835494\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5305/12662], [Dloss: 0.396896] [G loss: 0.215869] time: 0:25:53.134033\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5306/12662], [Dloss: 0.108515] [G loss: 0.321815] time: 0:25:54.477440\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5307/12662], [Dloss: 0.257041] [G loss: 0.360191] time: 0:25:55.762005\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5308/12662], [Dloss: 0.243158] [G loss: 0.283047] time: 0:25:57.093444\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5309/12662], [Dloss: 0.224596] [G loss: 0.312242] time: 0:25:58.382995\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5310/12662], [Dloss: -0.144403] [G loss: 0.615165] time: 0:25:59.809181\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5311/12662], [Dloss: 0.396665] [G loss: 0.598319] time: 0:26:01.163573\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5312/12662], [Dloss: 0.227869] [G loss: 0.353321] time: 0:26:02.532910\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5313/12662], [Dloss: 0.439064] [G loss: -0.056223] time: 0:26:03.835427\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5314/12662], [Dloss: 0.452337] [G loss: 0.171316] time: 0:26:05.177837\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5315/12662], [Dloss: 0.565829] [G loss: -0.262627] time: 0:26:06.629970\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5316/12662], [Dloss: -0.096274] [G loss: 0.017673] time: 0:26:07.921516\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5317/12662], [Dloss: 0.102181] [G loss: -0.319882] time: 0:26:09.230017\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5318/12662], [Dloss: 0.018839] [G loss: 0.191677] time: 0:26:10.510592\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5319/12662], [Dloss: 0.388979] [G loss: 0.344499] time: 0:26:11.810116\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5320/12662], [Dloss: -0.295580] [G loss: 0.022628] time: 0:26:13.182446\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5321/12662], [Dloss: 0.113557] [G loss: 0.132083] time: 0:26:14.539816\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5322/12662], [Dloss: 0.172666] [G loss: 0.375279] time: 0:26:15.818397\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5323/12662], [Dloss: 0.370961] [G loss: 0.481717] time: 0:26:17.102961\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5324/12662], [Dloss: 0.151193] [G loss: 0.361031] time: 0:26:18.422433\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5325/12662], [Dloss: 0.165588] [G loss: -0.007802] time: 0:26:19.645163\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5326/12662], [Dloss: 0.573585] [G loss: 0.323495] time: 0:26:20.916762\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5327/12662], [Dloss: 0.523134] [G loss: -0.186839] time: 0:26:22.168415\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5328/12662], [Dloss: 0.311094] [G loss: -0.224155] time: 0:26:23.478910\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5329/12662], [Dloss: -0.137646] [G loss: -0.215579] time: 0:26:24.821320\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5330/12662], [Dloss: 0.352704] [G loss: -0.347648] time: 0:26:26.139794\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5331/12662], [Dloss: 0.367108] [G loss: -0.446499] time: 0:26:27.437324\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5332/12662], [Dloss: 0.319685] [G loss: -0.536054] time: 0:26:28.629136\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5333/12662], [Dloss: 0.132158] [G loss: -0.373521] time: 0:26:29.953594\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5334/12662], [Dloss: 0.380815] [G loss: 0.179382] time: 0:26:31.332905\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5335/12662], [Dloss: 0.205283] [G loss: 0.364322] time: 0:26:32.667337\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5336/12662], [Dloss: 0.165920] [G loss: 0.219012] time: 0:26:34.009746\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5337/12662], [Dloss: 0.090046] [G loss: 0.426668] time: 0:26:35.368114\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5338/12662], [Dloss: 0.412835] [G loss: 0.215629] time: 0:26:36.652678\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5339/12662], [Dloss: 0.203867] [G loss: 0.061090] time: 0:26:37.976139\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5340/12662], [Dloss: 0.139863] [G loss: 0.413327] time: 0:26:39.215824\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5341/12662], [Dloss: 0.447038] [G loss: 0.005316] time: 0:26:40.537289\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5342/12662], [Dloss: 0.406227] [G loss: -0.101267] time: 0:26:41.820857\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5343/12662], [Dloss: 0.498314] [G loss: 0.069170] time: 0:26:43.080488\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5344/12662], [Dloss: 0.349030] [G loss: -0.587684] time: 0:26:44.385997\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5345/12662], [Dloss: 0.454522] [G loss: -0.161803] time: 0:26:45.693500\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5346/12662], [Dloss: 0.485466] [G loss: -0.232068] time: 0:26:47.006987\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5347/12662], [Dloss: 0.040425] [G loss: 0.135262] time: 0:26:48.291552\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5348/12662], [Dloss: 0.432504] [G loss: 0.202544] time: 0:26:49.592074\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5349/12662], [Dloss: 0.385397] [G loss: 0.719562] time: 0:26:50.889604\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5350/12662], [Dloss: 0.310422] [G loss: 0.487627] time: 0:26:52.179155\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5351/12662], [Dloss: 0.216656] [G loss: 0.637287] time: 0:26:53.530542\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5352/12662], [Dloss: 0.160458] [G loss: 0.482939] time: 0:26:54.864972\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5353/12662], [Dloss: 0.298226] [G loss: 0.628648] time: 0:26:56.160507\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5354/12662], [Dloss: 0.505064] [G loss: 0.605711] time: 0:26:57.454048\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5355/12662], [Dloss: 0.206802] [G loss: 0.585944] time: 0:26:58.788479\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5356/12662], [Dloss: 0.333789] [G loss: 0.549750] time: 0:27:00.065065\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5357/12662], [Dloss: 0.546431] [G loss: 0.176042] time: 0:27:01.409470\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5358/12662], [Dloss: 0.461301] [G loss: 0.494604] time: 0:27:02.755869\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5359/12662], [Dloss: 0.484419] [G loss: 0.064068] time: 0:27:04.130194\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5360/12662], [Dloss: 0.202075] [G loss: 0.183999] time: 0:27:05.463628\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5361/12662], [Dloss: 0.516957] [G loss: 0.027210] time: 0:27:06.870864\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5362/12662], [Dloss: 0.254977] [G loss: 0.076341] time: 0:27:08.221253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5363/12662], [Dloss: 0.302945] [G loss: 0.260062] time: 0:27:09.555684\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5364/12662], [Dloss: 0.262906] [G loss: -0.053300] time: 0:27:10.808334\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5365/12662], [Dloss: 0.507670] [G loss: 0.179205] time: 0:27:12.129800\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5366/12662], [Dloss: 0.335383] [G loss: -0.032061] time: 0:27:13.398407\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5367/12662], [Dloss: 0.158166] [G loss: 0.085182] time: 0:27:14.722865\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5368/12662], [Dloss: 0.158255] [G loss: 0.358818] time: 0:27:16.101180\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5369/12662], [Dloss: 0.346942] [G loss: 0.267911] time: 0:27:17.396714\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5370/12662], [Dloss: -0.044663] [G loss: 0.465468] time: 0:27:18.689257\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5371/12662], [Dloss: 0.703459] [G loss: 0.086909] time: 0:27:20.163820\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5372/12662], [Dloss: 0.019899] [G loss: 0.200894] time: 0:27:21.542134\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5373/12662], [Dloss: 0.160901] [G loss: 0.394768] time: 0:27:22.926432\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5374/12662], [Dloss: 0.410671] [G loss: 0.106084] time: 0:27:24.331674\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5375/12662], [Dloss: 0.140683] [G loss: 0.042843] time: 0:27:25.656132\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5376/12662], [Dloss: 0.340713] [G loss: -0.260428] time: 0:27:26.978598\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5377/12662], [Dloss: 0.379045] [G loss: 0.162151] time: 0:27:28.365942\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5378/12662], [Dloss: 0.622250] [G loss: 0.313343] time: 0:27:29.693331\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5379/12662], [Dloss: 0.146642] [G loss: 0.139621] time: 0:27:31.014797\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5380/12662], [Dloss: 0.309563] [G loss: 0.252099] time: 0:27:32.309335\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5381/12662], [Dloss: 0.658640] [G loss: 0.032165] time: 0:27:33.601878\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5382/12662], [Dloss: 0.528587] [G loss: 0.029151] time: 0:27:34.953768\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5383/12662], [Dloss: 0.395177] [G loss: 0.211433] time: 0:27:36.222376\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5384/12662], [Dloss: -0.108561] [G loss: -0.153229] time: 0:27:37.533868\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5385/12662], [Dloss: 0.308698] [G loss: 0.041846] time: 0:27:38.849350\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5386/12662], [Dloss: 0.640072] [G loss: -0.329252] time: 0:27:40.111974\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5387/12662], [Dloss: 0.365010] [G loss: -0.315737] time: 0:27:41.476325\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5388/12662], [Dloss: 0.489203] [G loss: 0.182289] time: 0:27:42.841673\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5389/12662], [Dloss: 0.487621] [G loss: 0.074939] time: 0:27:44.172619\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5390/12662], [Dloss: 0.157792] [G loss: -0.056665] time: 0:27:45.577872\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5391/12662], [Dloss: 0.125634] [G loss: 0.086885] time: 0:27:46.941228\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5392/12662], [Dloss: 0.018994] [G loss: 0.079560] time: 0:27:48.280147\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5393/12662], [Dloss: 0.351538] [G loss: 0.159248] time: 0:27:49.613087\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5394/12662], [Dloss: 0.034616] [G loss: 0.467699] time: 0:27:50.953502\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5395/12662], [Dloss: 0.528174] [G loss: 0.245635] time: 0:27:52.229609\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5396/12662], [Dloss: 0.363942] [G loss: 0.307348] time: 0:27:53.555064\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5397/12662], [Dloss: 0.080975] [G loss: 0.249811] time: 0:27:54.834642\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5398/12662], [Dloss: 0.389930] [G loss: 0.245252] time: 0:27:56.108236\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5399/12662], [Dloss: 0.205350] [G loss: 0.079530] time: 0:27:57.423717\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5400/12662], [Dloss: 0.120282] [G loss: 0.173089] time: 0:27:58.798042\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5401/12662], [Dloss: 0.043910] [G loss: 0.201390] time: 0:28:00.087593\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5402/12662], [Dloss: 0.322170] [G loss: 0.241787] time: 0:28:01.337252\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5403/12662], [Dloss: -0.049553] [G loss: 0.392615] time: 0:28:02.641763\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5404/12662], [Dloss: 0.044963] [G loss: 0.215264] time: 0:28:04.029052\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5405/12662], [Dloss: 0.048054] [G loss: 0.013082] time: 0:28:05.380439\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5406/12662], [Dloss: 0.201302] [G loss: 0.040616] time: 0:28:06.670987\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5407/12662], [Dloss: 0.463242] [G loss: -0.396204] time: 0:28:07.938597\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5408/12662], [Dloss: 0.096787] [G loss: -0.231000] time: 0:28:09.280010\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5409/12662], [Dloss: 0.080327] [G loss: -0.076742] time: 0:28:10.562579\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5410/12662], [Dloss: 0.240289] [G loss: 0.168455] time: 0:28:11.884046\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5411/12662], [Dloss: 0.342678] [G loss: 0.604066] time: 0:28:13.156642\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5412/12662], [Dloss: 0.272173] [G loss: 0.696856] time: 0:28:14.487084\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5413/12662], [Dloss: -0.126811] [G loss: 0.823153] time: 0:28:15.800571\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5414/12662], [Dloss: 0.455833] [G loss: 0.903745] time: 0:28:17.134005\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5415/12662], [Dloss: 0.410702] [G loss: 0.762411] time: 0:28:18.386655\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5416/12662], [Dloss: 0.356704] [G loss: 0.895201] time: 0:28:19.736048\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5417/12662], [Dloss: 0.326836] [G loss: 0.533465] time: 0:28:21.026595\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5418/12662], [Dloss: 0.018949] [G loss: 0.643054] time: 0:28:22.311160\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5419/12662], [Dloss: 0.179829] [G loss: 0.564753] time: 0:28:23.618663\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5420/12662], [Dloss: 0.319764] [G loss: -0.132602] time: 0:28:24.864332\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5421/12662], [Dloss: 0.087445] [G loss: -0.144840] time: 0:28:26.225691\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5422/12662], [Dloss: 0.378570] [G loss: -0.119790] time: 0:28:27.497290\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5423/12662], [Dloss: 0.435993] [G loss: -0.331974] time: 0:28:28.787839\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5424/12662], [Dloss: 0.157364] [G loss: -0.052763] time: 0:28:30.050462\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5425/12662], [Dloss: 0.326359] [G loss: -0.335263] time: 0:28:31.381845\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5426/12662], [Dloss: 0.244772] [G loss: -0.146511] time: 0:28:32.687354\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5427/12662], [Dloss: 0.569258] [G loss: -0.018256] time: 0:28:34.011812\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5428/12662], [Dloss: 0.596259] [G loss: 0.321108] time: 0:28:35.278424\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5429/12662], [Dloss: 0.404082] [G loss: 0.197413] time: 0:28:36.556008\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5430/12662], [Dloss: 0.606784] [G loss: 0.549970] time: 0:28:37.907394\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5431/12662], [Dloss: 0.349654] [G loss: 0.867958] time: 0:28:39.154060\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5432/12662], [Dloss: 0.768150] [G loss: 0.469544] time: 0:28:40.392747\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5433/12662], [Dloss: 0.394535] [G loss: 0.811974] time: 0:28:41.629440\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5434/12662], [Dloss: 0.476379] [G loss: 0.513083] time: 0:28:42.902036\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5435/12662], [Dloss: 0.286685] [G loss: 0.451194] time: 0:28:44.198569\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5436/12662], [Dloss: 0.276304] [G loss: 0.356010] time: 0:28:45.511059\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5437/12662], [Dloss: 0.048800] [G loss: 0.486581] time: 0:28:46.794626\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5438/12662], [Dloss: 0.435139] [G loss: 0.225384] time: 0:28:48.079191\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5439/12662], [Dloss: 0.519050] [G loss: 0.105382] time: 0:28:49.381707\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5440/12662], [Dloss: 0.478905] [G loss: 0.064663] time: 0:28:50.721125\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5441/12662], [Dloss: 0.385509] [G loss: -0.193248] time: 0:28:51.973775\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5442/12662], [Dloss: 0.443696] [G loss: -0.067188] time: 0:28:53.235401\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5443/12662], [Dloss: 0.369016] [G loss: 0.194366] time: 0:28:54.521961\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5444/12662], [Dloss: 0.277827] [G loss: 0.176881] time: 0:28:55.830461\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5445/12662], [Dloss: 0.416441] [G loss: 0.327567] time: 0:28:57.133975\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5446/12662], [Dloss: 0.287122] [G loss: 0.494020] time: 0:28:58.447462\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5447/12662], [Dloss: 0.408464] [G loss: 0.689812] time: 0:28:59.781894\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5448/12662], [Dloss: 0.179202] [G loss: 0.470897] time: 0:29:00.996645\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5449/12662], [Dloss: 0.054670] [G loss: 0.531289] time: 0:29:02.296170\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5450/12662], [Dloss: 0.202109] [G loss: 0.195129] time: 0:29:03.596691\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5451/12662], [Dloss: 0.129181] [G loss: 0.813482] time: 0:29:04.932120\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5452/12662], [Dloss: 0.285752] [G loss: 0.377421] time: 0:29:06.197735\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5453/12662], [Dloss: 0.241276] [G loss: 0.107447] time: 0:29:07.505238\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5454/12662], [Dloss: 0.145774] [G loss: -0.290292] time: 0:29:08.840679\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5455/12662], [Dloss: 0.377988] [G loss: -0.212177] time: 0:29:10.176096\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5456/12662], [Dloss: 0.183245] [G loss: -0.248595] time: 0:29:11.514516\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5457/12662], [Dloss: 0.826849] [G loss: -0.169960] time: 0:29:12.800078\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5458/12662], [Dloss: 0.416225] [G loss: -0.012676] time: 0:29:14.080654\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5459/12662], [Dloss: 0.311276] [G loss: 0.023049] time: 0:29:15.326322\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5460/12662], [Dloss: 0.458657] [G loss: 0.258152] time: 0:29:16.637815\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5461/12662], [Dloss: 0.095936] [G loss: 0.256412] time: 0:29:17.972246\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5462/12662], [Dloss: 0.350035] [G loss: 0.332198] time: 0:29:19.260801\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5463/12662], [Dloss: -0.007494] [G loss: 0.286987] time: 0:29:20.587253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5464/12662], [Dloss: 0.229280] [G loss: 0.671577] time: 0:29:21.976537\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5465/12662], [Dloss: 0.495476] [G loss: 0.291533] time: 0:29:23.327923\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5466/12662], [Dloss: 0.419799] [G loss: 0.419072] time: 0:29:24.626453\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5467/12662], [Dloss: 0.586661] [G loss: 0.356459] time: 0:29:25.944924\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5468/12662], [Dloss: 0.061708] [G loss: 0.743617] time: 0:29:27.302294\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5469/12662], [Dloss: 0.165265] [G loss: 0.347403] time: 0:29:28.629744\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5470/12662], [Dloss: 0.103732] [G loss: -0.096088] time: 0:29:29.943231\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5471/12662], [Dloss: 0.080286] [G loss: 0.364482] time: 0:29:31.291625\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5472/12662], [Dloss: 0.236582] [G loss: -0.113212] time: 0:29:32.608105\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5473/12662], [Dloss: 0.240699] [G loss: -0.000454] time: 0:29:33.857763\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5474/12662], [Dloss: 0.689738] [G loss: -0.184012] time: 0:29:35.097447\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5475/12662], [Dloss: 0.108463] [G loss: -0.061081] time: 0:29:36.450828\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5476/12662], [Dloss: 0.238650] [G loss: -0.226677] time: 0:29:37.709462\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5477/12662], [Dloss: 0.365718] [G loss: 0.040369] time: 0:29:39.038906\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5478/12662], [Dloss: 0.219521] [G loss: 0.161703] time: 0:29:40.292553\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5479/12662], [Dloss: 0.080680] [G loss: 0.238052] time: 0:29:41.638953\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5480/12662], [Dloss: 0.048060] [G loss: 0.528144] time: 0:29:42.930499\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5481/12662], [Dloss: 0.228830] [G loss: 0.454154] time: 0:29:44.215578\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5482/12662], [Dloss: 0.068002] [G loss: 0.201312] time: 0:29:45.469729\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5483/12662], [Dloss: -0.073849] [G loss: -0.006178] time: 0:29:46.749307\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5484/12662], [Dloss: 0.438750] [G loss: -0.184556] time: 0:29:48.127620\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5485/12662], [Dloss: 0.034307] [G loss: -0.485862] time: 0:29:49.405732\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5486/12662], [Dloss: 0.440512] [G loss: -0.774948] time: 0:29:50.715721\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5487/12662], [Dloss: 0.314803] [G loss: -0.145677] time: 0:29:52.022227\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5488/12662], [Dloss: 0.469030] [G loss: -0.079555] time: 0:29:53.244956\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5489/12662], [Dloss: 0.089453] [G loss: 0.077256] time: 0:29:54.586369\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5490/12662], [Dloss: 0.380444] [G loss: -0.006785] time: 0:29:55.802118\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5491/12662], [Dloss: 0.171120] [G loss: 0.127792] time: 0:29:57.211349\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5492/12662], [Dloss: 0.449160] [G loss: 0.224401] time: 0:29:58.566724\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5493/12662], [Dloss: 0.053915] [G loss: 0.431911] time: 0:29:59.832339\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5494/12662], [Dloss: 0.514680] [G loss: 0.162391] time: 0:30:01.106931\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5495/12662], [Dloss: 0.294476] [G loss: 0.263007] time: 0:30:02.444354\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5496/12662], [Dloss: 0.337747] [G loss: 0.165580] time: 0:30:03.784769\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5497/12662], [Dloss: 0.270041] [G loss: 0.118778] time: 0:30:05.142139\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5498/12662], [Dloss: 0.121225] [G loss: 0.221038] time: 0:30:06.488539\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5499/12662], [Dloss: 0.333262] [G loss: 0.300926] time: 0:30:07.812997\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5500/12662], [Dloss: 0.172436] [G loss: 0.278821] time: 0:30:09.167374\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5500/12662], [Dloss: 0.172436] [G loss: 0.278821] time: 0:30:09.167374\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5501/12662], [Dloss: 0.364819] [G loss: 0.173313] time: 0:30:11.390429\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5502/12662], [Dloss: 0.565185] [G loss: 0.132212] time: 0:30:12.685964\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5503/12662], [Dloss: -0.042042] [G loss: 0.347639] time: 0:30:14.006433\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5504/12662], [Dloss: 0.199365] [G loss: 0.447123] time: 0:30:15.354827\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5505/12662], [Dloss: 0.560583] [G loss: 0.062933] time: 0:30:16.626426\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5506/12662], [Dloss: 0.043201] [G loss: 0.325633] time: 0:30:17.994767\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5507/12662], [Dloss: 0.216478] [G loss: 0.289031] time: 0:30:19.272350\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5508/12662], [Dloss: 0.303723] [G loss: 0.650919] time: 0:30:20.658642\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5509/12662], [Dloss: 0.253343] [G loss: 0.295820] time: 0:30:22.001052\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5510/12662], [Dloss: 0.217099] [G loss: 0.359687] time: 0:30:23.334486\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5511/12662], [Dloss: 0.067683] [G loss: 0.176637] time: 0:30:24.626032\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5512/12662], [Dloss: 0.518388] [G loss: 0.824128] time: 0:30:25.994373\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5513/12662], [Dloss: 0.514124] [G loss: 0.593537] time: 0:30:27.255002\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5514/12662], [Dloss: 0.313799] [G loss: 0.145026] time: 0:30:28.541561\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5515/12662], [Dloss: 0.335490] [G loss: 0.339342] time: 0:30:29.894941\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5516/12662], [Dloss: 0.436064] [G loss: 0.145847] time: 0:30:31.148589\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5517/12662], [Dloss: -0.123592] [G loss: -0.036916] time: 0:30:32.402236\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5518/12662], [Dloss: 0.466577] [G loss: -0.221208] time: 0:30:33.704753\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5519/12662], [Dloss: 0.355434] [G loss: -0.009563] time: 0:30:35.120965\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5520/12662], [Dloss: 0.630596] [G loss: 0.163231] time: 0:30:36.392564\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5521/12662], [Dloss: -0.009993] [G loss: -0.038039] time: 0:30:37.702062\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5522/12662], [Dloss: 0.451000] [G loss: 0.166346] time: 0:30:38.975656\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5523/12662], [Dloss: 0.233333] [G loss: -0.055853] time: 0:30:40.287149\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5524/12662], [Dloss: 0.768587] [G loss: 0.095042] time: 0:30:41.668455\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5525/12662], [Dloss: -0.044651] [G loss: 0.176690] time: 0:30:42.938059\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5526/12662], [Dloss: 0.399100] [G loss: -0.081619] time: 0:30:44.210656\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5527/12662], [Dloss: 0.261868] [G loss: 0.039799] time: 0:30:45.545087\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5528/12662], [Dloss: -0.038031] [G loss: 0.223401] time: 0:30:46.806713\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5529/12662], [Dloss: 0.319391] [G loss: 0.218955] time: 0:30:48.072328\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5530/12662], [Dloss: 0.250147] [G loss: 0.407458] time: 0:30:49.456626\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5531/12662], [Dloss: 0.508914] [G loss: 0.387479] time: 0:30:50.803025\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5532/12662], [Dloss: 0.654750] [G loss: 0.103568] time: 0:30:52.137457\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5533/12662], [Dloss: 0.934288] [G loss: 0.307148] time: 0:30:53.370160\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5534/12662], [Dloss: 0.467532] [G loss: 0.083372] time: 0:30:54.625802\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5535/12662], [Dloss: 0.093459] [G loss: 0.140131] time: 0:30:55.913359\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5536/12662], [Dloss: 0.231058] [G loss: 0.301367] time: 0:30:57.133097\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5537/12662], [Dloss: 0.344106] [G loss: 0.111458] time: 0:30:58.401704\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5538/12662], [Dloss: 0.641877] [G loss: -0.164952] time: 0:30:59.672306\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5539/12662], [Dloss: 0.402090] [G loss: 0.198439] time: 0:31:00.959863\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5540/12662], [Dloss: 0.536284] [G loss: 0.628225] time: 0:31:02.228470\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5541/12662], [Dloss: 0.216326] [G loss: 0.939615] time: 0:31:03.595813\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5542/12662], [Dloss: 0.400631] [G loss: 0.724784] time: 0:31:04.871402\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5543/12662], [Dloss: 0.008792] [G loss: 0.420472] time: 0:31:06.133028\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5544/12662], [Dloss: 0.599999] [G loss: 0.476968] time: 0:31:07.521315\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5545/12662], [Dloss: 0.716593] [G loss: 0.245425] time: 0:31:08.829816\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5546/12662], [Dloss: 0.519683] [G loss: 0.370550] time: 0:31:10.113383\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5547/12662], [Dloss: 0.402495] [G loss: 0.048073] time: 0:31:11.345089\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5548/12662], [Dloss: 0.414602] [G loss: -0.119966] time: 0:31:12.629653\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5549/12662], [Dloss: 0.588582] [G loss: -0.167123] time: 0:31:13.936159\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5550/12662], [Dloss: 0.598733] [G loss: 0.002357] time: 0:31:15.176841\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5551/12662], [Dloss: 0.081876] [G loss: 0.489755] time: 0:31:16.518254\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5552/12662], [Dloss: 0.146835] [G loss: 0.153757] time: 0:31:17.823763\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5553/12662], [Dloss: 0.195513] [G loss: -0.109795] time: 0:31:19.235986\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5554/12662], [Dloss: 0.088580] [G loss: 0.196457] time: 0:31:20.511574\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5555/12662], [Dloss: -0.118742] [G loss: 0.519147] time: 0:31:21.808107\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5556/12662], [Dloss: 0.381719] [G loss: 0.365288] time: 0:31:23.109626\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5557/12662], [Dloss: -0.168632] [G loss: 0.397318] time: 0:31:24.468991\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5558/12662], [Dloss: 0.244625] [G loss: 0.511098] time: 0:31:25.832345\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5559/12662], [Dloss: 0.141385] [G loss: -0.008618] time: 0:31:27.102947\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5560/12662], [Dloss: 0.055004] [G loss: 0.322735] time: 0:31:28.375543\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5561/12662], [Dloss: 0.412775] [G loss: 0.044001] time: 0:31:29.750866\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5562/12662], [Dloss: 0.218872] [G loss: 0.233121] time: 0:31:31.053382\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5563/12662], [Dloss: 0.369590] [G loss: 0.411110] time: 0:31:32.348917\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5564/12662], [Dloss: 0.482124] [G loss: 0.217549] time: 0:31:33.650436\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5565/12662], [Dloss: 0.424576] [G loss: 0.509463] time: 0:31:35.022766\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5566/12662], [Dloss: 0.353696] [G loss: 0.152000] time: 0:31:36.328275\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5567/12662], [Dloss: 0.104249] [G loss: 0.404854] time: 0:31:37.580925\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5568/12662], [Dloss: 0.054796] [G loss: 0.477821] time: 0:31:38.883442\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5569/12662], [Dloss: 0.210197] [G loss: 0.494105] time: 0:31:40.130107\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5570/12662], [Dloss: 0.695764] [G loss: -0.202406] time: 0:31:41.413674\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5571/12662], [Dloss: 0.062597] [G loss: 0.002825] time: 0:31:42.762069\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5572/12662], [Dloss: 0.384724] [G loss: -0.480298] time: 0:31:43.960862\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5573/12662], [Dloss: 0.259037] [G loss: -0.331889] time: 0:31:45.260387\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5574/12662], [Dloss: 0.481118] [G loss: 0.168077] time: 0:31:46.522013\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5575/12662], [Dloss: 0.529549] [G loss: -0.061562] time: 0:31:47.884370\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5576/12662], [Dloss: 0.647574] [G loss: -0.057064] time: 0:31:49.202844\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5577/12662], [Dloss: 0.505143] [G loss: 0.499625] time: 0:31:50.507355\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5578/12662], [Dloss: 0.005416] [G loss: 0.516138] time: 0:31:51.784938\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5579/12662], [Dloss: 0.039191] [G loss: 0.571648] time: 0:31:53.160260\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5580/12662], [Dloss: 0.434628] [G loss: 0.664524] time: 0:31:54.484718\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5581/12662], [Dloss: 0.396769] [G loss: 0.706582] time: 0:31:55.813166\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5582/12662], [Dloss: 0.253183] [G loss: 0.290790] time: 0:31:57.114684\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5583/12662], [Dloss: 0.248626] [G loss: 0.775223] time: 0:31:58.434156\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5584/12662], [Dloss: 0.049012] [G loss: 0.285683] time: 0:31:59.831419\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5585/12662], [Dloss: 0.062358] [G loss: -0.028647] time: 0:32:01.106010\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5586/12662], [Dloss: 0.579607] [G loss: 0.307663] time: 0:32:02.425482\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5587/12662], [Dloss: 0.053855] [G loss: 0.243616] time: 0:32:03.646217\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5588/12662], [Dloss: 0.483865] [G loss: 0.430358] time: 0:32:04.888894\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5589/12662], [Dloss: 0.519866] [G loss: 0.370574] time: 0:32:06.250253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5590/12662], [Dloss: 0.434513] [G loss: 0.108500] time: 0:32:07.518860\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5591/12662], [Dloss: 0.365009] [G loss: 0.058873] time: 0:32:08.833345\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5592/12662], [Dloss: 0.251478] [G loss: 0.304945] time: 0:32:10.159797\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5593/12662], [Dloss: 0.386676] [G loss: 0.080348] time: 0:32:11.357594\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5594/12662], [Dloss: 0.147340] [G loss: 0.039114] time: 0:32:12.669086\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5595/12662], [Dloss: 0.082040] [G loss: 0.256184] time: 0:32:14.078318\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5596/12662], [Dloss: 0.566457] [G loss: 0.126552] time: 0:32:15.402775\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5597/12662], [Dloss: 0.339304] [G loss: 0.394925] time: 0:32:16.738204\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5598/12662], [Dloss: -0.372950] [G loss: 0.272603] time: 0:32:18.034737\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5599/12662], [Dloss: 0.133384] [G loss: 0.727166] time: 0:32:19.427013\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5600/12662], [Dloss: 0.644730] [G loss: 0.563690] time: 0:32:20.724543\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5601/12662], [Dloss: 0.448760] [G loss: 0.582782] time: 0:32:22.018084\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5602/12662], [Dloss: 0.598804] [G loss: 0.201774] time: 0:32:23.351518\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5603/12662], [Dloss: 0.474991] [G loss: 0.057831] time: 0:32:24.623117\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5604/12662], [Dloss: 0.216945] [G loss: 0.288422] time: 0:32:25.960540\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5605/12662], [Dloss: 0.263110] [G loss: 0.135757] time: 0:32:27.295969\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5606/12662], [Dloss: 0.276905] [G loss: 0.119630] time: 0:32:28.685253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5607/12662], [Dloss: 0.215848] [G loss: 0.284958] time: 0:32:29.924938\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5608/12662], [Dloss: 0.390169] [G loss: 0.083121] time: 0:32:31.219476\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5609/12662], [Dloss: 0.155851] [G loss: 0.269879] time: 0:32:32.537950\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5610/12662], [Dloss: -0.058543] [G loss: 0.406296] time: 0:32:33.897315\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5611/12662], [Dloss: 0.365701] [G loss: 0.478074] time: 0:32:35.245709\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5612/12662], [Dloss: 0.612649] [G loss: 0.482410] time: 0:32:36.618038\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5613/12662], [Dloss: 0.469643] [G loss: 0.347384] time: 0:32:37.874678\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5614/12662], [Dloss: 0.365435] [G loss: 0.367563] time: 0:32:39.226063\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5615/12662], [Dloss: 0.254148] [G loss: 0.451937] time: 0:32:40.551519\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5616/12662], [Dloss: 0.658776] [G loss: 0.189642] time: 0:32:41.884953\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5617/12662], [Dloss: 0.193944] [G loss: 0.492200] time: 0:32:43.191459\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5618/12662], [Dloss: 0.157470] [G loss: 0.449337] time: 0:32:44.468045\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5619/12662], [Dloss: -0.361357] [G loss: 0.099630] time: 0:32:45.737649\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5620/12662], [Dloss: 0.318841] [G loss: 0.340508] time: 0:32:47.044155\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5621/12662], [Dloss: 0.390441] [G loss: 0.255802] time: 0:32:48.343680\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5622/12662], [Dloss: 0.050383] [G loss: 0.658972] time: 0:32:49.725983\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5623/12662], [Dloss: 0.584937] [G loss: -0.056776] time: 0:32:51.034483\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5624/12662], [Dloss: 0.032768] [G loss: 0.450554] time: 0:32:52.359940\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5625/12662], [Dloss: 0.367542] [G loss: 0.266811] time: 0:32:53.561725\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5626/12662], [Dloss: 0.157219] [G loss: 0.450399] time: 0:32:54.753537\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5627/12662], [Dloss: 0.359484] [G loss: 0.568968] time: 0:32:56.054060\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5628/12662], [Dloss: 0.178253] [G loss: 0.374754] time: 0:32:57.383504\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5629/12662], [Dloss: 0.429290] [G loss: 0.531372] time: 0:32:58.631167\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5630/12662], [Dloss: 0.176412] [G loss: 0.477543] time: 0:32:59.840932\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5631/12662], [Dloss: 0.326534] [G loss: 0.344741] time: 0:33:01.162398\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5632/12662], [Dloss: 0.390402] [G loss: -0.222374] time: 0:33:02.334264\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5633/12662], [Dloss: 0.238553] [G loss: 0.040618] time: 0:33:03.669693\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5634/12662], [Dloss: 0.480488] [G loss: -0.004061] time: 0:33:04.990162\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5635/12662], [Dloss: 0.152577] [G loss: -0.370518] time: 0:33:06.290683\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5636/12662], [Dloss: 0.189426] [G loss: -0.529590] time: 0:33:07.494464\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5637/12662], [Dloss: -0.289901] [G loss: -0.876677] time: 0:33:08.893721\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5638/12662], [Dloss: 0.709385] [G loss: -0.519177] time: 0:33:10.180281\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5639/12662], [Dloss: 0.582953] [G loss: -0.732562] time: 0:33:11.513715\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5640/12662], [Dloss: 0.274206] [G loss: -0.440235] time: 0:33:12.835181\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5641/12662], [Dloss: 0.328265] [G loss: -0.516764] time: 0:33:14.082844\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5642/12662], [Dloss: 0.106354] [G loss: -0.137589] time: 0:33:15.384363\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5643/12662], [Dloss: 0.166066] [G loss: -0.151318] time: 0:33:16.690869\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5644/12662], [Dloss: 0.653268] [G loss: 0.164910] time: 0:33:17.959476\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5645/12662], [Dloss: 0.261035] [G loss: 0.189837] time: 0:33:19.243044\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5646/12662], [Dloss: 0.500574] [G loss: 0.049722] time: 0:33:20.580467\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5647/12662], [Dloss: 0.059401] [G loss: 0.320638] time: 0:33:21.898941\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5648/12662], [Dloss: -0.039462] [G loss: -0.079346] time: 0:33:23.215420\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5649/12662], [Dloss: 0.117065] [G loss: 0.228678] time: 0:33:24.533894\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5650/12662], [Dloss: 0.369685] [G loss: 0.040885] time: 0:33:25.799509\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5651/12662], [Dloss: 0.355154] [G loss: 0.064667] time: 0:33:27.088063\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5652/12662], [Dloss: 0.386930] [G loss: 0.280591] time: 0:33:28.384596\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5653/12662], [Dloss: 0.129709] [G loss: 0.170461] time: 0:33:29.729998\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5654/12662], [Dloss: 0.582031] [G loss: 0.362950] time: 0:33:31.035507\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5655/12662], [Dloss: 0.414609] [G loss: 0.189359] time: 0:33:32.339020\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5656/12662], [Dloss: 0.330582] [G loss: 0.291941] time: 0:33:33.639542\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5657/12662], [Dloss: 0.260793] [G loss: 0.113849] time: 0:33:34.859280\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5658/12662], [Dloss: 0.142129] [G loss: 0.307359] time: 0:33:36.079019\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5659/12662], [Dloss: 0.351735] [G loss: 0.121353] time: 0:33:37.419434\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5660/12662], [Dloss: 0.203544] [G loss: 0.372760] time: 0:33:38.696020\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5661/12662], [Dloss: 0.573862] [G loss: 0.281694] time: 0:33:39.955651\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5662/12662], [Dloss: 0.436493] [G loss: 0.441666] time: 0:33:41.285096\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5663/12662], [Dloss: 0.073037] [G loss: 0.315075] time: 0:33:42.659420\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5664/12662], [Dloss: 0.036922] [G loss: 0.581197] time: 0:33:43.947974\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5665/12662], [Dloss: -0.042124] [G loss: 0.303062] time: 0:33:45.224560\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5666/12662], [Dloss: -0.148987] [G loss: 0.506653] time: 0:33:46.524084\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5667/12662], [Dloss: 0.884199] [G loss: 0.057917] time: 0:33:47.736841\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5668/12662], [Dloss: 0.407040] [G loss: 0.194560] time: 0:33:48.964558\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5669/12662], [Dloss: 0.174668] [G loss: 0.014509] time: 0:33:50.327912\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5670/12662], [Dloss: 0.626500] [G loss: 0.145880] time: 0:33:51.604498\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5671/12662], [Dloss: 0.435374] [G loss: 0.217132] time: 0:33:52.944913\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5672/12662], [Dloss: 0.295794] [G loss: 0.226666] time: 0:33:54.268374\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5673/12662], [Dloss: 0.121437] [G loss: 0.194655] time: 0:33:55.536981\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5674/12662], [Dloss: 0.243854] [G loss: 0.005191] time: 0:33:56.845482\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5675/12662], [Dloss: 0.294798] [G loss: 0.065947] time: 0:33:58.059236\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5676/12662], [Dloss: 0.046071] [G loss: -0.052078] time: 0:33:59.336819\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5677/12662], [Dloss: 0.318086] [G loss: 0.252358] time: 0:34:00.579495\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5678/12662], [Dloss: 0.383543] [G loss: 0.219726] time: 0:34:01.860071\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5679/12662], [Dloss: 0.316362] [G loss: 0.114046] time: 0:34:03.252347\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5680/12662], [Dloss: 0.347688] [G loss: -0.049258] time: 0:34:04.518960\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5681/12662], [Dloss: 0.153065] [G loss: 0.487446] time: 0:34:05.808511\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5682/12662], [Dloss: 0.320148] [G loss: 0.481594] time: 0:34:07.084100\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5683/12662], [Dloss: -0.022066] [G loss: 0.707951] time: 0:34:08.352707\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5684/12662], [Dloss: 0.291762] [G loss: 0.968392] time: 0:34:09.603362\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5685/12662], [Dloss: 0.156545] [G loss: 0.712010] time: 0:34:10.891916\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5686/12662], [Dloss: 0.588571] [G loss: 0.231018] time: 0:34:12.159526\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5687/12662], [Dloss: 0.111637] [G loss: 0.742932] time: 0:34:13.474011\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5688/12662], [Dloss: 0.421419] [G loss: 0.558189] time: 0:34:14.755584\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5689/12662], [Dloss: -0.046343] [G loss: -0.111673] time: 0:34:16.042143\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5690/12662], [Dloss: 0.154302] [G loss: -0.220187] time: 0:34:17.390537\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5691/12662], [Dloss: 0.569902] [G loss: -0.562804] time: 0:34:18.663133\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5692/12662], [Dloss: 0.151238] [G loss: -0.689133] time: 0:34:19.897831\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5693/12662], [Dloss: 0.358716] [G loss: -0.198217] time: 0:34:21.199350\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5694/12662], [Dloss: 0.683714] [G loss: -0.172246] time: 0:34:22.507851\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5695/12662], [Dloss: 0.383590] [G loss: -0.282242] time: 0:34:23.722602\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5696/12662], [Dloss: 0.402738] [G loss: -0.069200] time: 0:34:25.019135\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5697/12662], [Dloss: 0.098405] [G loss: 0.169867] time: 0:34:26.422382\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5698/12662], [Dloss: 0.016569] [G loss: 0.117778] time: 0:34:27.628157\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5699/12662], [Dloss: 0.114591] [G loss: -0.070682] time: 0:34:28.916711\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5700/12662], [Dloss: 0.525661] [G loss: -0.071749] time: 0:34:30.307991\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5701/12662], [Dloss: 0.051800] [G loss: 0.257850] time: 0:34:31.591558\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5702/12662], [Dloss: 0.190939] [G loss: 0.043784] time: 0:34:32.885099\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5703/12662], [Dloss: 0.306972] [G loss: -0.277924] time: 0:34:34.252442\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5704/12662], [Dloss: 0.409544] [G loss: -0.053748] time: 0:34:35.582884\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5705/12662], [Dloss: 0.609086] [G loss: 0.039785] time: 0:34:36.812595\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5706/12662], [Dloss: -0.146840] [G loss: 0.284012] time: 0:34:38.114114\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5707/12662], [Dloss: 0.629718] [G loss: 0.005666] time: 0:34:39.325874\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5708/12662], [Dloss: 0.051749] [G loss: 0.167134] time: 0:34:40.585505\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5709/12662], [Dloss: 0.312793] [G loss: 0.173553] time: 0:34:41.821200\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5710/12662], [Dloss: 0.438707] [G loss: 0.164257] time: 0:34:43.148650\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5711/12662], [Dloss: 0.519439] [G loss: 0.260264] time: 0:34:44.401300\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5712/12662], [Dloss: -0.003666] [G loss: 0.178367] time: 0:34:45.745705\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5713/12662], [Dloss: 0.102775] [G loss: 0.594389] time: 0:34:47.079139\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5714/12662], [Dloss: 0.218296] [G loss: 0.479815] time: 0:34:48.350738\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5715/12662], [Dloss: 0.248106] [G loss: 0.192994] time: 0:34:49.624332\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5716/12662], [Dloss: 0.368480] [G loss: 0.288368] time: 0:34:50.960758\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5717/12662], [Dloss: 0.649495] [G loss: 0.776487] time: 0:34:52.349045\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5718/12662], [Dloss: 0.304693] [G loss: 0.353562] time: 0:34:53.542852\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5719/12662], [Dloss: -0.175359] [G loss: 0.449914] time: 0:34:54.973028\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5720/12662], [Dloss: 0.467473] [G loss: 0.311913] time: 0:34:56.165838\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5721/12662], [Dloss: 0.379250] [G loss: 0.048514] time: 0:34:57.469351\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5722/12662], [Dloss: 0.315864] [G loss: 0.174028] time: 0:34:58.764887\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5723/12662], [Dloss: 0.475869] [G loss: 0.260047] time: 0:35:00.024518\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5724/12662], [Dloss: 0.307227] [G loss: 0.526892] time: 0:35:01.311077\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5725/12662], [Dloss: -0.003538] [G loss: 0.736713] time: 0:35:02.578687\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5726/12662], [Dloss: 0.500289] [G loss: 0.682435] time: 0:35:03.847295\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5727/12662], [Dloss: 0.067091] [G loss: 0.388270] time: 0:35:05.146819\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5728/12662], [Dloss: -0.099620] [G loss: 0.741188] time: 0:35:06.378525\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5729/12662], [Dloss: 0.592397] [G loss: 0.069377] time: 0:35:07.702983\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5730/12662], [Dloss: 0.405219] [G loss: 0.035031] time: 0:35:08.980566\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5731/12662], [Dloss: 0.460712] [G loss: 0.143407] time: 0:35:10.315995\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5732/12662], [Dloss: 0.373580] [G loss: 0.327704] time: 0:35:11.625493\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5733/12662], [Dloss: 0.148575] [G loss: 0.434217] time: 0:35:12.967903\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5734/12662], [Dloss: -0.134465] [G loss: -0.260361] time: 0:35:14.269422\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5735/12662], [Dloss: 0.565440] [G loss: 0.004952] time: 0:35:15.585901\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5736/12662], [Dloss: 0.518963] [G loss: 0.388555] time: 0:35:16.987154\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5737/12662], [Dloss: 0.431416] [G loss: 0.290713] time: 0:35:18.216865\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5738/12662], [Dloss: 0.566364] [G loss: 0.374085] time: 0:35:19.462534\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5739/12662], [Dloss: 0.388198] [G loss: 0.200409] time: 0:35:20.702218\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5740/12662], [Dloss: 0.290571] [G loss: -0.044607] time: 0:35:22.095492\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5741/12662], [Dloss: 0.302546] [G loss: 0.232324] time: 0:35:23.401001\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5742/12662], [Dloss: 0.307645] [G loss: 0.185900] time: 0:35:24.706510\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5743/12662], [Dloss: 0.194732] [G loss: 0.462992] time: 0:35:25.896328\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5744/12662], [Dloss: 0.490599] [G loss: 0.243968] time: 0:35:27.164935\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5745/12662], [Dloss: 0.663521] [G loss: 0.178415] time: 0:35:28.439526\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5746/12662], [Dloss: 0.283932] [G loss: 0.052154] time: 0:35:29.755008\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5747/12662], [Dloss: 0.408036] [G loss: 0.045322] time: 0:35:31.047551\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5748/12662], [Dloss: 0.082312] [G loss: 0.139338] time: 0:35:32.288233\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5749/12662], [Dloss: 0.187193] [G loss: 0.307385] time: 0:35:33.547865\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5750/12662], [Dloss: 0.406408] [G loss: 0.340093] time: 0:35:34.765608\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5751/12662], [Dloss: 0.433245] [G loss: 0.352733] time: 0:35:36.072114\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5752/12662], [Dloss: 0.214816] [G loss: 0.639663] time: 0:35:37.352689\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5753/12662], [Dloss: 0.285237] [G loss: 0.515971] time: 0:35:38.684128\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5754/12662], [Dloss: -0.008623] [G loss: 0.626464] time: 0:35:40.024544\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5755/12662], [Dloss: 0.272700] [G loss: 0.424042] time: 0:35:41.331050\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5756/12662], [Dloss: 0.228828] [G loss: 0.684936] time: 0:35:42.658499\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5757/12662], [Dloss: 0.186889] [G loss: 0.398129] time: 0:35:43.932094\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5758/12662], [Dloss: 0.256886] [G loss: 0.537665] time: 0:35:45.329357\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5759/12662], [Dloss: 0.443718] [G loss: 0.234991] time: 0:35:46.640850\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5760/12662], [Dloss: 0.238505] [G loss: 0.255507] time: 0:35:47.966305\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5761/12662], [Dloss: 0.529046] [G loss: -0.072296] time: 0:35:49.232917\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5762/12662], [Dloss: 0.449508] [G loss: 0.279299] time: 0:35:50.508506\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5763/12662], [Dloss: 0.443515] [G loss: 0.009952] time: 0:35:51.739215\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5764/12662], [Dloss: 0.448030] [G loss: 0.695722] time: 0:35:53.014803\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5765/12662], [Dloss: 0.536147] [G loss: 0.557068] time: 0:35:54.248504\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5766/12662], [Dloss: 0.458788] [G loss: 0.261023] time: 0:35:55.561991\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5767/12662], [Dloss: 0.289095] [G loss: 0.179515] time: 0:35:56.880465\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5768/12662], [Dloss: 0.212532] [G loss: 0.179022] time: 0:35:58.126133\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5769/12662], [Dloss: 0.222213] [G loss: -0.153915] time: 0:35:59.472533\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5770/12662], [Dloss: 0.286495] [G loss: 0.258744] time: 0:36:00.840873\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5771/12662], [Dloss: 0.339418] [G loss: 0.000441] time: 0:36:02.109481\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5772/12662], [Dloss: 0.129768] [G loss: 0.507745] time: 0:36:03.386067\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5773/12662], [Dloss: 0.129644] [G loss: 0.454941] time: 0:36:04.661655\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5774/12662], [Dloss: 0.222432] [G loss: -0.146712] time: 0:36:05.941233\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5775/12662], [Dloss: 0.275031] [G loss: 0.638862] time: 0:36:07.222806\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5776/12662], [Dloss: 0.293453] [G loss: -0.101746] time: 0:36:08.553248\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5777/12662], [Dloss: 0.048834] [G loss: 0.448030] time: 0:36:09.848783\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5778/12662], [Dloss: 0.546924] [G loss: 0.278726] time: 0:36:11.151300\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5779/12662], [Dloss: 0.408581] [G loss: 0.626219] time: 0:36:12.542579\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5780/12662], [Dloss: 0.307710] [G loss: 0.451538] time: 0:36:13.798221\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5781/12662], [Dloss: -0.186259] [G loss: 0.436558] time: 0:36:15.050871\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5782/12662], [Dloss: 0.258017] [G loss: 0.177377] time: 0:36:16.303521\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5783/12662], [Dloss: 0.441039] [G loss: 0.273958] time: 0:36:17.550187\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5784/12662], [Dloss: 0.362308] [G loss: 0.356178] time: 0:36:18.870655\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5785/12662], [Dloss: 0.562488] [G loss: 0.202202] time: 0:36:20.183145\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5786/12662], [Dloss: 0.276536] [G loss: -0.114199] time: 0:36:21.507603\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5787/12662], [Dloss: 0.616156] [G loss: 0.073502] time: 0:36:22.796157\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5788/12662], [Dloss: 0.206996] [G loss: 0.265801] time: 0:36:24.023874\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5789/12662], [Dloss: 0.049245] [G loss: 0.182604] time: 0:36:25.266550\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5790/12662], [Dloss: 0.428127] [G loss: 0.125345] time: 0:36:26.523190\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5791/12662], [Dloss: 0.151259] [G loss: 0.139108] time: 0:36:27.836677\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5792/12662], [Dloss: 0.500008] [G loss: -0.111799] time: 0:36:29.083343\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5793/12662], [Dloss: 0.366855] [G loss: -0.325942] time: 0:36:30.290116\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5794/12662], [Dloss: 0.030668] [G loss: -0.197395] time: 0:36:31.605598\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5795/12662], [Dloss: 0.627263] [G loss: -0.344418] time: 0:36:32.897144\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5796/12662], [Dloss: 0.014926] [G loss: -0.717508] time: 0:36:34.104913\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5797/12662], [Dloss: 0.366385] [G loss: -0.205360] time: 0:36:35.458294\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5798/12662], [Dloss: 0.327538] [G loss: -0.432428] time: 0:36:36.724906\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5799/12662], [Dloss: 0.272851] [G loss: -0.111090] time: 0:36:38.112198\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5800/12662], [Dloss: 0.558946] [G loss: 0.247050] time: 0:36:39.381801\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5801/12662], [Dloss: 0.493556] [G loss: 0.460422] time: 0:36:40.665368\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5802/12662], [Dloss: 0.024656] [G loss: 0.645876] time: 0:36:41.899069\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5803/12662], [Dloss: 0.529404] [G loss: 0.258331] time: 0:36:43.184631\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5804/12662], [Dloss: 0.132629] [G loss: 0.637752] time: 0:36:44.492134\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5805/12662], [Dloss: 0.327580] [G loss: 0.200793] time: 0:36:45.784678\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5806/12662], [Dloss: 0.402321] [G loss: -0.283629] time: 0:36:47.175957\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5807/12662], [Dloss: 0.654639] [G loss: -0.136858] time: 0:36:48.447556\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5808/12662], [Dloss: 0.124659] [G loss: 0.057524] time: 0:36:49.704195\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5809/12662], [Dloss: -0.032054] [G loss: 0.257307] time: 0:36:51.021672\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5810/12662], [Dloss: 0.212424] [G loss: 0.134054] time: 0:36:52.343138\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5811/12662], [Dloss: 0.126933] [G loss: -0.016800] time: 0:36:53.609751\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5812/12662], [Dloss: 0.266630] [G loss: -0.104042] time: 0:36:54.869382\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5813/12662], [Dloss: 0.124286] [G loss: 0.103173] time: 0:36:56.137989\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5814/12662], [Dloss: 0.321242] [G loss: -0.115379] time: 0:36:57.343765\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5815/12662], [Dloss: 0.208455] [G loss: 0.361113] time: 0:36:58.598409\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5816/12662], [Dloss: -0.053029] [G loss: 0.128508] time: 0:36:59.894942\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5817/12662], [Dloss: 0.228141] [G loss: 0.182109] time: 0:37:01.183495\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5818/12662], [Dloss: 0.271140] [G loss: -0.121246] time: 0:37:02.439137\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5819/12662], [Dloss: 0.299932] [G loss: 0.016480] time: 0:37:03.756614\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5820/12662], [Dloss: 0.286836] [G loss: -0.095250] time: 0:37:05.054144\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5821/12662], [Dloss: 0.166641] [G loss: 0.089732] time: 0:37:06.281861\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5822/12662], [Dloss: 0.398190] [G loss: -0.145071] time: 0:37:07.561439\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5823/12662], [Dloss: 0.183158] [G loss: 0.329903] time: 0:37:08.820073\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5824/12662], [Dloss: 0.544667] [G loss: 0.023511] time: 0:37:10.154504\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5825/12662], [Dloss: 0.126723] [G loss: -0.217516] time: 0:37:11.436076\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5826/12662], [Dloss: 0.272077] [G loss: 0.296047] time: 0:37:12.700694\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5827/12662], [Dloss: 0.215610] [G loss: -0.100407] time: 0:37:13.909462\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5828/12662], [Dloss: 0.247921] [G loss: -0.264490] time: 0:37:15.222949\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5829/12662], [Dloss: 0.169334] [G loss: 0.323692] time: 0:37:16.397807\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5830/12662], [Dloss: 0.352565] [G loss: -0.233355] time: 0:37:17.652452\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5831/12662], [Dloss: 0.137169] [G loss: -0.001464] time: 0:37:18.947987\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5832/12662], [Dloss: 0.384157] [G loss: 0.077055] time: 0:37:20.279426\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5833/12662], [Dloss: 0.352917] [G loss: -0.072560] time: 0:37:21.569975\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5834/12662], [Dloss: 0.400614] [G loss: -0.094406] time: 0:37:22.939313\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5835/12662], [Dloss: 0.592155] [G loss: 0.227099] time: 0:37:24.220885\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5836/12662], [Dloss: 0.122418] [G loss: 0.126795] time: 0:37:25.521407\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5837/12662], [Dloss: 0.293152] [G loss: 0.246414] time: 0:37:26.788020\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5838/12662], [Dloss: 0.234945] [G loss: 0.224654] time: 0:37:28.128435\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5839/12662], [Dloss: 0.452601] [G loss: 0.082354] time: 0:37:29.351165\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5840/12662], [Dloss: 0.347320] [G loss: 0.170423] time: 0:37:30.672631\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5841/12662], [Dloss: -0.001059] [G loss: 0.193040] time: 0:37:31.969164\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5842/12662], [Dloss: 0.432477] [G loss: -0.284791] time: 0:37:33.240763\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5843/12662], [Dloss: 0.795271] [G loss: -0.117710] time: 0:37:34.497402\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5844/12662], [Dloss: 0.524066] [G loss: -0.092369] time: 0:37:35.811887\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5845/12662], [Dloss: 0.513800] [G loss: 0.238572] time: 0:37:37.178233\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5846/12662], [Dloss: 0.533023] [G loss: 0.056262] time: 0:37:38.462797\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5847/12662], [Dloss: 0.163512] [G loss: 0.195613] time: 0:37:39.689517\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5848/12662], [Dloss: -0.146604] [G loss: 0.333407] time: 0:37:40.972087\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5849/12662], [Dloss: 0.159747] [G loss: 0.041082] time: 0:37:42.185841\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5850/12662], [Dloss: 0.209963] [G loss: 0.374150] time: 0:37:43.458437\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5851/12662], [Dloss: 0.404168] [G loss: 0.517354] time: 0:37:44.671194\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5852/12662], [Dloss: 0.128868] [G loss: 0.352814] time: 0:37:46.020585\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5853/12662], [Dloss: 0.206210] [G loss: 0.201786] time: 0:37:47.345043\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5854/12662], [Dloss: 0.073505] [G loss: 0.178870] time: 0:37:48.667506\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5855/12662], [Dloss: 0.525165] [G loss: -0.286170] time: 0:37:49.912178\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5856/12662], [Dloss: 0.170024] [G loss: -0.248394] time: 0:37:51.246609\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5857/12662], [Dloss: 0.381267] [G loss: -0.216035] time: 0:37:52.534165\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5858/12662], [Dloss: 0.393828] [G loss: -0.142203] time: 0:37:53.817733\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5859/12662], [Dloss: 0.246622] [G loss: -0.086212] time: 0:37:55.104292\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5860/12662], [Dloss: 0.372690] [G loss: -0.354573] time: 0:37:56.449694\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5861/12662], [Dloss: 0.087578] [G loss: 0.094556] time: 0:37:57.727277\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5862/12662], [Dloss: 0.157247] [G loss: 0.087364] time: 0:37:59.074674\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5863/12662], [Dloss: 0.410294] [G loss: 0.262508] time: 0:38:00.369212\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5864/12662], [Dloss: 0.179464] [G loss: 0.316454] time: 0:38:01.684694\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5865/12662], [Dloss: 0.034718] [G loss: 0.150262] time: 0:38:02.976240\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5866/12662], [Dloss: 0.024895] [G loss: 0.633946] time: 0:38:04.226895\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5867/12662], [Dloss: 0.329759] [G loss: 0.544820] time: 0:38:05.507470\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5868/12662], [Dloss: 0.231255] [G loss: 0.503678] time: 0:38:06.737182\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5869/12662], [Dloss: 0.441550] [G loss: 0.701228] time: 0:38:08.072610\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5870/12662], [Dloss: 0.004021] [G loss: 0.414767] time: 0:38:09.432972\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5871/12662], [Dloss: 0.491634] [G loss: 0.394827] time: 0:38:10.731499\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5872/12662], [Dloss: 0.466015] [G loss: -0.044616] time: 0:38:11.955227\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5873/12662], [Dloss: 0.197932] [G loss: 0.138504] time: 0:38:13.295642\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5874/12662], [Dloss: 0.185833] [G loss: 0.038150] time: 0:38:14.492441\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5875/12662], [Dloss: 0.489372] [G loss: -0.330731] time: 0:38:15.733123\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5876/12662], [Dloss: 0.166247] [G loss: -0.093958] time: 0:38:17.104456\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5877/12662], [Dloss: 0.256655] [G loss: 0.333887] time: 0:38:18.326189\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5878/12662], [Dloss: 0.161881] [G loss: 0.286350] time: 0:38:19.635687\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5879/12662], [Dloss: 0.471262] [G loss: 0.163576] time: 0:38:20.913270\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5880/12662], [Dloss: 0.121997] [G loss: 0.341826] time: 0:38:22.176890\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5881/12662], [Dloss: -0.103935] [G loss: 0.179743] time: 0:38:23.539247\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5882/12662], [Dloss: 0.550488] [G loss: 0.068985] time: 0:38:24.821817\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5883/12662], [Dloss: 0.491148] [G loss: 0.101006] time: 0:38:26.061501\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5884/12662], [Dloss: 0.040517] [G loss: 0.607790] time: 0:38:27.339085\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5885/12662], [Dloss: 0.320092] [G loss: 0.399325] time: 0:38:28.586748\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5886/12662], [Dloss: 0.136245] [G loss: 0.308821] time: 0:38:29.881286\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5887/12662], [Dloss: 0.267958] [G loss: 0.037288] time: 0:38:31.133936\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5888/12662], [Dloss: 0.582417] [G loss: 0.132243] time: 0:38:32.348687\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5889/12662], [Dloss: 0.165963] [G loss: 0.184450] time: 0:38:33.687108\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5890/12662], [Dloss: 0.383297] [G loss: 0.192298] time: 0:38:34.924798\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5891/12662], [Dloss: 0.424540] [G loss: -0.228746] time: 0:38:36.250253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5892/12662], [Dloss: 0.350902] [G loss: -0.007777] time: 0:38:37.570722\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5893/12662], [Dloss: -0.223424] [G loss: 0.280707] time: 0:38:38.802428\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5894/12662], [Dloss: 0.159412] [G loss: 0.154319] time: 0:38:40.052085\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5895/12662], [Dloss: 0.657403] [G loss: 0.143348] time: 0:38:41.299749\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5896/12662], [Dloss: 0.307297] [G loss: 0.585975] time: 0:38:42.420751\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5897/12662], [Dloss: 0.042159] [G loss: 0.803272] time: 0:38:43.672404\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5898/12662], [Dloss: 0.603357] [G loss: 0.235619] time: 0:38:44.978910\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5899/12662], [Dloss: 0.489506] [G loss: 0.322591] time: 0:38:46.259485\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5900/12662], [Dloss: -0.139795] [G loss: 0.425399] time: 0:38:47.510140\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5901/12662], [Dloss: 0.430419] [G loss: 0.440878] time: 0:38:48.823628\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5902/12662], [Dloss: 0.152620] [G loss: 0.628318] time: 0:38:50.160053\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5903/12662], [Dloss: 0.501240] [G loss: 0.072468] time: 0:38:51.374805\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5904/12662], [Dloss: 0.073647] [G loss: 0.154308] time: 0:38:52.735167\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5905/12662], [Dloss: 0.398103] [G loss: -0.129904] time: 0:38:54.068600\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5906/12662], [Dloss: 0.416384] [G loss: -0.481297] time: 0:38:55.414002\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5907/12662], [Dloss: 0.577236] [G loss: -0.567221] time: 0:38:56.717516\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5908/12662], [Dloss: 0.655312] [G loss: -0.420118] time: 0:38:58.028012\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5909/12662], [Dloss: 0.629847] [G loss: -0.251422] time: 0:38:59.346485\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5910/12662], [Dloss: 0.674798] [G loss: -0.046837] time: 0:39:00.603125\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5911/12662], [Dloss: 0.526495] [G loss: 0.109229] time: 0:39:01.917609\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5912/12662], [Dloss: 0.359078] [G loss: 0.378555] time: 0:39:03.186217\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5913/12662], [Dloss: 0.330918] [G loss: 0.633875] time: 0:39:04.480755\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5914/12662], [Dloss: 0.391299] [G loss: 0.795504] time: 0:39:05.707474\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5915/12662], [Dloss: 0.285010] [G loss: 0.414571] time: 0:39:07.064844\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5916/12662], [Dloss: 0.310038] [G loss: 0.652610] time: 0:39:08.384315\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5917/12662], [Dloss: 0.220293] [G loss: 0.306110] time: 0:39:09.639957\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5918/12662], [Dloss: 0.161287] [G loss: 0.154044] time: 0:39:10.908564\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5919/12662], [Dloss: 0.448335] [G loss: 0.227219] time: 0:39:12.113342\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5920/12662], [Dloss: 0.229080] [G loss: -0.068916] time: 0:39:13.469715\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5921/12662], [Dloss: 0.689910] [G loss: -0.270677] time: 0:39:14.718375\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5922/12662], [Dloss: 0.093406] [G loss: 0.028031] time: 0:39:16.021890\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5923/12662], [Dloss: 0.202143] [G loss: -0.341266] time: 0:39:17.333382\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5924/12662], [Dloss: 0.342985] [G loss: -0.409999] time: 0:39:18.612960\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5925/12662], [Dloss: 0.181996] [G loss: 0.311337] time: 0:39:19.928442\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5926/12662], [Dloss: 0.398559] [G loss: 0.499207] time: 0:39:21.053433\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5927/12662], [Dloss: 0.152112] [G loss: 0.489987] time: 0:39:22.327027\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5928/12662], [Dloss: -0.240888] [G loss: 0.880641] time: 0:39:23.650488\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5929/12662], [Dloss: 0.586915] [G loss: 0.680645] time: 0:39:24.890173\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5930/12662], [Dloss: 0.382679] [G loss: 0.317165] time: 0:39:26.153793\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5931/12662], [Dloss: 0.300027] [G loss: 0.190134] time: 0:39:27.468278\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5932/12662], [Dloss: 0.822080] [G loss: -0.023271] time: 0:39:28.687018\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5933/12662], [Dloss: 0.067027] [G loss: -0.282626] time: 0:39:29.899775\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5934/12662], [Dloss: 0.346951] [G loss: -0.032389] time: 0:39:31.171375\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5935/12662], [Dloss: 0.213060] [G loss: -0.127222] time: 0:39:32.433000\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5936/12662], [Dloss: 0.198354] [G loss: 0.056512] time: 0:39:33.674680\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5937/12662], [Dloss: 0.156440] [G loss: -0.319930] time: 0:39:35.042023\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5938/12662], [Dloss: 0.335394] [G loss: -0.018271] time: 0:39:36.301654\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 5939/12662], [Dloss: 0.049121] [G loss: 0.080715] time: 0:39:37.621125\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5940/12662], [Dloss: 0.510470] [G loss: -0.258129] time: 0:39:38.890730\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5941/12662], [Dloss: 0.382705] [G loss: 0.072207] time: 0:39:40.178286\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5942/12662], [Dloss: 0.223485] [G loss: 0.105562] time: 0:39:41.409993\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5943/12662], [Dloss: -0.054937] [G loss: 0.346175] time: 0:39:42.744424\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5944/12662], [Dloss: 0.064320] [G loss: 0.561732] time: 0:39:44.060903\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5945/12662], [Dloss: 0.166791] [G loss: 0.427416] time: 0:39:45.321532\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5946/12662], [Dloss: 0.420679] [G loss: 0.562927] time: 0:39:46.574182\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5947/12662], [Dloss: 0.362591] [G loss: 0.275716] time: 0:39:47.882682\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5948/12662], [Dloss: 0.040124] [G loss: 0.350574] time: 0:39:49.119375\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5949/12662], [Dloss: 0.264517] [G loss: -0.001960] time: 0:39:50.421892\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5950/12662], [Dloss: 0.448825] [G loss: -0.582407] time: 0:39:51.662573\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5951/12662], [Dloss: 0.239330] [G loss: -0.482104] time: 0:39:52.970077\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5952/12662], [Dloss: 0.372993] [G loss: -0.557835] time: 0:39:54.211756\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5953/12662], [Dloss: 0.569198] [G loss: -0.371040] time: 0:39:55.451441\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5954/12662], [Dloss: -0.031928] [G loss: -0.274485] time: 0:39:56.687136\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5955/12662], [Dloss: 0.087730] [G loss: -0.176735] time: 0:39:57.888922\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5956/12662], [Dloss: 0.212755] [G loss: -0.449465] time: 0:39:59.184457\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5957/12662], [Dloss: 0.763671] [G loss: 0.092529] time: 0:40:00.443091\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5958/12662], [Dloss: -0.030181] [G loss: 0.473713] time: 0:40:01.728653\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5959/12662], [Dloss: 0.418830] [G loss: 0.016716] time: 0:40:03.034162\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5960/12662], [Dloss: 0.148579] [G loss: 0.071785] time: 0:40:04.296785\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5961/12662], [Dloss: 0.217417] [G loss: 0.426240] time: 0:40:05.506550\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5962/12662], [Dloss: 0.142610] [G loss: -0.218654] time: 0:40:06.697365\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5963/12662], [Dloss: 0.408328] [G loss: 0.328352] time: 0:40:07.949018\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5964/12662], [Dloss: 0.433096] [G loss: -0.113216] time: 0:40:09.194686\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5965/12662], [Dloss: 0.090335] [G loss: -0.097383] time: 0:40:10.512163\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5966/12662], [Dloss: 0.265353] [G loss: -0.194122] time: 0:40:11.737885\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5967/12662], [Dloss: 0.345890] [G loss: -0.097829] time: 0:40:13.007490\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5968/12662], [Dloss: 0.077137] [G loss: -0.161918] time: 0:40:14.308011\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5969/12662], [Dloss: 0.410657] [G loss: -0.237715] time: 0:40:15.588587\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5970/12662], [Dloss: -0.074498] [G loss: -0.243968] time: 0:40:16.752474\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5971/12662], [Dloss: 0.551219] [G loss: -0.117689] time: 0:40:18.004127\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5972/12662], [Dloss: -0.014608] [G loss: -0.497691] time: 0:40:19.310633\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5973/12662], [Dloss: 0.341416] [G loss: 0.214842] time: 0:40:20.647058\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5974/12662], [Dloss: 0.458806] [G loss: -0.140326] time: 0:40:21.881756\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5975/12662], [Dloss: 0.252229] [G loss: 0.041564] time: 0:40:23.083542\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5976/12662], [Dloss: -0.117398] [G loss: 0.029884] time: 0:40:24.401019\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5977/12662], [Dloss: 0.495275] [G loss: -0.054769] time: 0:40:25.742432\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5978/12662], [Dloss: 0.240002] [G loss: -0.111111] time: 0:40:27.062900\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5979/12662], [Dloss: -0.035733] [G loss: -0.036782] time: 0:40:28.363422\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5980/12662], [Dloss: 0.270644] [G loss: -0.247158] time: 0:40:29.629038\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5981/12662], [Dloss: 0.203830] [G loss: 0.306007] time: 0:40:30.894653\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5982/12662], [Dloss: 0.480306] [G loss: 0.373944] time: 0:40:32.171239\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5983/12662], [Dloss: 0.511891] [G loss: 0.139057] time: 0:40:33.410923\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5984/12662], [Dloss: 0.447390] [G loss: 0.288538] time: 0:40:34.701472\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5985/12662], [Dloss: 0.313672] [G loss: 0.694606] time: 0:40:36.007978\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5986/12662], [Dloss: 0.560600] [G loss: 0.416407] time: 0:40:37.296532\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5987/12662], [Dloss: 0.572544] [G loss: 0.601557] time: 0:40:38.470393\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5988/12662], [Dloss: 0.174127] [G loss: 0.298473] time: 0:40:39.729027\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5989/12662], [Dloss: 0.447451] [G loss: 0.351437] time: 0:40:41.028551\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5990/12662], [Dloss: 0.139806] [G loss: 0.464705] time: 0:40:42.291175\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5991/12662], [Dloss: 0.037870] [G loss: 0.016125] time: 0:40:43.607654\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5992/12662], [Dloss: 0.228889] [G loss: -0.056428] time: 0:40:44.937098\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5993/12662], [Dloss: 0.464418] [G loss: -0.115754] time: 0:40:46.229642\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5994/12662], [Dloss: -0.052122] [G loss: -0.183753] time: 0:40:47.471321\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5995/12662], [Dloss: 0.328971] [G loss: 0.057269] time: 0:40:48.757880\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5996/12662], [Dloss: 0.378668] [G loss: -0.232693] time: 0:40:50.052418\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5997/12662], [Dloss: 0.371611] [G loss: 0.023032] time: 0:40:51.297090\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5998/12662], [Dloss: 0.506081] [G loss: -0.274511] time: 0:40:52.516827\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 5999/12662], [Dloss: 0.201538] [G loss: 0.419183] time: 0:40:53.824331\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6000/12662], [Dloss: 0.393205] [G loss: 0.157292] time: 0:40:55.077978\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6000/12662], [Dloss: 0.393205] [G loss: 0.157292] time: 0:40:55.077978\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6001/12662], [Dloss: 0.120802] [G loss: 0.724438] time: 0:41:04.595525\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6002/12662], [Dloss: 0.475502] [G loss: 0.465336] time: 0:41:05.898041\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6003/12662], [Dloss: 0.214161] [G loss: 0.541048] time: 0:41:07.134734\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6004/12662], [Dloss: 0.439928] [G loss: -0.028666] time: 0:41:08.479139\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6005/12662], [Dloss: 0.429201] [G loss: -0.265179] time: 0:41:09.793623\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6006/12662], [Dloss: 0.168691] [G loss: -0.090019] time: 0:41:11.047270\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6007/12662], [Dloss: 0.253185] [G loss: -0.196237] time: 0:41:12.327846\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6008/12662], [Dloss: 0.408804] [G loss: -0.195446] time: 0:41:13.550576\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6009/12662], [Dloss: 0.562314] [G loss: 0.050154] time: 0:41:14.704490\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6010/12662], [Dloss: 0.078302] [G loss: 0.295493] time: 0:41:16.031940\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6011/12662], [Dloss: 0.588387] [G loss: 0.551702] time: 0:41:17.312515\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6012/12662], [Dloss: 0.338635] [G loss: 0.165676] time: 0:41:18.563170\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6013/12662], [Dloss: 0.196422] [G loss: 0.505635] time: 0:41:19.816817\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6014/12662], [Dloss: 0.148022] [G loss: 0.536629] time: 0:41:21.129307\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6015/12662], [Dloss: 0.515622] [G loss: 0.591702] time: 0:41:22.459750\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6016/12662], [Dloss: -0.098622] [G loss: 0.606808] time: 0:41:23.686469\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6017/12662], [Dloss: 0.522478] [G loss: 0.344954] time: 0:41:24.903215\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6018/12662], [Dloss: 0.198530] [G loss: 0.249905] time: 0:41:26.217699\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6019/12662], [Dloss: 0.562181] [G loss: -0.128220] time: 0:41:27.529192\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6020/12662], [Dloss: 0.187213] [G loss: 0.121837] time: 0:41:28.769874\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6021/12662], [Dloss: 0.320900] [G loss: -0.251227] time: 0:41:30.017537\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6022/12662], [Dloss: 0.206925] [G loss: -0.287852] time: 0:41:31.343990\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6023/12662], [Dloss: 0.792909] [G loss: -0.213188] time: 0:41:32.606613\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6024/12662], [Dloss: 0.576445] [G loss: -0.206161] time: 0:41:33.854276\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6025/12662], [Dloss: 0.112897] [G loss: 0.304423] time: 0:41:35.066036\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6026/12662], [Dloss: 0.448059] [G loss: 0.543904] time: 0:41:36.391491\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6027/12662], [Dloss: 0.526872] [G loss: 0.432145] time: 0:41:37.705976\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6028/12662], [Dloss: 0.314986] [G loss: 0.593606] time: 0:41:38.977575\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6029/12662], [Dloss: 0.548668] [G loss: 0.729476] time: 0:41:40.234214\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6030/12662], [Dloss: 0.481300] [G loss: 0.484581] time: 0:41:41.547701\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6031/12662], [Dloss: 0.111725] [G loss: 0.506895] time: 0:41:42.769434\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6032/12662], [Dloss: 0.231840] [G loss: 0.088634] time: 0:41:44.018095\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6033/12662], [Dloss: 0.567131] [G loss: 0.012321] time: 0:41:45.257779\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6034/12662], [Dloss: -0.059580] [G loss: 0.024131] time: 0:41:46.535362\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6035/12662], [Dloss: 0.339094] [G loss: 0.063104] time: 0:41:47.736151\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6036/12662], [Dloss: 0.152359] [G loss: -0.284351] time: 0:41:48.994785\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6037/12662], [Dloss: 0.186092] [G loss: -0.034884] time: 0:41:50.208539\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6038/12662], [Dloss: 0.484708] [G loss: 0.186742] time: 0:41:51.457200\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6039/12662], [Dloss: 0.163987] [G loss: 0.368784] time: 0:41:52.716831\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6040/12662], [Dloss: 0.331949] [G loss: 0.285609] time: 0:41:54.013364\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6041/12662], [Dloss: 0.285281] [G loss: 0.487516] time: 0:41:55.271000\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6042/12662], [Dloss: 0.081096] [G loss: 0.089959] time: 0:41:56.522653\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6043/12662], [Dloss: 0.417845] [G loss: 0.313462] time: 0:41:57.817191\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6044/12662], [Dloss: 0.120420] [G loss: 0.451046] time: 0:41:59.088790\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6045/12662], [Dloss: 0.327120] [G loss: 0.200045] time: 0:42:00.347424\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6046/12662], [Dloss: 0.231035] [G loss: 0.076140] time: 0:42:01.616031\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6047/12662], [Dloss: 0.357584] [G loss: 0.130563] time: 0:42:02.910569\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6048/12662], [Dloss: 0.487379] [G loss: -0.079652] time: 0:42:04.228046\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6049/12662], [Dloss: -0.243031] [G loss: 0.290815] time: 0:42:05.486682\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6050/12662], [Dloss: 0.130671] [G loss: -0.333561] time: 0:42:06.709410\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6051/12662], [Dloss: 0.251858] [G loss: -0.219851] time: 0:42:07.982007\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6052/12662], [Dloss: 0.163109] [G loss: -0.034305] time: 0:42:09.269563\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6053/12662], [Dloss: 0.378991] [G loss: -0.134171] time: 0:42:10.543157\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6054/12662], [Dloss: 0.137340] [G loss: 0.016757] time: 0:42:11.765887\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6055/12662], [Dloss: 0.508631] [G loss: -0.105662] time: 0:42:13.008564\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6056/12662], [Dloss: 0.539621] [G loss: -0.051632] time: 0:42:14.281160\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6057/12662], [Dloss: 0.340692] [G loss: 0.127016] time: 0:42:15.567720\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6058/12662], [Dloss: -0.061644] [G loss: 0.498493] time: 0:42:16.893175\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6059/12662], [Dloss: 0.204058] [G loss: 0.475205] time: 0:42:18.051078\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6060/12662], [Dloss: 0.075426] [G loss: 0.242031] time: 0:42:19.345616\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6061/12662], [Dloss: 0.328062] [G loss: 0.395795] time: 0:42:20.667082\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6062/12662], [Dloss: 0.319727] [G loss: 0.639283] time: 0:42:21.941673\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6063/12662], [Dloss: 0.529790] [G loss: -0.347116] time: 0:42:23.283086\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6064/12662], [Dloss: 0.169778] [G loss: 0.253054] time: 0:42:24.544712\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6065/12662], [Dloss: 0.665436] [G loss: -0.168440] time: 0:42:25.793373\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6066/12662], [Dloss: 0.265111] [G loss: 0.096678] time: 0:42:27.038044\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6067/12662], [Dloss: 0.321230] [G loss: -0.739415] time: 0:42:28.298673\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6068/12662], [Dloss: 0.208222] [G loss: -1.181620] time: 0:42:29.559301\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6069/12662], [Dloss: 0.232193] [G loss: -0.839343] time: 0:42:30.817935\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6070/12662], [Dloss: 0.460735] [G loss: -0.398360] time: 0:42:32.022713\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6071/12662], [Dloss: 0.121609] [G loss: -0.410680] time: 0:42:33.269379\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6072/12662], [Dloss: 0.315532] [G loss: -0.061023] time: 0:42:34.545965\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6073/12662], [Dloss: 0.007490] [G loss: 0.287113] time: 0:42:35.828535\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6074/12662], [Dloss: 0.311112] [G loss: 0.698995] time: 0:42:37.067222\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6075/12662], [Dloss: 0.618538] [G loss: 0.198685] time: 0:42:38.440549\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6076/12662], [Dloss: 0.250409] [G loss: 0.487010] time: 0:42:39.664277\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6077/12662], [Dloss: 0.715271] [G loss: 0.219947] time: 0:42:40.904959\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6078/12662], [Dloss: 0.140142] [G loss: -0.243870] time: 0:42:42.199497\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6079/12662], [Dloss: 0.454652] [G loss: -0.350575] time: 0:42:43.475085\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6080/12662], [Dloss: 0.408012] [G loss: -0.206862] time: 0:42:44.738706\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6081/12662], [Dloss: 0.320693] [G loss: -0.144714] time: 0:42:45.890625\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6082/12662], [Dloss: 0.255229] [G loss: 0.035979] time: 0:42:47.129313\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6083/12662], [Dloss: 0.017353] [G loss: -0.068528] time: 0:42:48.402907\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6084/12662], [Dloss: 0.352612] [G loss: 0.026159] time: 0:42:49.647578\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6085/12662], [Dloss: 0.436306] [G loss: 0.256152] time: 0:42:50.925161\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6086/12662], [Dloss: 0.402791] [G loss: 0.018872] time: 0:42:52.222691\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6087/12662], [Dloss: 0.519404] [G loss: 0.056440] time: 0:42:53.489304\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6088/12662], [Dloss: 0.085294] [G loss: 0.179995] time: 0:42:54.715026\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6089/12662], [Dloss: 0.288235] [G loss: -0.295445] time: 0:42:56.010561\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6090/12662], [Dloss: 0.124762] [G loss: -0.277242] time: 0:42:57.346987\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6091/12662], [Dloss: 0.228344] [G loss: 0.193756] time: 0:42:58.797645\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6092/12662], [Dloss: 0.134803] [G loss: 0.159766] time: 0:43:00.181975\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6093/12662], [Dloss: 0.458561] [G loss: -0.038762] time: 0:43:01.499966\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6094/12662], [Dloss: 0.440007] [G loss: 0.266266] time: 0:43:02.757603\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6095/12662], [Dloss: 0.360263] [G loss: -0.186801] time: 0:43:04.056131\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6096/12662], [Dloss: 0.250516] [G loss: -0.160893] time: 0:43:05.277369\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6097/12662], [Dloss: 0.312542] [G loss: -0.514340] time: 0:43:06.552465\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6098/12662], [Dloss: 0.281916] [G loss: -0.280197] time: 0:43:07.850992\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6099/12662], [Dloss: 0.717722] [G loss: 0.024153] time: 0:43:09.077711\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6100/12662], [Dloss: 0.067411] [G loss: -0.210903] time: 0:43:10.346319\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6101/12662], [Dloss: 0.476574] [G loss: 0.279111] time: 0:43:11.667795\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6102/12662], [Dloss: 0.282999] [G loss: 0.504375] time: 0:43:12.919448\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6103/12662], [Dloss: 0.218884] [G loss: 0.533656] time: 0:43:14.212988\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6104/12662], [Dloss: 0.233900] [G loss: 0.595880] time: 0:43:15.555398\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6105/12662], [Dloss: 0.213956] [G loss: 0.745483] time: 0:43:16.771147\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6106/12662], [Dloss: 0.207917] [G loss: 0.316212] time: 0:43:18.156442\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6107/12662], [Dloss: 0.121572] [G loss: 0.289824] time: 0:43:19.435023\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6108/12662], [Dloss: 0.379879] [G loss: 0.248160] time: 0:43:20.740531\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6109/12662], [Dloss: 0.548728] [G loss: 0.178488] time: 0:43:22.071487\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6110/12662], [Dloss: 0.121585] [G loss: -0.146940] time: 0:43:23.425386\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6111/12662], [Dloss: 0.045294] [G loss: -0.173811] time: 0:43:24.802702\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6112/12662], [Dloss: 0.506269] [G loss: -0.060343] time: 0:43:26.032414\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6113/12662], [Dloss: 0.131086] [G loss: -0.178226] time: 0:43:27.310514\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6114/12662], [Dloss: 0.344812] [G loss: 0.054810] time: 0:43:28.620012\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6115/12662], [Dloss: 0.124716] [G loss: 0.001207] time: 0:43:29.915546\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6116/12662], [Dloss: 0.205764] [G loss: -0.040041] time: 0:43:31.314312\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6117/12662], [Dloss: 0.200518] [G loss: 0.153214] time: 0:43:32.537042\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6118/12662], [Dloss: 0.135722] [G loss: 0.637606] time: 0:43:33.917854\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6119/12662], [Dloss: 0.197964] [G loss: 0.501329] time: 0:43:35.235331\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6120/12662], [Dloss: 0.293828] [G loss: 0.505513] time: 0:43:36.481997\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6121/12662], [Dloss: 0.473698] [G loss: -0.097311] time: 0:43:37.640403\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6122/12662], [Dloss: 0.210108] [G loss: -0.066702] time: 0:43:38.985311\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6123/12662], [Dloss: 0.634433] [G loss: -0.257942] time: 0:43:40.305286\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6124/12662], [Dloss: 0.481279] [G loss: -0.339218] time: 0:43:41.612295\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6125/12662], [Dloss: -0.035832] [G loss: -0.086959] time: 0:43:42.866940\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6126/12662], [Dloss: 0.246797] [G loss: -0.012504] time: 0:43:44.112642\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6127/12662], [Dloss: 0.079543] [G loss: -0.053412] time: 0:43:45.392230\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6128/12662], [Dloss: 0.468957] [G loss: -0.383956] time: 0:43:46.699733\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6129/12662], [Dloss: 0.145271] [G loss: -0.401127] time: 0:43:47.875094\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6130/12662], [Dloss: 0.247297] [G loss: -0.090799] time: 0:43:49.122768\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6131/12662], [Dloss: 0.510978] [G loss: -0.004722] time: 0:43:50.379417\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6132/12662], [Dloss: 0.547075] [G loss: -0.002414] time: 0:43:51.678448\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6133/12662], [Dloss: 0.537265] [G loss: -0.194987] time: 0:43:52.971989\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6134/12662], [Dloss: 0.142877] [G loss: 0.032930] time: 0:43:54.276018\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6135/12662], [Dloss: 0.327250] [G loss: -0.201017] time: 0:43:55.559585\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6136/12662], [Dloss: 0.408611] [G loss: -0.327057] time: 0:43:56.809747\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6137/12662], [Dloss: 0.080349] [G loss: 0.010747] time: 0:43:58.026494\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6138/12662], [Dloss: 0.704968] [G loss: 0.050914] time: 0:43:59.254210\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6139/12662], [Dloss: 0.263631] [G loss: 0.152907] time: 0:44:00.561714\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6140/12662], [Dloss: 0.880817] [G loss: 0.421948] time: 0:44:01.815361\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6141/12662], [Dloss: 0.292005] [G loss: 0.474560] time: 0:44:03.005179\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6142/12662], [Dloss: 0.537473] [G loss: 0.367243] time: 0:44:04.314677\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6143/12662], [Dloss: 0.342443] [G loss: 0.277661] time: 0:44:05.709956\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6144/12662], [Dloss: 0.349423] [G loss: -0.040520] time: 0:44:07.024441\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6145/12662], [Dloss: 0.144850] [G loss: -0.092293] time: 0:44:08.325960\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6146/12662], [Dloss: 0.272074] [G loss: -0.166352] time: 0:44:09.638450\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6147/12662], [Dloss: 0.221151] [G loss: -0.184925] time: 0:44:10.947948\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6148/12662], [Dloss: 0.446233] [G loss: 0.182779] time: 0:44:12.343721\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6149/12662], [Dloss: 0.217799] [G loss: 0.305382] time: 0:44:13.679149\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6150/12662], [Dloss: 0.169774] [G loss: -0.116488] time: 0:44:14.954245\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6151/12662], [Dloss: 0.164391] [G loss: -0.105078] time: 0:44:16.319594\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6152/12662], [Dloss: 0.236389] [G loss: 0.457374] time: 0:44:17.632083\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6153/12662], [Dloss: 0.375915] [G loss: 0.204290] time: 0:44:18.864294\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6154/12662], [Dloss: 0.621358] [G loss: 0.156817] time: 0:44:20.140879\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6155/12662], [Dloss: 0.422366] [G loss: 0.287047] time: 0:44:21.316735\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6156/12662], [Dloss: 0.378077] [G loss: 0.485608] time: 0:44:22.620249\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6157/12662], [Dloss: 0.382970] [G loss: 0.214244] time: 0:44:23.957672\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6158/12662], [Dloss: 0.088897] [G loss: 0.652463] time: 0:44:25.335986\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6159/12662], [Dloss: 0.605966] [G loss: 0.510237] time: 0:44:26.625537\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6160/12662], [Dloss: 0.338228] [G loss: 0.419938] time: 0:44:27.873210\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6161/12662], [Dloss: 0.510113] [G loss: -0.254870] time: 0:44:29.127866\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6162/12662], [Dloss: -0.112628] [G loss: 0.162502] time: 0:44:30.424398\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6163/12662], [Dloss: 0.500203] [G loss: 0.499616] time: 0:44:31.667075\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6164/12662], [Dloss: 0.408382] [G loss: 0.171158] time: 0:44:32.887810\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6165/12662], [Dloss: 0.511838] [G loss: 0.199766] time: 0:44:34.183346\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6166/12662], [Dloss: 0.225401] [G loss: -0.015800] time: 0:44:35.472403\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6167/12662], [Dloss: 0.028468] [G loss: 0.418665] time: 0:44:36.716077\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6168/12662], [Dloss: 0.097577] [G loss: 0.476913] time: 0:44:37.944791\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6169/12662], [Dloss: 0.107830] [G loss: 0.304962] time: 0:44:39.292188\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6170/12662], [Dloss: -0.000066] [G loss: 0.231116] time: 0:44:40.491266\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6171/12662], [Dloss: 0.221348] [G loss: 0.171999] time: 0:44:41.759874\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6172/12662], [Dloss: 0.085166] [G loss: 0.347728] time: 0:44:42.970636\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6173/12662], [Dloss: 0.032150] [G loss: -0.288597] time: 0:44:44.283126\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6174/12662], [Dloss: 0.630912] [G loss: 0.076797] time: 0:44:45.660947\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6175/12662], [Dloss: 0.343168] [G loss: 0.192316] time: 0:44:46.888663\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6176/12662], [Dloss: 0.362511] [G loss: -0.529536] time: 0:44:48.110396\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6177/12662], [Dloss: 0.757950] [G loss: -0.156958] time: 0:44:49.402939\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6178/12662], [Dloss: 0.191211] [G loss: 0.066709] time: 0:44:50.736891\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6179/12662], [Dloss: 0.251150] [G loss: 0.465038] time: 0:44:52.116202\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6180/12662], [Dloss: 0.746940] [G loss: 0.030047] time: 0:44:53.511472\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6181/12662], [Dloss: -0.181526] [G loss: 0.533646] time: 0:44:54.854879\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6182/12662], [Dloss: 0.198627] [G loss: 0.047517] time: 0:44:56.198791\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6183/12662], [Dloss: 0.536019] [G loss: 0.582118] time: 0:44:57.619990\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6184/12662], [Dloss: 0.514989] [G loss: 0.586058] time: 0:44:58.956416\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6185/12662], [Dloss: 0.335862] [G loss: 0.240402] time: 0:45:00.317776\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6186/12662], [Dloss: 0.537938] [G loss: -0.138058] time: 0:45:01.645226\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6187/12662], [Dloss: 0.346593] [G loss: 0.111725] time: 0:45:03.052462\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6188/12662], [Dloss: 0.626634] [G loss: -0.137516] time: 0:45:04.334034\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6189/12662], [Dloss: 0.005311] [G loss: 0.151135] time: 0:45:05.672455\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6190/12662], [Dloss: 0.613434] [G loss: 0.163879] time: 0:45:07.047777\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6191/12662], [Dloss: 0.197911] [G loss: 0.217241] time: 0:45:08.359270\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6192/12662], [Dloss: 0.529351] [G loss: 0.216942] time: 0:45:09.622890\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6193/12662], [Dloss: 0.115333] [G loss: 0.374916] time: 0:45:10.902468\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6194/12662], [Dloss: 0.331754] [G loss: -0.015337] time: 0:45:12.317696\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6195/12662], [Dloss: 0.302151] [G loss: -0.114458] time: 0:45:13.668097\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6196/12662], [Dloss: 0.343204] [G loss: -0.178050] time: 0:45:14.955653\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6197/12662], [Dloss: 0.545982] [G loss: -0.101700] time: 0:45:16.273130\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6198/12662], [Dloss: 0.387966] [G loss: -0.342055] time: 0:45:17.678372\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6199/12662], [Dloss: 0.336525] [G loss: -0.153490] time: 0:45:18.942004\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6200/12662], [Dloss: 0.053902] [G loss: -0.107224] time: 0:45:20.321317\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6201/12662], [Dloss: 0.194323] [G loss: -0.215446] time: 0:45:21.620840\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6202/12662], [Dloss: 0.296703] [G loss: -0.372723] time: 0:45:22.988183\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6203/12662], [Dloss: 0.275727] [G loss: 0.136130] time: 0:45:24.233853\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6204/12662], [Dloss: 0.461606] [G loss: 0.083477] time: 0:45:25.575769\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6205/12662], [Dloss: 0.311390] [G loss: 0.357168] time: 0:45:26.871305\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6206/12662], [Dloss: 0.394107] [G loss: 0.601695] time: 0:45:28.094261\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6207/12662], [Dloss: 0.222814] [G loss: 0.624367] time: 0:45:29.411738\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6208/12662], [Dloss: 0.225857] [G loss: 0.457750] time: 0:45:30.692313\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6209/12662], [Dloss: 0.103857] [G loss: 0.631106] time: 0:45:31.860695\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6210/12662], [Dloss: 0.340959] [G loss: -0.006008] time: 0:45:33.144262\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6211/12662], [Dloss: 0.467580] [G loss: -0.014399] time: 0:45:34.566964\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6212/12662], [Dloss: 0.371572] [G loss: 0.095101] time: 0:45:35.910370\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6213/12662], [Dloss: 0.377332] [G loss: 0.400707] time: 0:45:37.242807\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6214/12662], [Dloss: 0.371090] [G loss: 0.034224] time: 0:45:38.562783\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6215/12662], [Dloss: 0.357560] [G loss: -0.113945] time: 0:45:39.832387\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6216/12662], [Dloss: 0.370548] [G loss: 0.249654] time: 0:45:41.206712\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6217/12662], [Dloss: 0.628893] [G loss: 0.375277] time: 0:45:42.579042\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6218/12662], [Dloss: 0.439703] [G loss: 0.072026] time: 0:45:43.919972\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6219/12662], [Dloss: 0.029924] [G loss: 0.010777] time: 0:45:45.213052\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6220/12662], [Dloss: 0.370779] [G loss: 0.050201] time: 0:45:46.461219\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6221/12662], [Dloss: 0.164633] [G loss: 0.254515] time: 0:45:47.808121\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6222/12662], [Dloss: 0.087852] [G loss: 0.522201] time: 0:45:49.104654\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6223/12662], [Dloss: -0.279875] [G loss: 0.065409] time: 0:45:50.429112\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6224/12662], [Dloss: 0.191450] [G loss: 0.491195] time: 0:45:51.802439\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6225/12662], [Dloss: 0.307240] [G loss: -0.239495] time: 0:45:53.108451\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6226/12662], [Dloss: 0.884834] [G loss: -0.139154] time: 0:45:54.410978\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6227/12662], [Dloss: 0.903358] [G loss: -0.353760] time: 0:45:55.716991\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6228/12662], [Dloss: 0.451214] [G loss: -0.096942] time: 0:45:56.990585\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6229/12662], [Dloss: 0.459047] [G loss: -0.355282] time: 0:45:58.233262\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6230/12662], [Dloss: 0.124911] [G loss: -0.063996] time: 0:45:59.582162\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6231/12662], [Dloss: 0.164536] [G loss: -0.017040] time: 0:46:00.854759\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6232/12662], [Dloss: 0.516437] [G loss: -0.065023] time: 0:46:02.056545\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6233/12662], [Dloss: 0.484250] [G loss: 0.281136] time: 0:46:03.279275\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6234/12662], [Dloss: 0.446356] [G loss: 0.226175] time: 0:46:04.499012\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6235/12662], [Dloss: 0.445876] [G loss: 0.488634] time: 0:46:05.813497\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6236/12662], [Dloss: 0.355066] [G loss: 0.621810] time: 0:46:07.155907\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6237/12662], [Dloss: 0.555259] [G loss: 0.516804] time: 0:46:08.450445\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6238/12662], [Dloss: 0.523179] [G loss: 0.214531] time: 0:46:09.689132\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6239/12662], [Dloss: 0.477361] [G loss: 0.542544] time: 0:46:10.948764\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6240/12662], [Dloss: 0.512287] [G loss: 0.309900] time: 0:46:12.221360\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6241/12662], [Dloss: -0.030234] [G loss: 0.197038] time: 0:46:13.406192\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6242/12662], [Dloss: 0.555173] [G loss: 0.101939] time: 0:46:14.788495\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6243/12662], [Dloss: 0.213136] [G loss: -0.099345] time: 0:46:16.003246\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6244/12662], [Dloss: 0.411529] [G loss: 0.050800] time: 0:46:17.225976\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6245/12662], [Dloss: 0.328068] [G loss: -0.423127] time: 0:46:18.456685\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6246/12662], [Dloss: 0.057318] [G loss: -0.011336] time: 0:46:19.671436\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6247/12662], [Dloss: 0.540911] [G loss: -0.161505] time: 0:46:20.945030\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6248/12662], [Dloss: 0.210495] [G loss: 0.037875] time: 0:46:22.267493\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6249/12662], [Dloss: 0.368533] [G loss: 0.247874] time: 0:46:23.574997\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6250/12662], [Dloss: 0.244651] [G loss: 0.376707] time: 0:46:24.889481\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6251/12662], [Dloss: 0.246261] [G loss: -0.189881] time: 0:46:26.162082\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6252/12662], [Dloss: 0.432959] [G loss: -0.282403] time: 0:46:27.556366\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6253/12662], [Dloss: 0.339396] [G loss: -0.256734] time: 0:46:29.040398\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6254/12662], [Dloss: 0.351612] [G loss: -0.153129] time: 0:46:30.433672\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6255/12662], [Dloss: 0.218107] [G loss: -0.263653] time: 0:46:31.747159\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6256/12662], [Dloss: 0.108982] [G loss: -0.090059] time: 0:46:33.114502\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6257/12662], [Dloss: 0.485474] [G loss: -0.446765] time: 0:46:34.394080\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6258/12662], [Dloss: 0.291461] [G loss: -0.051349] time: 0:46:35.704575\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6259/12662], [Dloss: 0.418214] [G loss: 0.017735] time: 0:46:37.049978\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6260/12662], [Dloss: 0.379495] [G loss: 0.201453] time: 0:46:38.317587\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6261/12662], [Dloss: -0.013809] [G loss: 0.379057] time: 0:46:39.728823\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6262/12662], [Dloss: 0.230012] [G loss: 0.243337] time: 0:46:41.085195\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6263/12662], [Dloss: 0.110820] [G loss: 0.268928] time: 0:46:42.341835\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6264/12662], [Dloss: -0.039704] [G loss: 0.488381] time: 0:46:43.630388\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6265/12662], [Dloss: 0.370921] [G loss: 0.052912] time: 0:46:44.886031\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6266/12662], [Dloss: 0.523432] [G loss: 0.354730] time: 0:46:46.135688\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6267/12662], [Dloss: 0.047868] [G loss: 0.642543] time: 0:46:47.363405\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6268/12662], [Dloss: -0.302383] [G loss: 0.385198] time: 0:46:48.679391\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6269/12662], [Dloss: 0.165197] [G loss: 0.233244] time: 0:46:49.926057\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6270/12662], [Dloss: 0.640169] [G loss: 0.348136] time: 0:46:51.236553\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6271/12662], [Dloss: 0.321996] [G loss: 0.513031] time: 0:46:52.494189\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6272/12662], [Dloss: 0.328236] [G loss: 0.211924] time: 0:46:53.723900\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6273/12662], [Dloss: -0.149049] [G loss: 0.238230] time: 0:46:55.035393\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6274/12662], [Dloss: 0.369334] [G loss: -0.149792] time: 0:46:56.377803\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6275/12662], [Dloss: 0.420021] [G loss: -0.190884] time: 0:46:57.678710\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6276/12662], [Dloss: 0.481145] [G loss: -0.312121] time: 0:46:58.959285\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6277/12662], [Dloss: 0.304037] [G loss: -0.639088] time: 0:47:00.163066\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6278/12662], [Dloss: 0.413515] [G loss: -0.067793] time: 0:47:01.445637\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6279/12662], [Dloss: 0.340199] [G loss: -0.020812] time: 0:47:02.657395\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6280/12662], [Dloss: 0.260582] [G loss: 0.312422] time: 0:47:03.918024\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6281/12662], [Dloss: 0.224434] [G loss: 0.477116] time: 0:47:05.213559\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6282/12662], [Dloss: 0.346095] [G loss: 0.294032] time: 0:47:06.494135\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6283/12662], [Dloss: 0.346986] [G loss: 0.503075] time: 0:47:07.803633\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6284/12662], [Dloss: -0.204942] [G loss: 0.602218] time: 0:47:09.192917\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6285/12662], [Dloss: -0.056749] [G loss: 0.391630] time: 0:47:10.376751\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6286/12662], [Dloss: 0.408500] [G loss: 0.396076] time: 0:47:11.726142\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6287/12662], [Dloss: 0.764772] [G loss: 0.022010] time: 0:47:13.077529\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6288/12662], [Dloss: 0.153874] [G loss: -0.031290] time: 0:47:14.359101\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6289/12662], [Dloss: 0.576732] [G loss: 0.192253] time: 0:47:15.593799\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6290/12662], [Dloss: 0.394229] [G loss: 0.267450] time: 0:47:16.835478\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6291/12662], [Dloss: 0.395042] [G loss: 0.119001] time: 0:47:18.086134\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6292/12662], [Dloss: 0.262006] [G loss: 0.214422] time: 0:47:19.388650\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6293/12662], [Dloss: 0.209887] [G loss: 0.391536] time: 0:47:20.705634\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6294/12662], [Dloss: 0.411508] [G loss: 0.217529] time: 0:47:21.985211\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6295/12662], [Dloss: 0.668178] [G loss: 0.131438] time: 0:47:23.303686\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6296/12662], [Dloss: 0.330661] [G loss: 0.066727] time: 0:47:24.598223\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6297/12662], [Dloss: 0.416884] [G loss: 0.300175] time: 0:47:25.874809\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6298/12662], [Dloss: 0.636431] [G loss: 0.419807] time: 0:47:27.110505\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6299/12662], [Dloss: 0.251454] [G loss: 0.523761] time: 0:47:28.504776\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6300/12662], [Dloss: 0.296750] [G loss: 0.396708] time: 0:47:29.787346\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6301/12662], [Dloss: 0.386370] [G loss: 0.595275] time: 0:47:31.011073\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6302/12662], [Dloss: 0.222408] [G loss: 0.393183] time: 0:47:32.304614\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6303/12662], [Dloss: 0.498809] [G loss: 0.291465] time: 0:47:33.646026\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6304/12662], [Dloss: 0.369377] [G loss: 0.460422] time: 0:47:34.883716\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6305/12662], [Dloss: 0.457558] [G loss: 0.324408] time: 0:47:36.155316\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6306/12662], [Dloss: 0.053344] [G loss: -0.068489] time: 0:47:37.388019\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6307/12662], [Dloss: 0.152952] [G loss: -0.213345] time: 0:47:38.599778\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6308/12662], [Dloss: 0.240603] [G loss: -0.188084] time: 0:47:39.864396\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6309/12662], [Dloss: 0.412175] [G loss: -0.376446] time: 0:47:41.135995\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6310/12662], [Dloss: 0.144655] [G loss: 0.017123] time: 0:47:42.369707\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6311/12662], [Dloss: 0.626900] [G loss: -0.155310] time: 0:47:43.595933\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6312/12662], [Dloss: 0.036465] [G loss: 0.443226] time: 0:47:44.915414\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6313/12662], [Dloss: 0.374154] [G loss: 0.631302] time: 0:47:46.098795\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6314/12662], [Dloss: 0.644690] [G loss: 0.518771] time: 0:47:47.393837\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6315/12662], [Dloss: 0.162818] [G loss: 0.692028] time: 0:47:48.618561\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6316/12662], [Dloss: 0.120982] [G loss: 0.677923] time: 0:47:49.890665\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6317/12662], [Dloss: 0.098716] [G loss: 0.623649] time: 0:47:51.075002\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6318/12662], [Dloss: 0.388936] [G loss: 0.274141] time: 0:47:52.397465\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6319/12662], [Dloss: 0.025204] [G loss: 0.398365] time: 0:47:53.650115\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6320/12662], [Dloss: 0.231778] [G loss: -0.085046] time: 0:47:54.885811\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6321/12662], [Dloss: 0.425934] [G loss: -0.230281] time: 0:47:56.169378\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6322/12662], [Dloss: 0.347117] [G loss: 0.011265] time: 0:47:57.394102\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6323/12662], [Dloss: 0.412048] [G loss: -0.224288] time: 0:47:58.705595\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6324/12662], [Dloss: 0.507141] [G loss: -0.120569] time: 0:47:59.966224\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6325/12662], [Dloss: 0.249564] [G loss: 0.131200] time: 0:48:01.208900\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6326/12662], [Dloss: 0.143817] [G loss: 0.170176] time: 0:48:02.431630\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6327/12662], [Dloss: 0.403634] [G loss: 0.164794] time: 0:48:03.642392\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6328/12662], [Dloss: 0.535714] [G loss: 0.274453] time: 0:48:04.899032\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6329/12662], [Dloss: 0.326366] [G loss: 0.284363] time: 0:48:06.085858\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6330/12662], [Dloss: 0.163580] [G loss: 0.087298] time: 0:48:07.326540\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6331/12662], [Dloss: 0.288118] [G loss: 0.267455] time: 0:48:08.545280\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6332/12662], [Dloss: 0.345839] [G loss: 0.645129] time: 0:48:09.741082\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6333/12662], [Dloss: 0.373929] [G loss: 0.538874] time: 0:48:11.041604\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6334/12662], [Dloss: 0.619762] [G loss: 0.408665] time: 0:48:12.410943\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6335/12662], [Dloss: 0.573020] [G loss: 0.507886] time: 0:48:13.685535\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6336/12662], [Dloss: 0.589400] [G loss: 0.735950] time: 0:48:14.947160\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6337/12662], [Dloss: 0.333923] [G loss: 0.605097] time: 0:48:16.225246\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6338/12662], [Dloss: 0.394293] [G loss: 0.766502] time: 0:48:17.476899\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6339/12662], [Dloss: 0.320473] [G loss: 0.236364] time: 0:48:18.753485\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6340/12662], [Dloss: 0.400926] [G loss: 0.530349] time: 0:48:19.960258\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6341/12662], [Dloss: 0.331167] [G loss: 0.302150] time: 0:48:21.232854\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6342/12662], [Dloss: 0.225187] [G loss: 0.511906] time: 0:48:22.554320\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6343/12662], [Dloss: 0.258094] [G loss: 0.043015] time: 0:48:23.834896\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6344/12662], [Dloss: 0.383778] [G loss: 0.255597] time: 0:48:25.128436\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6345/12662], [Dloss: 0.665698] [G loss: 0.335222] time: 0:48:26.439928\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6346/12662], [Dloss: 0.604086] [G loss: 0.074654] time: 0:48:27.768376\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6347/12662], [Dloss: 0.105142] [G loss: 0.419133] time: 0:48:29.052447\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6348/12662], [Dloss: 0.256300] [G loss: 0.203569] time: 0:48:30.285150\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6349/12662], [Dloss: 0.463106] [G loss: -0.277671] time: 0:48:31.603624\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6350/12662], [Dloss: 0.195217] [G loss: 0.246263] time: 0:48:32.858269\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6351/12662], [Dloss: 0.449585] [G loss: -0.079482] time: 0:48:34.281463\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6352/12662], [Dloss: 0.491140] [G loss: 0.140476] time: 0:48:35.637835\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6353/12662], [Dloss: 0.454680] [G loss: 0.272741] time: 0:48:36.943344\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6354/12662], [Dloss: 0.040613] [G loss: 0.253117] time: 0:48:38.205968\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6355/12662], [Dloss: 0.752600] [G loss: -0.188234] time: 0:48:39.561414\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6356/12662], [Dloss: 0.013249] [G loss: -0.038644] time: 0:48:40.855951\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6357/12662], [Dloss: 0.174237] [G loss: 0.083838] time: 0:48:42.181407\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6358/12662], [Dloss: 0.143851] [G loss: -0.357302] time: 0:48:43.430067\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6359/12662], [Dloss: 0.406780] [G loss: 0.003295] time: 0:48:44.696680\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6360/12662], [Dloss: 0.364153] [G loss: 0.263229] time: 0:48:46.057042\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6361/12662], [Dloss: 0.540444] [G loss: 0.261998] time: 0:48:47.326153\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6362/12662], [Dloss: 0.196955] [G loss: 0.169229] time: 0:48:48.554868\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6363/12662], [Dloss: 0.218908] [G loss: -0.117065] time: 0:48:49.823475\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6364/12662], [Dloss: 0.448931] [G loss: 0.251894] time: 0:48:51.028252\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6365/12662], [Dloss: 0.069230] [G loss: -0.239542] time: 0:48:52.297857\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6366/12662], [Dloss: 0.320574] [G loss: -0.083532] time: 0:48:53.576439\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6367/12662], [Dloss: 0.598151] [G loss: -0.177708] time: 0:48:54.882887\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6368/12662], [Dloss: 0.299158] [G loss: 0.046244] time: 0:48:56.125563\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6369/12662], [Dloss: 0.302061] [G loss: 0.031366] time: 0:48:57.378213\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6370/12662], [Dloss: 0.488515] [G loss: -0.019755] time: 0:48:58.639839\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6371/12662], [Dloss: 0.446001] [G loss: 0.036445] time: 0:48:59.924404\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6372/12662], [Dloss: 0.217667] [G loss: 0.335347] time: 0:49:01.136163\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6373/12662], [Dloss: 0.577011] [G loss: -0.035092] time: 0:49:02.413747\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6374/12662], [Dloss: 0.420066] [G loss: 0.602198] time: 0:49:03.713271\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6375/12662], [Dloss: 0.193709] [G loss: 0.482891] time: 0:49:05.066651\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6376/12662], [Dloss: 0.335563] [G loss: 0.707826] time: 0:49:06.345232\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6377/12662], [Dloss: 0.268721] [G loss: 0.436074] time: 0:49:07.594397\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6378/12662], [Dloss: 0.462720] [G loss: 0.529046] time: 0:49:08.884946\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6379/12662], [Dloss: 0.654648] [G loss: 0.414899] time: 0:49:10.088726\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6380/12662], [Dloss: 0.263615] [G loss: 0.278599] time: 0:49:11.290512\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6381/12662], [Dloss: 0.305950] [G loss: 0.124960] time: 0:49:12.672815\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6382/12662], [Dloss: -0.082569] [G loss: 0.219068] time: 0:49:14.000265\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6383/12662], [Dloss: 0.603384] [G loss: 0.262275] time: 0:49:15.296798\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6384/12662], [Dloss: 0.569621] [G loss: 0.153320] time: 0:49:16.526510\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6385/12662], [Dloss: 0.396248] [G loss: 0.465913] time: 0:49:17.821551\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6386/12662], [Dloss: 0.113722] [G loss: 0.125278] time: 0:49:19.123070\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6387/12662], [Dloss: 0.252736] [G loss: 0.215422] time: 0:49:20.402648\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6388/12662], [Dloss: 0.369105] [G loss: 0.131673] time: 0:49:21.683223\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6389/12662], [Dloss: 0.250330] [G loss: 0.334539] time: 0:49:22.942855\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6390/12662], [Dloss: 0.187126] [G loss: 0.521666] time: 0:49:24.229414\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6391/12662], [Dloss: 0.395569] [G loss: 0.262855] time: 0:49:25.567835\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6392/12662], [Dloss: 0.426339] [G loss: -0.134943] time: 0:49:26.844421\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6393/12662], [Dloss: 0.241832] [G loss: -0.364174] time: 0:49:28.100062\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6394/12662], [Dloss: 0.094419] [G loss: -0.161786] time: 0:49:29.373656\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6395/12662], [Dloss: 0.421743] [G loss: -0.151257] time: 0:49:30.620323\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6396/12662], [Dloss: 0.334137] [G loss: -0.300005] time: 0:49:31.919847\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6397/12662], [Dloss: -0.079659] [G loss: 0.004734] time: 0:49:33.107670\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6398/12662], [Dloss: 0.622473] [G loss: -0.165524] time: 0:49:34.355334\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6399/12662], [Dloss: 0.514492] [G loss: 0.037429] time: 0:49:35.627930\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6400/12662], [Dloss: 0.388004] [G loss: -0.057637] time: 0:49:36.898533\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6401/12662], [Dloss: -0.009371] [G loss: 0.491745] time: 0:49:38.146195\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6402/12662], [Dloss: 0.391483] [G loss: 0.311168] time: 0:49:39.482622\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6403/12662], [Dloss: 0.443930] [G loss: 0.394046] time: 0:49:40.746253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6404/12662], [Dloss: 0.379747] [G loss: 0.530171] time: 0:49:41.982946\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6405/12662], [Dloss: 0.072124] [G loss: 0.085926] time: 0:49:43.397701\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6406/12662], [Dloss: -0.264607] [G loss: 0.645149] time: 0:49:44.683298\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6407/12662], [Dloss: 0.259163] [G loss: -0.056329] time: 0:49:46.011764\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6408/12662], [Dloss: 0.467219] [G loss: -0.173295] time: 0:49:47.266914\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6409/12662], [Dloss: 0.195497] [G loss: 0.155636] time: 0:49:48.436292\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6410/12662], [Dloss: 0.480564] [G loss: 0.191294] time: 0:49:49.712385\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6411/12662], [Dloss: 0.399146] [G loss: 0.195040] time: 0:49:50.943093\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6412/12662], [Dloss: 0.402621] [G loss: 0.111319] time: 0:49:52.258575\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6413/12662], [Dloss: 0.348542] [G loss: -0.029755] time: 0:49:53.580041\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6414/12662], [Dloss: 0.102906] [G loss: -0.188414] time: 0:49:54.790815\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6415/12662], [Dloss: 0.484084] [G loss: -0.156949] time: 0:49:56.005566\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6416/12662], [Dloss: 0.424580] [G loss: 0.231335] time: 0:49:57.257219\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6417/12662], [Dloss: 0.200977] [G loss: 0.022055] time: 0:49:58.530813\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6418/12662], [Dloss: 0.506076] [G loss: -0.147518] time: 0:49:59.735591\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6419/12662], [Dloss: 0.429071] [G loss: 0.005801] time: 0:50:01.003201\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6420/12662], [Dloss: 0.037086] [G loss: 0.192936] time: 0:50:02.359586\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6421/12662], [Dloss: 0.380150] [G loss: 0.501486] time: 0:50:03.614231\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6422/12662], [Dloss: 0.785217] [G loss: -0.106106] time: 0:50:04.834966\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6423/12662], [Dloss: 0.238473] [G loss: 0.123241] time: 0:50:06.120528\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6424/12662], [Dloss: 0.100061] [G loss: 0.196890] time: 0:50:07.412074\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6425/12662], [Dloss: -0.238780] [G loss: 0.248049] time: 0:50:08.590922\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6426/12662], [Dloss: -0.001298] [G loss: 0.386253] time: 0:50:09.837588\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6427/12662], [Dloss: -0.126311] [G loss: 0.344836] time: 0:50:11.115171\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6428/12662], [Dloss: 0.274929] [G loss: 0.066110] time: 0:50:12.487501\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6429/12662], [Dloss: 0.189525] [G loss: -0.090026] time: 0:50:13.818940\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6430/12662], [Dloss: 0.513796] [G loss: -0.257068] time: 0:50:15.048651\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6431/12662], [Dloss: 0.394977] [G loss: -0.306192] time: 0:50:16.268389\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6432/12662], [Dloss: 0.131856] [G loss: -0.226344] time: 0:50:17.466702\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6433/12662], [Dloss: 0.369171] [G loss: 0.289643] time: 0:50:18.697411\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6434/12662], [Dloss: 0.533401] [G loss: 0.133843] time: 0:50:19.941084\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6435/12662], [Dloss: 0.352891] [G loss: -0.224399] time: 0:50:21.120929\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6436/12662], [Dloss: 0.178544] [G loss: -0.183542] time: 0:50:22.428432\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6437/12662], [Dloss: 0.277243] [G loss: 0.144652] time: 0:50:23.675098\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6438/12662], [Dloss: 0.106201] [G loss: 0.295917] time: 0:50:24.995075\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6439/12662], [Dloss: 0.333683] [G loss: 0.345472] time: 0:50:26.257698\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6440/12662], [Dloss: 0.292469] [G loss: 0.437853] time: 0:50:27.495389\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6441/12662], [Dloss: 0.007841] [G loss: 0.442709] time: 0:50:28.862744\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6442/12662], [Dloss: 0.424039] [G loss: 0.334948] time: 0:50:30.130354\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6443/12662], [Dloss: 0.041429] [G loss: 0.672204] time: 0:50:31.449825\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6444/12662], [Dloss: 0.652593] [G loss: 0.626329] time: 0:50:32.729403\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6445/12662], [Dloss: 0.280716] [G loss: 0.022461] time: 0:50:33.981056\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6446/12662], [Dloss: 0.576379] [G loss: 0.014806] time: 0:50:35.222735\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6447/12662], [Dloss: 0.423654] [G loss: 0.000710] time: 0:50:36.511289\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6448/12662], [Dloss: 0.216393] [G loss: 0.069006] time: 0:50:37.768925\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6449/12662], [Dloss: 0.199761] [G loss: 0.127636] time: 0:50:38.969714\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6450/12662], [Dloss: -0.442932] [G loss: 0.154719] time: 0:50:40.225860\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6451/12662], [Dloss: 0.645952] [G loss: 0.407494] time: 0:50:41.418670\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6452/12662], [Dloss: 0.443439] [G loss: 0.083444] time: 0:50:42.704232\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6453/12662], [Dloss: 0.410618] [G loss: 0.501650] time: 0:50:43.984807\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6454/12662], [Dloss: 0.246778] [G loss: 0.083189] time: 0:50:45.292311\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6455/12662], [Dloss: 0.207186] [G loss: 0.388758] time: 0:50:46.526011\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6456/12662], [Dloss: 0.190539] [G loss: 0.277387] time: 0:50:47.732784\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6457/12662], [Dloss: 0.257377] [G loss: 0.144696] time: 0:50:49.066724\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6458/12662], [Dloss: 0.661585] [G loss: 0.181722] time: 0:50:50.397165\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6459/12662], [Dloss: 0.178428] [G loss: 0.166709] time: 0:50:51.732101\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6460/12662], [Dloss: 0.667478] [G loss: 0.224588] time: 0:50:52.986253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6461/12662], [Dloss: 0.417234] [G loss: 0.613021] time: 0:50:54.243890\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6462/12662], [Dloss: 0.512807] [G loss: 0.966703] time: 0:50:55.463209\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6463/12662], [Dloss: -0.000108] [G loss: 0.850070] time: 0:50:56.759741\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6464/12662], [Dloss: 0.471500] [G loss: 0.445157] time: 0:50:57.947565\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6465/12662], [Dloss: 0.232153] [G loss: 0.389356] time: 0:50:59.194231\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6466/12662], [Dloss: 0.420413] [G loss: 0.561423] time: 0:51:00.430924\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6467/12662], [Dloss: 0.417869] [G loss: 0.344677] time: 0:51:01.593814\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6468/12662], [Dloss: 0.316443] [G loss: -0.082324] time: 0:51:02.818538\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6469/12662], [Dloss: 0.144157] [G loss: -0.020551] time: 0:51:04.062716\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6470/12662], [Dloss: 0.438407] [G loss: -0.179820] time: 0:51:05.341297\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6471/12662], [Dloss: 0.080878] [G loss: 0.106572] time: 0:51:06.519147\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6472/12662], [Dloss: 0.282196] [G loss: 0.185725] time: 0:51:07.745866\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6473/12662], [Dloss: -0.305222] [G loss: -0.046911] time: 0:51:08.995524\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6474/12662], [Dloss: 0.304713] [G loss: 0.250325] time: 0:51:10.329956\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6475/12662], [Dloss: 0.588748] [G loss: 0.240039] time: 0:51:11.649427\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6476/12662], [Dloss: 0.398856] [G loss: 0.008723] time: 0:51:12.811320\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6477/12662], [Dloss: 0.226888] [G loss: 0.193767] time: 0:51:13.995154\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6478/12662], [Dloss: 0.393287] [G loss: 0.301625] time: 0:51:15.205916\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6479/12662], [Dloss: 0.518094] [G loss: -0.056904] time: 0:51:16.552315\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6480/12662], [Dloss: 0.123950] [G loss: 0.178512] time: 0:51:17.826906\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6481/12662], [Dloss: 0.078423] [G loss: 0.206091] time: 0:51:19.045647\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6482/12662], [Dloss: 0.216318] [G loss: 0.026023] time: 0:51:20.321235\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6483/12662], [Dloss: 0.306609] [G loss: 0.161945] time: 0:51:21.609790\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6484/12662], [Dloss: 0.277940] [G loss: 0.622774] time: 0:51:22.886375\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6485/12662], [Dloss: -0.157689] [G loss: 0.712113] time: 0:51:24.163958\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6486/12662], [Dloss: 0.077132] [G loss: 0.553485] time: 0:51:25.467472\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6487/12662], [Dloss: 0.286091] [G loss: 0.630674] time: 0:51:26.638341\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6488/12662], [Dloss: 0.709117] [G loss: 0.680130] time: 0:51:27.863066\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6489/12662], [Dloss: 0.419856] [G loss: 0.206182] time: 0:51:29.096766\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6490/12662], [Dloss: 0.508011] [G loss: 0.077174] time: 0:51:30.366371\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6491/12662], [Dloss: 0.539457] [G loss: 0.031592] time: 0:51:31.624008\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6492/12662], [Dloss: 0.174836] [G loss: 0.227619] time: 0:51:32.877655\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6493/12662], [Dloss: 0.144618] [G loss: 0.059856] time: 0:51:34.082433\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6494/12662], [Dloss: 0.653695] [G loss: 0.062216] time: 0:51:35.323115\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6495/12662], [Dloss: 0.133928] [G loss: 0.175055] time: 0:51:36.518917\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6496/12662], [Dloss: 0.340965] [G loss: 0.002326] time: 0:51:37.683802\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6497/12662], [Dloss: 0.267765] [G loss: 0.265194] time: 0:51:38.927476\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6498/12662], [Dloss: 0.289110] [G loss: 0.478375] time: 0:51:40.252931\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6499/12662], [Dloss: 0.392254] [G loss: 0.240326] time: 0:51:41.534503\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6500/12662], [Dloss: 0.333075] [G loss: 0.038499] time: 0:51:42.840022\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6500/12662], [Dloss: 0.333075] [G loss: 0.038499] time: 0:51:42.840022\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6501/12662], [Dloss: 0.733700] [G loss: 0.524374] time: 0:51:45.114464\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6502/12662], [Dloss: 0.476546] [G loss: 0.285415] time: 0:51:46.333216\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6503/12662], [Dloss: 0.327017] [G loss: 0.309658] time: 0:51:47.569470\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6504/12662], [Dloss: -0.122168] [G loss: 0.370880] time: 0:51:48.836597\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6505/12662], [Dloss: 0.377955] [G loss: 0.267813] time: 0:51:50.134631\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6506/12662], [Dloss: 0.008827] [G loss: 0.368662] time: 0:51:51.364342\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6507/12662], [Dloss: 0.192621] [G loss: 0.025902] time: 0:51:52.546684\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6508/12662], [Dloss: 0.405134] [G loss: 0.398622] time: 0:51:53.805318\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6509/12662], [Dloss: 0.131188] [G loss: 0.626604] time: 0:51:55.103845\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6510/12662], [Dloss: 0.172985] [G loss: 0.419652] time: 0:51:56.372453\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6511/12662], [Dloss: 0.221174] [G loss: 0.281485] time: 0:51:57.618144\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6512/12662], [Dloss: 0.440615] [G loss: 0.344663] time: 0:51:58.844862\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6513/12662], [Dloss: 0.134467] [G loss: 0.280357] time: 0:52:00.157352\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6514/12662], [Dloss: 0.144797] [G loss: 0.223848] time: 0:52:01.404019\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6515/12662], [Dloss: 0.221936] [G loss: 0.199085] time: 0:52:02.653676\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6516/12662], [Dloss: 0.399092] [G loss: -0.073229] time: 0:52:03.902337\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6517/12662], [Dloss: 0.382170] [G loss: 0.176986] time: 0:52:05.154987\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6518/12662], [Dloss: 0.549026] [G loss: 0.472158] time: 0:52:06.375722\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6519/12662], [Dloss: 0.502908] [G loss: 0.474547] time: 0:52:07.612951\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6520/12662], [Dloss: 0.197806] [G loss: 0.704078] time: 0:52:09.023179\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6521/12662], [Dloss: 0.088113] [G loss: 0.070305] time: 0:52:10.315723\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6522/12662], [Dloss: 0.094827] [G loss: 0.809566] time: 0:52:11.511525\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6523/12662], [Dloss: 0.231920] [G loss: 0.568158] time: 0:52:12.708324\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6524/12662], [Dloss: -0.012718] [G loss: 0.271273] time: 0:52:13.975934\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6525/12662], [Dloss: 0.338641] [G loss: 0.470646] time: 0:52:15.241549\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6526/12662], [Dloss: 0.622123] [G loss: 0.054810] time: 0:52:16.454306\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6527/12662], [Dloss: 0.332183] [G loss: 0.588905] time: 0:52:17.963270\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6528/12662], [Dloss: 0.248973] [G loss: 0.277810] time: 0:52:19.270774\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6529/12662], [Dloss: 0.324352] [G loss: 0.637165] time: 0:52:20.502479\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6530/12662], [Dloss: 0.272766] [G loss: -0.038599] time: 0:52:21.748148\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6531/12662], [Dloss: -0.055165] [G loss: 0.013483] time: 0:52:22.946942\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6532/12662], [Dloss: 0.425277] [G loss: 0.435399] time: 0:52:24.214552\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6533/12662], [Dloss: 0.631714] [G loss: 0.200490] time: 0:52:25.477176\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6534/12662], [Dloss: 0.444988] [G loss: 0.434552] time: 0:52:26.737804\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6535/12662], [Dloss: 0.463833] [G loss: 0.177624] time: 0:52:28.003419\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6536/12662], [Dloss: 0.396682] [G loss: -0.047810] time: 0:52:29.123424\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6537/12662], [Dloss: 0.202089] [G loss: 0.386815] time: 0:52:30.399013\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6538/12662], [Dloss: 0.228493] [G loss: 0.033586] time: 0:52:31.707513\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6539/12662], [Dloss: 0.574562] [G loss: 0.069923] time: 0:52:32.984099\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6540/12662], [Dloss: 0.400267] [G loss: 0.296552] time: 0:52:34.214808\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6541/12662], [Dloss: 0.108137] [G loss: 0.122405] time: 0:52:35.456487\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6542/12662], [Dloss: -0.019008] [G loss: 0.346043] time: 0:52:36.638327\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6543/12662], [Dloss: 0.470530] [G loss: 0.252178] time: 0:52:37.897958\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6544/12662], [Dloss: 0.393611] [G loss: 0.065554] time: 0:52:39.122683\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6545/12662], [Dloss: 0.337619] [G loss: 0.001753] time: 0:52:40.295546\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6546/12662], [Dloss: 0.449576] [G loss: 0.252470] time: 0:52:41.556175\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6547/12662], [Dloss: 0.106146] [G loss: 0.483814] time: 0:52:42.765939\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6548/12662], [Dloss: 0.251199] [G loss: 0.329624] time: 0:52:43.975704\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6549/12662], [Dloss: 0.342724] [G loss: 0.179029] time: 0:52:45.189458\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6550/12662], [Dloss: 0.523570] [G loss: 0.332631] time: 0:52:46.476017\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6551/12662], [Dloss: 0.342165] [G loss: 0.093607] time: 0:52:47.702736\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6552/12662], [Dloss: -0.025802] [G loss: 0.363031] time: 0:52:48.984309\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6553/12662], [Dloss: 0.088404] [G loss: 0.085239] time: 0:52:50.243221\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6554/12662], [Dloss: 0.267249] [G loss: 0.271888] time: 0:52:51.493876\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6555/12662], [Dloss: 0.498632] [G loss: 0.151732] time: 0:52:52.700648\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6556/12662], [Dloss: 0.046919] [G loss: 0.182064] time: 0:52:53.896451\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6557/12662], [Dloss: 0.121096] [G loss: 0.267163] time: 0:52:55.152254\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6558/12662], [Dloss: 0.784531] [G loss: 0.201188] time: 0:52:56.367841\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6559/12662], [Dloss: 0.211160] [G loss: 0.345674] time: 0:52:57.659387\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6560/12662], [Dloss: 0.214231] [G loss: 0.446324] time: 0:52:58.929989\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6561/12662], [Dloss: 0.308313] [G loss: 0.634164] time: 0:53:00.122799\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6562/12662], [Dloss: 0.552590] [G loss: 0.456237] time: 0:53:01.415342\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6563/12662], [Dloss: 0.215667] [G loss: 0.482383] time: 0:53:02.582222\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6564/12662], [Dloss: 0.132894] [G loss: 0.204425] time: 0:53:03.854818\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6565/12662], [Dloss: 0.199086] [G loss: 0.017765] time: 0:53:05.089516\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6566/12662], [Dloss: 0.485877] [G loss: -0.067335] time: 0:53:06.253403\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6567/12662], [Dloss: 0.205496] [G loss: 0.113930] time: 0:53:07.504059\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6568/12662], [Dloss: 0.184996] [G loss: 0.199884] time: 0:53:08.714821\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6569/12662], [Dloss: 0.265471] [G loss: 0.254154] time: 0:53:09.921594\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6570/12662], [Dloss: 0.046766] [G loss: 0.089284] time: 0:53:11.203166\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6571/12662], [Dloss: 0.183095] [G loss: -0.009638] time: 0:53:12.404470\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6572/12662], [Dloss: 0.370221] [G loss: 0.256671] time: 0:53:13.596282\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6573/12662], [Dloss: 0.426183] [G loss: 0.208334] time: 0:53:14.850927\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6574/12662], [Dloss: 0.192460] [G loss: 0.471650] time: 0:53:16.055705\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6575/12662], [Dloss: 0.211765] [G loss: 0.584085] time: 0:53:17.319326\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6576/12662], [Dloss: 0.640502] [G loss: 0.303580] time: 0:53:18.540061\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6577/12662], [Dloss: 0.057531] [G loss: 0.309475] time: 0:53:19.803803\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6578/12662], [Dloss: 0.439480] [G loss: 0.539229] time: 0:53:21.126266\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6579/12662], [Dloss: 0.259930] [G loss: 0.390357] time: 0:53:22.424301\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6580/12662], [Dloss: 0.152057] [G loss: 0.469190] time: 0:53:23.669969\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6581/12662], [Dloss: 0.482744] [G loss: 0.280822] time: 0:53:24.910651\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6582/12662], [Dloss: 0.154577] [G loss: 0.430406] time: 0:53:26.180256\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6583/12662], [Dloss: 0.405006] [G loss: 0.384679] time: 0:53:27.445871\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6584/12662], [Dloss: 0.309915] [G loss: 0.715583] time: 0:53:28.737417\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6585/12662], [Dloss: 0.408527] [G loss: 0.277296] time: 0:53:29.959150\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6586/12662], [Dloss: 0.166123] [G loss: 0.325595] time: 0:53:31.121546\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6587/12662], [Dloss: 0.388457] [G loss: 0.044193] time: 0:53:32.389156\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6588/12662], [Dloss: 0.390453] [G loss: 0.068890] time: 0:53:33.765475\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6589/12662], [Dloss: 0.034414] [G loss: 0.413569] time: 0:53:35.071981\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6590/12662], [Dloss: 0.502152] [G loss: 0.406751] time: 0:53:36.277756\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6591/12662], [Dloss: 0.287283] [G loss: 0.559847] time: 0:53:37.585260\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6592/12662], [Dloss: 0.279256] [G loss: 0.807486] time: 0:53:38.813974\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6593/12662], [Dloss: -0.065766] [G loss: 0.929738] time: 0:53:40.096544\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6594/12662], [Dloss: 0.764039] [G loss: 0.949447] time: 0:53:41.261429\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6595/12662], [Dloss: 0.536400] [G loss: 0.697121] time: 0:53:42.543001\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6596/12662], [Dloss: 0.325598] [G loss: 0.263545] time: 0:53:43.803640\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6597/12662], [Dloss: 0.197828] [G loss: 0.459391] time: 0:53:45.051817\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6598/12662], [Dloss: 0.636286] [G loss: 0.111843] time: 0:53:46.310964\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6599/12662], [Dloss: 0.099307] [G loss: 0.466913] time: 0:53:47.603015\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6600/12662], [Dloss: 0.244542] [G loss: -0.124619] time: 0:53:48.878614\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6601/12662], [Dloss: -0.055440] [G loss: 0.437343] time: 0:53:50.092380\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6602/12662], [Dloss: 0.151484] [G loss: -0.188062] time: 0:53:51.321598\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6603/12662], [Dloss: 0.042375] [G loss: 0.020161] time: 0:53:52.565777\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6604/12662], [Dloss: 0.157168] [G loss: -0.118126] time: 0:53:53.786512\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6605/12662], [Dloss: 0.151336] [G loss: 0.180069] time: 0:53:55.034176\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6606/12662], [Dloss: 0.444696] [G loss: -0.249557] time: 0:53:56.277849\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6607/12662], [Dloss: 0.105989] [G loss: 0.028005] time: 0:53:57.452708\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6608/12662], [Dloss: 0.418663] [G loss: 0.470579] time: 0:53:58.718323\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6609/12662], [Dloss: 0.221767] [G loss: 0.433986] time: 0:53:59.934071\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6610/12662], [Dloss: -0.011060] [G loss: 1.138511] time: 0:54:01.174754\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6611/12662], [Dloss: 0.560011] [G loss: 0.449454] time: 0:54:02.353600\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6612/12662], [Dloss: 0.428874] [G loss: 0.553825] time: 0:54:03.597275\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6613/12662], [Dloss: 0.381054] [G loss: 0.344199] time: 0:54:04.934698\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6614/12662], [Dloss: 0.149612] [G loss: 0.685720] time: 0:54:06.167401\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6615/12662], [Dloss: 0.621696] [G loss: 0.309368] time: 0:54:07.439001\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6616/12662], [Dloss: 0.348239] [G loss: -0.031533] time: 0:54:08.689162\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6617/12662], [Dloss: 0.071189] [G loss: 0.319886] time: 0:54:09.947796\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6618/12662], [Dloss: 0.300146] [G loss: 0.560721] time: 0:54:11.141603\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6619/12662], [Dloss: 0.388167] [G loss: 0.221889] time: 0:54:12.367326\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6620/12662], [Dloss: 0.597191] [G loss: -0.228217] time: 0:54:13.617981\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6621/12662], [Dloss: 0.280213] [G loss: 0.024188] time: 0:54:14.861655\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6622/12662], [Dloss: 0.452351] [G loss: 0.026673] time: 0:54:15.999611\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6623/12662], [Dloss: 0.318052] [G loss: -0.113925] time: 0:54:17.224336\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6624/12662], [Dloss: 0.565229] [G loss: 0.401222] time: 0:54:18.456042\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6625/12662], [Dloss: 0.179836] [G loss: 0.415400] time: 0:54:19.634890\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6626/12662], [Dloss: 0.470415] [G loss: 0.377104] time: 0:54:20.888537\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6627/12662], [Dloss: 0.232399] [G loss: 0.416019] time: 0:54:22.176093\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6628/12662], [Dloss: 0.395114] [G loss: 0.383226] time: 0:54:23.344968\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6629/12662], [Dloss: 0.391807] [G loss: 0.782291] time: 0:54:24.559719\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6630/12662], [Dloss: 0.343307] [G loss: 0.648820] time: 0:54:25.805387\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6631/12662], [Dloss: 0.117779] [G loss: 0.350930] time: 0:54:26.985232\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6632/12662], [Dloss: 0.423233] [G loss: 0.494066] time: 0:54:28.213946\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6633/12662], [Dloss: 0.119947] [G loss: 0.123428] time: 0:54:29.405759\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6634/12662], [Dloss: 0.215863] [G loss: 0.452252] time: 0:54:30.722238\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6635/12662], [Dloss: 0.616296] [G loss: 0.257616] time: 0:54:31.883133\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6636/12662], [Dloss: 0.412842] [G loss: 0.144447] time: 0:54:33.096887\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6637/12662], [Dloss: 0.311726] [G loss: 0.231034] time: 0:54:34.345548\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6638/12662], [Dloss: 0.384785] [G loss: -0.190140] time: 0:54:35.603185\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6639/12662], [Dloss: 0.176621] [G loss: -0.532942] time: 0:54:36.882762\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6640/12662], [Dloss: -0.067358] [G loss: -0.350427] time: 0:54:38.123444\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6641/12662], [Dloss: 0.244926] [G loss: -0.608735] time: 0:54:39.293316\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6642/12662], [Dloss: 0.372640] [G loss: -0.285540] time: 0:54:40.567907\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6643/12662], [Dloss: 0.337197] [G loss: -0.115094] time: 0:54:41.830530\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6644/12662], [Dloss: 0.363219] [G loss: 0.018616] time: 0:54:43.175932\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6645/12662], [Dloss: 0.242819] [G loss: 0.236411] time: 0:54:44.425590\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6646/12662], [Dloss: 0.131254] [G loss: 0.249376] time: 0:54:45.692204\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6647/12662], [Dloss: 0.443541] [G loss: 0.030342] time: 0:54:46.891994\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6648/12662], [Dloss: 0.230005] [G loss: 0.455953] time: 0:54:48.099764\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6649/12662], [Dloss: 0.278187] [G loss: 0.336272] time: 0:54:49.311523\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6650/12662], [Dloss: 0.524602] [G loss: 0.566179] time: 0:54:50.518296\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6651/12662], [Dloss: 0.215312] [G loss: 0.433687] time: 0:54:51.753992\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6652/12662], [Dloss: 0.743651] [G loss: 0.437584] time: 0:54:52.919873\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6653/12662], [Dloss: 0.546482] [G loss: 0.564427] time: 0:54:54.141606\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6654/12662], [Dloss: 0.539878] [G loss: 0.240188] time: 0:54:55.480027\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6655/12662], [Dloss: 0.307229] [G loss: 0.364049] time: 0:54:56.763594\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6656/12662], [Dloss: 0.240616] [G loss: 0.471204] time: 0:54:58.012255\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6657/12662], [Dloss: 0.383040] [G loss: 0.080282] time: 0:54:59.247950\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6658/12662], [Dloss: 0.373889] [G loss: -0.215313] time: 0:55:00.513565\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6659/12662], [Dloss: 0.150881] [G loss: 0.035116] time: 0:55:01.757239\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6660/12662], [Dloss: 0.340409] [G loss: -0.039453] time: 0:55:02.923121\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6661/12662], [Dloss: 0.042712] [G loss: 0.071677] time: 0:55:04.111942\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6662/12662], [Dloss: 0.348219] [G loss: -0.242358] time: 0:55:05.332677\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6663/12662], [Dloss: -0.243580] [G loss: 0.078844] time: 0:55:06.589317\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6664/12662], [Dloss: 0.447787] [G loss: -0.024802] time: 0:55:07.782126\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6665/12662], [Dloss: 0.020439] [G loss: 0.609524] time: 0:55:09.014830\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6666/12662], [Dloss: 0.194223] [G loss: 0.500538] time: 0:55:10.259501\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6667/12662], [Dloss: 0.278269] [G loss: 0.402969] time: 0:55:11.538082\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6668/12662], [Dloss: 0.519924] [G loss: 0.202733] time: 0:55:12.717926\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6669/12662], [Dloss: 0.597694] [G loss: -0.027102] time: 0:55:14.011467\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6670/12662], [Dloss: 0.048040] [G loss: 0.268117] time: 0:55:15.230208\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6671/12662], [Dloss: 0.564647] [G loss: 0.166722] time: 0:55:16.454932\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6672/12662], [Dloss: 0.412356] [G loss: 0.082047] time: 0:55:17.668193\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6673/12662], [Dloss: 0.104639] [G loss: -0.025577] time: 0:55:18.887931\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6674/12662], [Dloss: 0.409995] [G loss: 0.038768] time: 0:55:20.123627\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6675/12662], [Dloss: 0.341564] [G loss: 0.001239] time: 0:55:21.341370\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6676/12662], [Dloss: 0.230641] [G loss: 0.294032] time: 0:55:22.513236\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6677/12662], [Dloss: 0.233213] [G loss: 0.031183] time: 0:55:23.741950\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6678/12662], [Dloss: 0.261346] [G loss: -0.044225] time: 0:55:25.026515\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6679/12662], [Dloss: 0.320690] [G loss: -0.091671] time: 0:55:26.263207\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6680/12662], [Dloss: 0.244547] [G loss: -0.121036] time: 0:55:27.485938\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6681/12662], [Dloss: 0.335768] [G loss: -0.091134] time: 0:55:28.784464\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6682/12662], [Dloss: 0.440220] [G loss: 0.179414] time: 0:55:30.040106\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6683/12662], [Dloss: 0.234068] [G loss: 0.321097] time: 0:55:31.289765\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6684/12662], [Dloss: 0.214253] [G loss: 0.217216] time: 0:55:32.498532\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6685/12662], [Dloss: 0.127426] [G loss: 0.080135] time: 0:55:33.708297\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6686/12662], [Dloss: 0.448304] [G loss: 0.361315] time: 0:55:34.937011\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6687/12662], [Dloss: 0.308465] [G loss: 0.038092] time: 0:55:36.171708\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6688/12662], [Dloss: -0.046827] [G loss: 0.289706] time: 0:55:37.418374\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6689/12662], [Dloss: 0.515550] [G loss: 0.002895] time: 0:55:38.696955\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6690/12662], [Dloss: 0.483635] [G loss: 0.415944] time: 0:55:39.984511\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6691/12662], [Dloss: 0.478748] [G loss: 0.423596] time: 0:55:41.281044\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6692/12662], [Dloss: 0.473216] [G loss: 0.307054] time: 0:55:42.533694\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6693/12662], [Dloss: 0.435754] [G loss: 0.806159] time: 0:55:43.754481\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6694/12662], [Dloss: 0.459163] [G loss: 0.627216] time: 0:55:45.013085\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6695/12662], [Dloss: 0.141246] [G loss: 0.706060] time: 0:55:46.311633\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6696/12662], [Dloss: 0.512379] [G loss: 0.395607] time: 0:55:47.592221\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6697/12662], [Dloss: -0.049355] [G loss: 0.320255] time: 0:55:48.836892\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6698/12662], [Dloss: 0.175797] [G loss: 0.435249] time: 0:55:50.019729\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6699/12662], [Dloss: 0.233727] [G loss: 0.497807] time: 0:55:51.257418\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6700/12662], [Dloss: 0.779810] [G loss: 0.000154] time: 0:55:52.370442\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6701/12662], [Dloss: 0.434364] [G loss: 0.542202] time: 0:55:53.623596\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6702/12662], [Dloss: 0.371984] [G loss: 0.440882] time: 0:55:54.834861\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6703/12662], [Dloss: 0.035817] [G loss: 0.328690] time: 0:55:56.092004\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6704/12662], [Dloss: 0.290670] [G loss: 0.048141] time: 0:55:57.321716\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6705/12662], [Dloss: 0.048377] [G loss: 0.250139] time: 0:55:58.534472\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6706/12662], [Dloss: 0.113785] [G loss: 0.542182] time: 0:55:59.688386\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6707/12662], [Dloss: 0.169795] [G loss: 0.162937] time: 0:56:00.925080\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6708/12662], [Dloss: 0.383173] [G loss: 0.381156] time: 0:56:02.152796\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6709/12662], [Dloss: 0.268549] [G loss: 0.384105] time: 0:56:03.435366\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6710/12662], [Dloss: 0.249853] [G loss: 0.133624] time: 0:56:04.692005\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6711/12662], [Dloss: 0.618656] [G loss: 0.464343] time: 0:56:05.983551\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6712/12662], [Dloss: 0.090949] [G loss: 0.098662] time: 0:56:07.230217\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6713/12662], [Dloss: 0.293967] [G loss: -0.012922] time: 0:56:08.385128\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6714/12662], [Dloss: 0.638826] [G loss: 0.032764] time: 0:56:09.523085\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6715/12662], [Dloss: 0.456350] [G loss: -0.122516] time: 0:56:10.803660\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6716/12662], [Dloss: 0.232470] [G loss: 0.353561] time: 0:56:12.067281\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6717/12662], [Dloss: 0.379275] [G loss: 0.637232] time: 0:56:13.221195\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6718/12662], [Dloss: 0.083079] [G loss: 0.284156] time: 0:56:14.496784\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6719/12662], [Dloss: 0.470108] [G loss: 0.482871] time: 0:56:15.723503\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6720/12662], [Dloss: 0.205796] [G loss: 0.336275] time: 0:56:16.998094\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6721/12662], [Dloss: 0.568642] [G loss: 0.486070] time: 0:56:18.261715\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6722/12662], [Dloss: 0.410411] [G loss: 0.078780] time: 0:56:19.460509\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6723/12662], [Dloss: 0.507222] [G loss: 0.604962] time: 0:56:20.762532\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6724/12662], [Dloss: 0.560197] [G loss: 0.371758] time: 0:56:21.978280\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6725/12662], [Dloss: 0.316459] [G loss: 0.396797] time: 0:56:23.251874\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6726/12662], [Dloss: 0.282330] [G loss: 0.346445] time: 0:56:24.481585\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6727/12662], [Dloss: 0.622656] [G loss: 0.142005] time: 0:56:25.734235\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6728/12662], [Dloss: 0.578546] [G loss: -0.020010] time: 0:56:26.886155\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6729/12662], [Dloss: -0.210157] [G loss: 0.256962] time: 0:56:28.095920\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6730/12662], [Dloss: 0.512054] [G loss: 0.145338] time: 0:56:29.287732\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6731/12662], [Dloss: 0.065599] [G loss: 0.288275] time: 0:56:30.534398\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6732/12662], [Dloss: 0.374423] [G loss: 0.164311] time: 0:56:31.880798\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6733/12662], [Dloss: 0.083134] [G loss: 0.175430] time: 0:56:33.065629\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6734/12662], [Dloss: 0.319005] [G loss: 0.366595] time: 0:56:34.285367\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6735/12662], [Dloss: 0.298813] [G loss: 0.764162] time: 0:56:35.540012\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6736/12662], [Dloss: 0.084884] [G loss: 0.286567] time: 0:56:36.788672\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6737/12662], [Dloss: 0.219255] [G loss: 0.015551] time: 0:56:38.006416\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6738/12662], [Dloss: 0.721024] [G loss: 0.028511] time: 0:56:39.227151\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6739/12662], [Dloss: 0.726326] [G loss: 0.146876] time: 0:56:40.458857\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6740/12662], [Dloss: 0.291144] [G loss: -0.078471] time: 0:56:41.694552\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6741/12662], [Dloss: 0.348845] [G loss: -0.070304] time: 0:56:42.946204\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6742/12662], [Dloss: 0.037671] [G loss: 0.094628] time: 0:56:44.112087\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6743/12662], [Dloss: 0.414262] [G loss: 0.098165] time: 0:56:45.314870\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6744/12662], [Dloss: 0.266089] [G loss: 0.382556] time: 0:56:46.558544\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6745/12662], [Dloss: 0.297496] [G loss: 0.622142] time: 0:56:47.756340\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6746/12662], [Dloss: 0.477391] [G loss: 0.273256] time: 0:56:49.055018\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6747/12662], [Dloss: 0.446247] [G loss: 0.301157] time: 0:56:50.218906\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6748/12662], [Dloss: 0.326460] [G loss: 0.904867] time: 0:56:51.463577\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6749/12662], [Dloss: 0.274579] [G loss: 0.243478] time: 0:56:52.724205\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6750/12662], [Dloss: 0.228656] [G loss: 0.519937] time: 0:56:53.980845\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6751/12662], [Dloss: -0.042293] [G loss: 0.330751] time: 0:56:55.190609\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6752/12662], [Dloss: 0.506464] [G loss: 0.375361] time: 0:56:56.414337\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6753/12662], [Dloss: 0.220968] [G loss: 0.303445] time: 0:56:57.724832\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6754/12662], [Dloss: 0.353718] [G loss: 0.184991] time: 0:56:58.893706\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6755/12662], [Dloss: 0.042499] [G loss: 0.181953] time: 0:57:00.139374\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6756/12662], [Dloss: 0.484282] [G loss: -0.041300] time: 0:57:01.309246\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6757/12662], [Dloss: 0.229512] [G loss: 0.364498] time: 0:57:02.492083\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6758/12662], [Dloss: 0.128925] [G loss: 0.062768] time: 0:57:03.818535\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6759/12662], [Dloss: 0.139665] [G loss: -0.214419] time: 0:57:05.001372\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6760/12662], [Dloss: 0.393265] [G loss: 0.278898] time: 0:57:06.225099\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6761/12662], [Dloss: 0.227233] [G loss: 0.278730] time: 0:57:07.488720\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6762/12662], [Dloss: -0.096666] [G loss: 0.425633] time: 0:57:08.749348\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6763/12662], [Dloss: 0.431515] [G loss: 0.445964] time: 0:57:09.969086\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6764/12662], [Dloss: -0.086372] [G loss: 0.230430] time: 0:57:11.170872\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6765/12662], [Dloss: -0.064826] [G loss: 0.776203] time: 0:57:12.345730\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6766/12662], [Dloss: 0.149751] [G loss: 0.287317] time: 0:57:13.570455\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6767/12662], [Dloss: 0.382298] [G loss: 0.031084] time: 0:57:14.823019\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6768/12662], [Dloss: 0.161245] [G loss: 0.199012] time: 0:57:16.050736\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6769/12662], [Dloss: -0.082983] [G loss: 0.564092] time: 0:57:17.296405\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6770/12662], [Dloss: 0.411789] [G loss: 0.779689] time: 0:57:18.484228\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6771/12662], [Dloss: 0.133193] [G loss: 0.366260] time: 0:57:19.739858\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6772/12662], [Dloss: 0.569550] [G loss: 0.863059] time: 0:57:21.027415\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6773/12662], [Dloss: 0.218644] [G loss: 0.672415] time: 0:57:22.288043\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6774/12662], [Dloss: 0.367395] [G loss: 0.408319] time: 0:57:23.537702\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6775/12662], [Dloss: 0.381372] [G loss: 0.183873] time: 0:57:24.705578\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6776/12662], [Dloss: 0.513301] [G loss: 0.092320] time: 0:57:26.015076\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6777/12662], [Dloss: 0.496747] [G loss: -0.255468] time: 0:57:27.267726\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6778/12662], [Dloss: 0.077599] [G loss: 0.452242] time: 0:57:28.534339\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6779/12662], [Dloss: 0.356909] [G loss: 0.013639] time: 0:57:29.786989\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6780/12662], [Dloss: 0.164754] [G loss: 0.340419] time: 0:57:30.966833\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6781/12662], [Dloss: 0.215628] [G loss: 0.720603] time: 0:57:32.110774\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6782/12662], [Dloss: 0.158532] [G loss: 0.506900] time: 0:57:33.353451\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6783/12662], [Dloss: 0.476208] [G loss: 0.636562] time: 0:57:34.631034\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6784/12662], [Dloss: 0.550277] [G loss: 0.452196] time: 0:57:35.899641\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6785/12662], [Dloss: -0.008493] [G loss: 0.732680] time: 0:57:37.070510\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6786/12662], [Dloss: 0.157606] [G loss: 0.719353] time: 0:57:38.298227\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6787/12662], [Dloss: 0.129848] [G loss: 0.486506] time: 0:57:39.526941\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6788/12662], [Dloss: 0.094232] [G loss: 0.107887] time: 0:57:40.833446\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6789/12662], [Dloss: -0.647309] [G loss: 0.999312] time: 0:57:42.014288\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6790/12662], [Dloss: 0.606018] [G loss: 0.181201] time: 0:57:43.292869\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6791/12662], [Dloss: 0.592688] [G loss: 0.035968] time: 0:57:44.521109\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6792/12662], [Dloss: 0.606387] [G loss: -0.156919] time: 0:57:45.775773\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6793/12662], [Dloss: 0.131830] [G loss: -0.031542] time: 0:57:47.075297\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6794/12662], [Dloss: 0.188175] [G loss: 0.153385] time: 0:57:48.265115\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6795/12662], [Dloss: 0.228181] [G loss: 0.079343] time: 0:57:49.545793\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6796/12662], [Dloss: 0.113734] [G loss: 0.093655] time: 0:57:50.821382\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6797/12662], [Dloss: 0.095541] [G loss: 0.295527] time: 0:57:52.084005\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6798/12662], [Dloss: 0.260770] [G loss: 0.321838] time: 0:57:53.282799\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6799/12662], [Dloss: 0.209991] [G loss: -0.075545] time: 0:57:54.538454\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6800/12662], [Dloss: 0.458832] [G loss: 0.179153] time: 0:57:55.826515\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6801/12662], [Dloss: 0.303434] [G loss: 0.816076] time: 0:57:56.990907\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6802/12662], [Dloss: 0.188636] [G loss: 0.457301] time: 0:57:58.219582\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6803/12662], [Dloss: 0.213568] [G loss: 0.741135] time: 0:57:59.469240\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6804/12662], [Dloss: 0.163310] [G loss: 0.640085] time: 0:58:00.727873\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6805/12662], [Dloss: 0.159507] [G loss: 0.270266] time: 0:58:01.944619\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6806/12662], [Dloss: 0.141144] [G loss: 0.234667] time: 0:58:03.216219\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6807/12662], [Dloss: -0.120897] [G loss: 0.173036] time: 0:58:04.415013\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6808/12662], [Dloss: 0.301529] [G loss: -0.162106] time: 0:58:05.614804\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6809/12662], [Dloss: 0.184059] [G loss: 0.120180] time: 0:58:06.785673\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6810/12662], [Dloss: 0.559320] [G loss: -0.186607] time: 0:58:08.072232\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6811/12662], [Dloss: 0.550990] [G loss: 0.046809] time: 0:58:09.263552\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6812/12662], [Dloss: 0.567682] [G loss: -0.512254] time: 0:58:10.465338\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6813/12662], [Dloss: 0.528955] [G loss: -0.010312] time: 0:58:11.645182\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6814/12662], [Dloss: -0.032700] [G loss: 0.247970] time: 0:58:12.921768\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6815/12662], [Dloss: 0.336627] [G loss: 0.183907] time: 0:58:14.165442\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6816/12662], [Dloss: 0.249746] [G loss: 0.239372] time: 0:58:15.366231\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6817/12662], [Dloss: 0.209003] [G loss: 0.140300] time: 0:58:16.562033\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6818/12662], [Dloss: 0.269113] [G loss: -0.068243] time: 0:58:17.721931\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6819/12662], [Dloss: 0.302151] [G loss: -0.118545] time: 0:58:19.009488\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6820/12662], [Dloss: -0.038539] [G loss: 0.125554] time: 0:58:20.243189\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6821/12662], [Dloss: 0.452289] [G loss: -0.194177] time: 0:58:21.456942\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6822/12662], [Dloss: 0.229271] [G loss: -0.101861] time: 0:58:22.683662\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6823/12662], [Dloss: 0.435142] [G loss: -0.222189] time: 0:58:23.935314\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6824/12662], [Dloss: 0.282211] [G loss: -0.262708] time: 0:58:25.122140\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6825/12662], [Dloss: 0.711049] [G loss: -0.358365] time: 0:58:26.426652\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6826/12662], [Dloss: -0.045675] [G loss: -0.050591] time: 0:58:27.694262\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6827/12662], [Dloss: 0.236408] [G loss: 0.338279] time: 0:58:28.855157\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6828/12662], [Dloss: 0.324260] [G loss: 0.652274] time: 0:58:30.142714\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6829/12662], [Dloss: 0.464368] [G loss: 0.640351] time: 0:58:31.363449\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6830/12662], [Dloss: 0.141477] [G loss: 0.359547] time: 0:58:32.622083\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6831/12662], [Dloss: 0.149975] [G loss: 0.200154] time: 0:58:33.797938\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6832/12662], [Dloss: 0.594563] [G loss: 0.257881] time: 0:58:35.010695\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6833/12662], [Dloss: 0.233860] [G loss: 0.431467] time: 0:58:36.253372\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6834/12662], [Dloss: 0.366984] [G loss: 0.161464] time: 0:58:37.500038\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6835/12662], [Dloss: 0.648417] [G loss: 0.171792] time: 0:58:38.717288\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6836/12662], [Dloss: 0.681572] [G loss: 0.289363] time: 0:58:39.902119\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6837/12662], [Dloss: 0.311308] [G loss: -0.120789] time: 0:58:41.122854\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6838/12662], [Dloss: 0.248591] [G loss: 0.063267] time: 0:58:42.274774\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6839/12662], [Dloss: 0.258610] [G loss: 0.086873] time: 0:58:43.475562\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6840/12662], [Dloss: 0.400112] [G loss: 0.333940] time: 0:58:44.751151\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6841/12662], [Dloss: -0.248750] [G loss: 0.308935] time: 0:58:45.962910\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6842/12662], [Dloss: 0.221505] [G loss: 0.113949] time: 0:58:47.133779\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6843/12662], [Dloss: 0.152844] [G loss: 0.865688] time: 0:58:48.377453\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6844/12662], [Dloss: -0.053570] [G loss: 0.981731] time: 0:58:49.703906\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6845/12662], [Dloss: 0.173021] [G loss: 0.810663] time: 0:58:50.884748\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6846/12662], [Dloss: 0.696382] [G loss: 0.390789] time: 0:58:52.163328\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6847/12662], [Dloss: 0.730787] [G loss: 0.549393] time: 0:58:53.432933\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6848/12662], [Dloss: -0.041506] [G loss: 0.622496] time: 0:58:54.711513\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6849/12662], [Dloss: 0.344587] [G loss: 0.416323] time: 0:58:55.857449\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6850/12662], [Dloss: 0.140270] [G loss: 0.472604] time: 0:58:57.156973\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6851/12662], [Dloss: 0.030534] [G loss: 0.074639] time: 0:58:58.430567\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6852/12662], [Dloss: 0.161752] [G loss: 0.423595] time: 0:58:59.725105\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6853/12662], [Dloss: 0.574978] [G loss: 0.435961] time: 0:59:00.865057\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6854/12662], [Dloss: -0.022929] [G loss: 0.125454] time: 0:59:02.075884\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6855/12662], [Dloss: 0.504865] [G loss: 0.353914] time: 0:59:03.327537\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6856/12662], [Dloss: 0.107011] [G loss: -0.098897] time: 0:59:04.611104\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6857/12662], [Dloss: 0.375508] [G loss: -0.173390] time: 0:59:05.821866\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6858/12662], [Dloss: 0.203792] [G loss: 0.387721] time: 0:59:06.992735\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6859/12662], [Dloss: 0.301124] [G loss: 0.588018] time: 0:59:08.151636\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6860/12662], [Dloss: -0.189917] [G loss: 0.247401] time: 0:59:09.365390\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6861/12662], [Dloss: 0.542572] [G loss: 0.392415] time: 0:59:10.587123\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6862/12662], [Dloss: -0.104169] [G loss: 0.675387] time: 0:59:11.834786\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6863/12662], [Dloss: 0.446509] [G loss: 0.671782] time: 0:59:13.065494\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6864/12662], [Dloss: 0.271459] [G loss: 0.369049] time: 0:59:14.224395\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6865/12662], [Dloss: 0.462678] [G loss: 0.467436] time: 0:59:15.474053\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6866/12662], [Dloss: 0.239554] [G loss: 0.409617] time: 0:59:16.683817\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6867/12662], [Dloss: 0.448922] [G loss: 0.197498] time: 0:59:17.881614\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6868/12662], [Dloss: 0.135987] [G loss: 0.552680] time: 0:59:19.045501\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6869/12662], [Dloss: 0.365910] [G loss: -0.304801] time: 0:59:20.295159\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6870/12662], [Dloss: 0.354602] [G loss: 0.026429] time: 0:59:21.556785\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6871/12662], [Dloss: 0.191848] [G loss: 0.040507] time: 0:59:22.803451\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6872/12662], [Dloss: 0.097327] [G loss: -0.160346] time: 0:59:23.991275\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6873/12662], [Dloss: 0.203287] [G loss: -0.211568] time: 0:59:25.247914\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6874/12662], [Dloss: 0.280439] [G loss: 0.211347] time: 0:59:26.441721\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6875/12662], [Dloss: 0.367573] [G loss: -0.286595] time: 0:59:27.674425\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6876/12662], [Dloss: 0.162814] [G loss: 0.072802] time: 0:59:28.909123\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6877/12662], [Dloss: 0.121303] [G loss: 0.321908] time: 0:59:30.064034\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6878/12662], [Dloss: 0.424657] [G loss: 0.555163] time: 0:59:31.346604\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6879/12662], [Dloss: 0.154904] [G loss: 0.622007] time: 0:59:32.566342\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6880/12662], [Dloss: 0.689196] [G loss: 0.012061] time: 0:59:33.786080\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6881/12662], [Dloss: 0.537738] [G loss: 0.386877] time: 0:59:34.949967\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6882/12662], [Dloss: 0.158084] [G loss: 0.355918] time: 0:59:36.102884\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6883/12662], [Dloss: 0.466152] [G loss: 0.556977] time: 0:59:37.275747\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6884/12662], [Dloss: 0.101128] [G loss: -0.058722] time: 0:59:38.421683\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6885/12662], [Dloss: 0.355522] [G loss: 0.000485] time: 0:59:39.655383\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6886/12662], [Dloss: 0.189548] [G loss: 0.207356] time: 0:59:40.868140\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6887/12662], [Dloss: -0.130244] [G loss: 0.068261] time: 0:59:42.139246\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6888/12662], [Dloss: 0.038601] [G loss: -0.186516] time: 0:59:43.378484\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6889/12662], [Dloss: 0.874224] [G loss: -0.017386] time: 0:59:44.611237\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6890/12662], [Dloss: 0.287641] [G loss: 0.286791] time: 0:59:45.832476\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6891/12662], [Dloss: 0.195857] [G loss: 0.481732] time: 0:59:47.054727\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6892/12662], [Dloss: 0.481032] [G loss: 0.514550] time: 0:59:48.337802\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6893/12662], [Dloss: 0.073168] [G loss: 0.583909] time: 0:59:49.540091\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6894/12662], [Dloss: 0.605571] [G loss: 0.279764] time: 0:59:50.790756\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6895/12662], [Dloss: 0.185168] [G loss: 0.064193] time: 0:59:52.001518\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6896/12662], [Dloss: -0.102480] [G loss: 0.435605] time: 0:59:53.187358\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6897/12662], [Dloss: 0.325628] [G loss: 0.616774] time: 0:59:54.448984\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6898/12662], [Dloss: 0.284523] [G loss: 0.584581] time: 0:59:55.661741\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6899/12662], [Dloss: 0.388405] [G loss: 0.198430] time: 0:59:56.880481\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6900/12662], [Dloss: 0.200968] [G loss: 0.020933] time: 0:59:58.088251\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6901/12662], [Dloss: 0.298038] [G loss: 0.154503] time: 0:59:59.365835\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6902/12662], [Dloss: 0.457381] [G loss: 0.107136] time: 1:00:00.613498\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6903/12662], [Dloss: -0.121416] [G loss: 0.077302] time: 1:00:01.837226\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6904/12662], [Dloss: -0.213241] [G loss: 0.046218] time: 1:00:02.979171\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6905/12662], [Dloss: 0.443717] [G loss: -0.052742] time: 1:00:04.287672\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6906/12662], [Dloss: 0.001768] [G loss: -0.078185] time: 1:00:05.505415\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6907/12662], [Dloss: 0.448326] [G loss: -0.301591] time: 1:00:06.816908\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6908/12662], [Dloss: 0.563283] [G loss: -0.124340] time: 1:00:08.068561\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6909/12662], [Dloss: 0.026484] [G loss: -0.322171] time: 1:00:09.299269\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6910/12662], [Dloss: 0.352223] [G loss: -0.352921] time: 1:00:10.583834\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6911/12662], [Dloss: -0.147277] [G loss: -0.351049] time: 1:00:11.824516\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6912/12662], [Dloss: 0.425208] [G loss: -0.264798] time: 1:00:13.041262\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6913/12662], [Dloss: -0.028478] [G loss: -0.132966] time: 1:00:14.331810\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6914/12662], [Dloss: 0.479802] [G loss: -0.171580] time: 1:00:15.531601\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6915/12662], [Dloss: 0.539435] [G loss: -0.140803] time: 1:00:16.784252\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6916/12662], [Dloss: 0.009800] [G loss: -0.135793] time: 1:00:17.996011\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6917/12662], [Dloss: 0.541345] [G loss: -0.207955] time: 1:00:19.142944\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6918/12662], [Dloss: 0.342258] [G loss: -0.166745] time: 1:00:20.358692\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6919/12662], [Dloss: 0.492095] [G loss: -0.028237] time: 1:00:21.534548\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6920/12662], [Dloss: 0.646914] [G loss: 0.085624] time: 1:00:22.784206\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6921/12662], [Dloss: -0.007222] [G loss: -0.373192] time: 1:00:24.016909\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6922/12662], [Dloss: 0.579973] [G loss: -0.189704] time: 1:00:25.256594\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6923/12662], [Dloss: 0.242054] [G loss: 0.041913] time: 1:00:26.389564\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6924/12662], [Dloss: 0.083689] [G loss: -0.232685] time: 1:00:27.600326\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6925/12662], [Dloss: 0.388488] [G loss: 0.187711] time: 1:00:28.904837\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6926/12662], [Dloss: 0.100685] [G loss: -0.186221] time: 1:00:30.093658\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6927/12662], [Dloss: 0.438043] [G loss: -0.249515] time: 1:00:31.272505\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6928/12662], [Dloss: 0.607413] [G loss: -0.139505] time: 1:00:32.561059\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6929/12662], [Dloss: 0.513098] [G loss: 0.524568] time: 1:00:33.815704\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6930/12662], [Dloss: -0.116089] [G loss: 0.204282] time: 1:00:35.012469\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6931/12662], [Dloss: 0.010897] [G loss: 0.396137] time: 1:00:36.221236\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6932/12662], [Dloss: 0.289052] [G loss: 0.092095] time: 1:00:37.478873\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6933/12662], [Dloss: -0.245961] [G loss: 0.441589] time: 1:00:38.736509\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6934/12662], [Dloss: 0.087920] [G loss: 0.041885] time: 1:00:39.900397\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6935/12662], [Dloss: 0.334766] [G loss: 0.163436] time: 1:00:41.200918\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6936/12662], [Dloss: 0.575239] [G loss: 0.077414] time: 1:00:42.447584\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6937/12662], [Dloss: 0.485439] [G loss: 0.572482] time: 1:00:43.662336\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6938/12662], [Dloss: 0.136691] [G loss: 0.405053] time: 1:00:44.954879\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6939/12662], [Dloss: 0.386760] [G loss: 0.439630] time: 1:00:46.162540\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6940/12662], [Dloss: 0.283592] [G loss: 0.372354] time: 1:00:47.463062\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6941/12662], [Dloss: 0.222452] [G loss: 0.030852] time: 1:00:48.644901\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 6942/12662], [Dloss: -0.014980] [G loss: 0.281222] time: 1:00:49.901540\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6943/12662], [Dloss: 0.014861] [G loss: 0.427962] time: 1:00:51.128260\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6944/12662], [Dloss: -0.052437] [G loss: 0.212034] time: 1:00:52.278184\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6945/12662], [Dloss: 0.414345] [G loss: -0.120190] time: 1:00:53.546792\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6946/12662], [Dloss: 0.114900] [G loss: 0.237738] time: 1:00:54.757554\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6947/12662], [Dloss: 0.119445] [G loss: 0.109156] time: 1:00:56.005217\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6948/12662], [Dloss: 0.514053] [G loss: -0.316137] time: 1:00:57.218971\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6949/12662], [Dloss: 0.314375] [G loss: 0.385911] time: 1:00:58.488576\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6950/12662], [Dloss: 0.122330] [G loss: 0.297288] time: 1:00:59.727263\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6951/12662], [Dloss: 0.369837] [G loss: 0.514108] time: 1:01:00.948996\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6952/12662], [Dloss: 0.374687] [G loss: 0.540465] time: 1:01:02.200648\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6953/12662], [Dloss: 0.159298] [G loss: 0.403960] time: 1:01:03.461277\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6954/12662], [Dloss: -0.076937] [G loss: 0.824223] time: 1:01:04.685936\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6955/12662], [Dloss: 0.207190] [G loss: 0.565443] time: 1:01:05.893213\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6956/12662], [Dloss: 0.235764] [G loss: 0.216529] time: 1:01:07.152844\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6957/12662], [Dloss: 0.027708] [G loss: 0.554909] time: 1:01:08.400507\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6958/12662], [Dloss: 0.034423] [G loss: 0.450589] time: 1:01:09.623237\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6959/12662], [Dloss: 0.477768] [G loss: 0.269438] time: 1:01:10.828987\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6960/12662], [Dloss: 0.744286] [G loss: -0.012765] time: 1:01:12.083632\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6961/12662], [Dloss: 0.450435] [G loss: 0.124404] time: 1:01:13.307359\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6962/12662], [Dloss: 0.223536] [G loss: -0.002408] time: 1:01:14.500170\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6963/12662], [Dloss: 0.029590] [G loss: 0.245643] time: 1:01:15.730878\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6964/12662], [Dloss: 0.359840] [G loss: -0.229684] time: 1:01:16.972064\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6965/12662], [Dloss: 0.447276] [G loss: 0.173248] time: 1:01:18.146922\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6966/12662], [Dloss: 0.479439] [G loss: 0.313287] time: 1:01:19.360676\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6967/12662], [Dloss: 0.104345] [G loss: 0.033106] time: 1:01:20.522076\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6968/12662], [Dloss: 0.363954] [G loss: 0.170473] time: 1:01:21.648065\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6969/12662], [Dloss: 0.185436] [G loss: 0.091093] time: 1:01:22.855835\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6970/12662], [Dloss: 0.074652] [G loss: -0.180806] time: 1:01:24.149376\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6971/12662], [Dloss: 0.092277] [G loss: -0.016399] time: 1:01:25.328223\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6972/12662], [Dloss: 0.408266] [G loss: 0.238911] time: 1:01:26.664649\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6973/12662], [Dloss: 0.239340] [G loss: 0.106284] time: 1:01:27.818563\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6974/12662], [Dloss: 0.428905] [G loss: 0.174636] time: 1:01:28.992423\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6975/12662], [Dloss: 0.138777] [G loss: 0.434907] time: 1:01:30.239089\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6976/12662], [Dloss: 0.069671] [G loss: 0.284582] time: 1:01:31.453841\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6977/12662], [Dloss: 0.336540] [G loss: 0.493908] time: 1:01:32.648645\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6978/12662], [Dloss: 0.132003] [G loss: 0.076802] time: 1:01:33.908277\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6979/12662], [Dloss: 0.598649] [G loss: -0.009464] time: 1:01:35.153945\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6980/12662], [Dloss: 0.573253] [G loss: 0.241021] time: 1:01:36.314841\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6981/12662], [Dloss: -0.159712] [G loss: 0.105266] time: 1:01:37.521613\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6982/12662], [Dloss: 0.660239] [G loss: 0.187324] time: 1:01:38.728386\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6983/12662], [Dloss: 0.299658] [G loss: 0.265321] time: 1:01:39.917206\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6984/12662], [Dloss: 0.542948] [G loss: -0.188467] time: 1:01:41.195787\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6985/12662], [Dloss: 0.566060] [G loss: -0.309333] time: 1:01:42.428491\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6986/12662], [Dloss: 0.495449] [G loss: -0.066809] time: 1:01:43.635263\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6987/12662], [Dloss: 0.196060] [G loss: 0.147519] time: 1:01:44.880942\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6988/12662], [Dloss: 0.217617] [G loss: 0.408243] time: 1:01:46.149561\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6989/12662], [Dloss: 0.474300] [G loss: 0.099142] time: 1:01:47.373288\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6990/12662], [Dloss: -0.055105] [G loss: 0.559738] time: 1:01:48.643889\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6991/12662], [Dloss: 0.123820] [G loss: 0.289219] time: 1:01:49.928454\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6992/12662], [Dloss: 0.324574] [G loss: 0.088879] time: 1:01:51.130240\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6993/12662], [Dloss: 0.291687] [G loss: 0.336317] time: 1:01:52.358482\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6994/12662], [Dloss: 0.477741] [G loss: -0.153290] time: 1:01:53.563259\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6995/12662], [Dloss: 0.134232] [G loss: 0.252778] time: 1:01:54.852811\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6996/12662], [Dloss: 0.283148] [G loss: 0.168265] time: 1:01:56.100486\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6997/12662], [Dloss: 0.449244] [G loss: 0.217025] time: 1:01:57.250411\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6998/12662], [Dloss: 0.524003] [G loss: 0.308801] time: 1:01:58.526997\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 6999/12662], [Dloss: 0.427717] [G loss: 0.361723] time: 1:01:59.694873\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7000/12662], [Dloss: 0.444526] [G loss: 0.383260] time: 1:02:00.865742\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7000/12662], [Dloss: 0.444526] [G loss: 0.383260] time: 1:02:00.865742\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7001/12662], [Dloss: 0.656092] [G loss: 0.156400] time: 1:02:02.938199\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7002/12662], [Dloss: 0.412459] [G loss: 0.163661] time: 1:02:04.138988\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7003/12662], [Dloss: -0.100153] [G loss: 0.343436] time: 1:02:05.342769\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7004/12662], [Dloss: 0.513516] [G loss: 0.439515] time: 1:02:06.568491\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7005/12662], [Dloss: 0.055288] [G loss: 0.414620] time: 1:02:07.856573\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7006/12662], [Dloss: 0.164131] [G loss: 0.309904] time: 1:02:09.134667\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7007/12662], [Dloss: 0.334263] [G loss: -0.056105] time: 1:02:10.380840\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7008/12662], [Dloss: 0.260479] [G loss: 0.114359] time: 1:02:11.586121\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7009/12662], [Dloss: 0.163429] [G loss: -0.004515] time: 1:02:12.835780\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7010/12662], [Dloss: 0.602851] [G loss: -0.179195] time: 1:02:14.081952\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7011/12662], [Dloss: 0.279663] [G loss: 0.120636] time: 1:02:15.259801\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7012/12662], [Dloss: 0.291293] [G loss: -0.379471] time: 1:02:16.462091\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7013/12662], [Dloss: 0.117857] [G loss: -0.582084] time: 1:02:17.654901\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7014/12662], [Dloss: 0.453145] [G loss: -0.297912] time: 1:02:18.834253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7015/12662], [Dloss: 0.070641] [G loss: -0.031862] time: 1:02:20.072940\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7016/12662], [Dloss: 0.039269] [G loss: -0.252122] time: 1:02:21.389419\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7017/12662], [Dloss: 0.199938] [G loss: -0.212356] time: 1:02:22.624118\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7018/12662], [Dloss: 0.364305] [G loss: 0.135339] time: 1:02:23.816927\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7019/12662], [Dloss: 0.279004] [G loss: 0.057317] time: 1:02:25.006745\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7020/12662], [Dloss: 0.185015] [G loss: -0.381714] time: 1:02:26.291310\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7021/12662], [Dloss: 0.298719] [G loss: -0.289936] time: 1:02:27.503069\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7022/12662], [Dloss: 0.283161] [G loss: -0.258264] time: 1:02:28.718818\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7023/12662], [Dloss: 0.438576] [G loss: -0.293097] time: 1:02:29.999393\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7024/12662], [Dloss: -0.016588] [G loss: -0.110301] time: 1:02:31.199184\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7025/12662], [Dloss: 0.223090] [G loss: -0.192596] time: 1:02:32.401968\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7026/12662], [Dloss: 0.297116] [G loss: -0.233673] time: 1:02:33.665589\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7027/12662], [Dloss: 0.163300] [G loss: -0.265316] time: 1:02:34.855406\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7028/12662], [Dloss: 0.201436] [G loss: -0.046928] time: 1:02:36.020291\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7029/12662], [Dloss: 0.370558] [G loss: 0.095928] time: 1:02:37.254989\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7030/12662], [Dloss: 0.373904] [G loss: -0.379866] time: 1:02:38.527586\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7031/12662], [Dloss: 0.392879] [G loss: 0.678582] time: 1:02:39.692470\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7032/12662], [Dloss: 0.396836] [G loss: 0.219148] time: 1:02:40.897248\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7033/12662], [Dloss: -0.033190] [G loss: 0.128680] time: 1:02:42.126960\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7034/12662], [Dloss: 0.278740] [G loss: -0.167054] time: 1:02:43.347695\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7035/12662], [Dloss: 0.123473] [G loss: -0.136452] time: 1:02:44.543497\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7036/12662], [Dloss: 0.645856] [G loss: 0.239221] time: 1:02:45.669485\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7037/12662], [Dloss: 0.370328] [G loss: -0.052755] time: 1:02:46.923133\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7038/12662], [Dloss: 0.499433] [G loss: 0.202692] time: 1:02:48.163815\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7039/12662], [Dloss: 0.259915] [G loss: 0.326210] time: 1:02:49.460348\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7040/12662], [Dloss: 0.531767] [G loss: 0.218604] time: 1:02:50.735936\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7041/12662], [Dloss: 0.067438] [G loss: 0.545769] time: 1:02:51.891351\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7042/12662], [Dloss: 0.434327] [G loss: 0.211474] time: 1:02:53.169932\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7043/12662], [Dloss: 0.526933] [G loss: 0.462373] time: 1:02:54.360748\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7044/12662], [Dloss: 0.335972] [G loss: 0.028586] time: 1:02:55.592454\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7045/12662], [Dloss: 0.549791] [G loss: 0.278690] time: 1:02:56.802218\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7046/12662], [Dloss: 0.114047] [G loss: 0.595537] time: 1:02:58.025453\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7047/12662], [Dloss: 0.087523] [G loss: 0.001675] time: 1:02:59.161415\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7048/12662], [Dloss: 0.168280] [G loss: -0.091223] time: 1:03:00.293388\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7049/12662], [Dloss: 0.351224] [G loss: -0.363191] time: 1:03:01.542125\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7050/12662], [Dloss: 0.290162] [G loss: -0.205243] time: 1:03:02.771836\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7051/12662], [Dloss: 0.426818] [G loss: -0.115319] time: 1:03:03.958662\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7052/12662], [Dloss: 0.426474] [G loss: 0.169637] time: 1:03:05.173413\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7053/12662], [Dloss: 0.283331] [G loss: 0.216025] time: 1:03:06.358246\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7054/12662], [Dloss: 0.053868] [G loss: 0.278604] time: 1:03:07.572996\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7055/12662], [Dloss: 0.542816] [G loss: 0.257750] time: 1:03:08.807694\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7056/12662], [Dloss: 0.186612] [G loss: 0.314588] time: 1:03:10.058349\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7057/12662], [Dloss: 0.612413] [G loss: 0.244257] time: 1:03:11.322967\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7058/12662], [Dloss: 0.158660] [G loss: 0.488897] time: 1:03:12.578609\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7059/12662], [Dloss: 0.117227] [G loss: 0.051606] time: 1:03:13.877136\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7060/12662], [Dloss: 0.334450] [G loss: 0.343643] time: 1:03:15.090891\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7061/12662], [Dloss: 0.275493] [G loss: -0.131741] time: 1:03:16.342543\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7062/12662], [Dloss: 0.765430] [G loss: -0.036666] time: 1:03:17.554303\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7063/12662], [Dloss: 0.393623] [G loss: 0.706814] time: 1:03:18.700238\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7064/12662], [Dloss: 0.606573] [G loss: 0.348493] time: 1:03:19.899032\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7065/12662], [Dloss: 0.245623] [G loss: 0.158984] time: 1:03:21.097826\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7066/12662], [Dloss: 0.248887] [G loss: 0.143068] time: 1:03:22.244759\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7067/12662], [Dloss: 0.179661] [G loss: 0.098341] time: 1:03:23.557249\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7068/12662], [Dloss: 0.219268] [G loss: -0.280611] time: 1:03:24.784473\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7069/12662], [Dloss: 0.303997] [G loss: 0.200469] time: 1:03:26.064051\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7070/12662], [Dloss: 0.576956] [G loss: -0.145962] time: 1:03:27.247392\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7071/12662], [Dloss: 0.203374] [G loss: 0.561883] time: 1:03:28.497051\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7072/12662], [Dloss: 0.460807] [G loss: 0.223016] time: 1:03:29.792586\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7073/12662], [Dloss: 0.261068] [G loss: 0.333635] time: 1:03:31.047735\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7074/12662], [Dloss: 0.436629] [G loss: 0.286908] time: 1:03:32.278444\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7075/12662], [Dloss: 0.411190] [G loss: 0.061668] time: 1:03:33.544059\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7076/12662], [Dloss: 0.172583] [G loss: 0.469546] time: 1:03:34.774767\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7077/12662], [Dloss: 0.533088] [G loss: 0.181244] time: 1:03:36.014452\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7078/12662], [Dloss: 0.524250] [G loss: 0.033354] time: 1:03:37.190308\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7079/12662], [Dloss: 0.011212] [G loss: 0.206894] time: 1:03:38.435976\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7080/12662], [Dloss: 0.349762] [G loss: 0.194544] time: 1:03:39.672669\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7081/12662], [Dloss: 0.156487] [G loss: 0.413402] time: 1:03:40.834561\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7082/12662], [Dloss: 0.247718] [G loss: -0.116600] time: 1:03:42.018395\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7083/12662], [Dloss: 0.390202] [G loss: 0.103022] time: 1:03:43.195762\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7084/12662], [Dloss: 0.131562] [G loss: 0.128077] time: 1:03:44.433965\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7085/12662], [Dloss: 0.689368] [G loss: -0.161524] time: 1:03:45.661199\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7086/12662], [Dloss: 0.356174] [G loss: -0.111223] time: 1:03:46.884926\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7087/12662], [Dloss: 0.288043] [G loss: 0.068864] time: 1:03:48.137576\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7088/12662], [Dloss: 0.503578] [G loss: 0.366271] time: 1:03:49.317925\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7089/12662], [Dloss: 0.299277] [G loss: 0.151451] time: 1:03:50.549644\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7090/12662], [Dloss: -0.010040] [G loss: 0.307388] time: 1:03:51.776363\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7091/12662], [Dloss: 0.547168] [G loss: 0.126963] time: 1:03:52.957205\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7092/12662], [Dloss: 0.148339] [G loss: -0.024310] time: 1:03:54.158497\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7093/12662], [Dloss: 0.409662] [G loss: 0.458571] time: 1:03:55.355297\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7094/12662], [Dloss: 0.227415] [G loss: 0.369720] time: 1:03:56.560075\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7095/12662], [Dloss: 0.384212] [G loss: 0.140582] time: 1:03:57.874559\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7096/12662], [Dloss: 0.257224] [G loss: 0.136281] time: 1:03:59.014511\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7097/12662], [Dloss: 0.277516] [G loss: 0.135893] time: 1:04:00.153464\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7098/12662], [Dloss: 0.089347] [G loss: -0.102226] time: 1:04:01.390157\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7099/12662], [Dloss: 0.589572] [G loss: 0.129552] time: 1:04:02.622860\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7100/12662], [Dloss: 0.490720] [G loss: -0.076007] time: 1:04:03.806694\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7101/12662], [Dloss: 0.457812] [G loss: -0.312229] time: 1:04:05.078294\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7102/12662], [Dloss: 0.451938] [G loss: -0.558821] time: 1:04:06.489520\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7103/12662], [Dloss: -0.272251] [G loss: -0.275339] time: 1:04:07.864841\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7104/12662], [Dloss: 0.254479] [G loss: -0.586522] time: 1:04:09.070618\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7105/12662], [Dloss: 0.221800] [G loss: 0.132523] time: 1:04:10.349702\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7106/12662], [Dloss: 0.218443] [G loss: 0.190667] time: 1:04:11.550490\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7107/12662], [Dloss: 0.096760] [G loss: 0.652198] time: 1:04:12.820095\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7108/12662], [Dloss: -0.245528] [G loss: 0.345022] time: 1:04:14.055790\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7109/12662], [Dloss: 0.570551] [G loss: 0.187913] time: 1:04:15.330382\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7110/12662], [Dloss: 0.482118] [G loss: 0.329275] time: 1:04:16.504242\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7111/12662], [Dloss: 0.266226] [G loss: 0.283473] time: 1:04:17.702039\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7112/12662], [Dloss: 0.384394] [G loss: 0.571837] time: 1:04:18.962667\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7113/12662], [Dloss: 0.511953] [G loss: 0.233467] time: 1:04:20.095638\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7114/12662], [Dloss: -0.017479] [G loss: 0.222436] time: 1:04:21.277478\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7115/12662], [Dloss: 0.779815] [G loss: -0.118540] time: 1:04:22.562042\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7116/12662], [Dloss: 0.337926] [G loss: -0.216935] time: 1:04:23.858574\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7117/12662], [Dloss: 0.252103] [G loss: -0.236222] time: 1:04:25.044413\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7118/12662], [Dloss: 0.254178] [G loss: -0.131583] time: 1:04:26.266146\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7119/12662], [Dloss: 0.440496] [G loss: 0.346619] time: 1:04:27.506828\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7120/12662], [Dloss: 0.513152] [G loss: 0.300474] time: 1:04:28.770448\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7121/12662], [Dloss: 0.488590] [G loss: 0.468221] time: 1:04:30.103882\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7122/12662], [Dloss: 0.169767] [G loss: 0.396748] time: 1:04:31.513126\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7123/12662], [Dloss: 0.292325] [G loss: 0.604861] time: 1:04:32.862026\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7124/12662], [Dloss: -0.137577] [G loss: 0.445619] time: 1:04:34.100713\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7125/12662], [Dloss: 0.051576] [G loss: 0.547198] time: 1:04:35.376302\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7126/12662], [Dloss: 0.362843] [G loss: 0.166536] time: 1:04:36.617981\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7127/12662], [Dloss: 0.256761] [G loss: -0.243407] time: 1:04:37.855671\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7128/12662], [Dloss: 0.503066] [G loss: -0.022076] time: 1:04:39.140739\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7129/12662], [Dloss: 0.022510] [G loss: -0.617362] time: 1:04:40.367459\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7130/12662], [Dloss: 0.151426] [G loss: -0.151847] time: 1:04:41.562264\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7131/12662], [Dloss: 0.270238] [G loss: -0.303743] time: 1:04:42.808930\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7132/12662], [Dloss: 0.536168] [G loss: 0.016825] time: 1:04:44.017697\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7133/12662], [Dloss: 0.254108] [G loss: 0.111217] time: 1:04:45.142689\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7134/12662], [Dloss: 0.184570] [G loss: -0.092386] time: 1:04:46.341482\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7135/12662], [Dloss: 0.105778] [G loss: 0.430523] time: 1:04:47.528308\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7136/12662], [Dloss: 0.468027] [G loss: -0.140610] time: 1:04:48.728099\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7137/12662], [Dloss: 0.738404] [G loss: 0.218174] time: 1:04:49.963795\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7138/12662], [Dloss: 0.165024] [G loss: 0.427161] time: 1:04:51.198493\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7139/12662], [Dloss: 0.486983] [G loss: 0.428241] time: 1:04:52.560849\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7140/12662], [Dloss: -0.076417] [G loss: 0.707806] time: 1:04:53.770614\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7141/12662], [Dloss: 0.217167] [G loss: 0.410610] time: 1:04:54.975392\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7142/12662], [Dloss: 0.353080] [G loss: 0.373418] time: 1:04:56.165210\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7143/12662], [Dloss: 0.330780] [G loss: 0.537866] time: 1:04:57.377967\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7144/12662], [Dloss: 0.438433] [G loss: 0.342520] time: 1:04:58.604686\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7145/12662], [Dloss: 0.307560] [G loss: 0.249517] time: 1:04:59.859331\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7146/12662], [Dloss: 0.310145] [G loss: 0.001538] time: 1:05:01.045160\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7147/12662], [Dloss: 0.077132] [G loss: -0.113584] time: 1:05:02.210044\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7148/12662], [Dloss: -0.071676] [G loss: -0.118295] time: 1:05:03.460699\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7149/12662], [Dloss: 0.217292] [G loss: 0.192258] time: 1:05:04.694400\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7150/12662], [Dloss: 0.068439] [G loss: 0.124673] time: 1:05:05.920122\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7151/12662], [Dloss: 0.573951] [G loss: -0.228508] time: 1:05:07.145844\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7152/12662], [Dloss: 0.366996] [G loss: 0.227242] time: 1:05:08.316713\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7153/12662], [Dloss: 0.485644] [G loss: 0.292495] time: 1:05:09.481597\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7154/12662], [Dloss: 0.344432] [G loss: 0.369546] time: 1:05:10.706322\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7155/12662], [Dloss: 0.192885] [G loss: 0.324877] time: 1:05:11.973932\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7156/12662], [Dloss: 0.207870] [G loss: 0.400364] time: 1:05:13.203643\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7157/12662], [Dloss: 0.456738] [G loss: 0.089246] time: 1:05:14.396453\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7158/12662], [Dloss: 0.197956] [G loss: 0.066664] time: 1:05:15.600234\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7159/12662], [Dloss: 0.310196] [G loss: -0.252764] time: 1:05:16.810996\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7160/12662], [Dloss: 0.047755] [G loss: -0.117553] time: 1:05:18.019764\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7161/12662], [Dloss: 0.510567] [G loss: -0.475561] time: 1:05:19.209581\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7162/12662], [Dloss: 0.154825] [G loss: -0.494900] time: 1:05:20.376460\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7163/12662], [Dloss: 0.199276] [G loss: -0.078726] time: 1:05:21.580241\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7164/12662], [Dloss: -0.135299] [G loss: -0.234490] time: 1:05:22.786017\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7165/12662], [Dloss: -0.280336] [G loss: 0.155851] time: 1:05:23.935941\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7166/12662], [Dloss: 0.387267] [G loss: 0.362806] time: 1:05:25.115786\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7167/12662], [Dloss: 0.412937] [G loss: 0.223557] time: 1:05:26.356468\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7168/12662], [Dloss: 0.060136] [G loss: 0.170555] time: 1:05:27.606126\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7169/12662], [Dloss: 0.181910] [G loss: 0.614927] time: 1:05:28.836835\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7170/12662], [Dloss: 0.401121] [G loss: 0.378541] time: 1:05:30.060562\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7171/12662], [Dloss: 0.439816] [G loss: 0.353344] time: 1:05:31.258358\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7172/12662], [Dloss: 0.118213] [G loss: 0.372726] time: 1:05:32.534945\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7173/12662], [Dloss: 0.028209] [G loss: 0.083163] time: 1:05:33.728752\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7174/12662], [Dloss: -0.300852] [G loss: 0.314025] time: 1:05:35.014314\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7175/12662], [Dloss: 0.371094] [G loss: 0.314282] time: 1:05:36.245022\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7176/12662], [Dloss: -0.115480] [G loss: -0.044067] time: 1:05:37.506649\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7177/12662], [Dloss: 0.511914] [G loss: -0.590136] time: 1:05:38.667544\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7178/12662], [Dloss: 0.484264] [G loss: -0.180812] time: 1:05:39.897255\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7179/12662], [Dloss: 0.092631] [G loss: 0.273257] time: 1:05:41.127964\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7180/12662], [Dloss: 0.304394] [G loss: 0.092358] time: 1:05:42.332741\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7181/12662], [Dloss: 0.639442] [G loss: 0.178081] time: 1:05:43.504608\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7182/12662], [Dloss: 0.390769] [G loss: 0.417563] time: 1:05:44.672484\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7183/12662], [Dloss: 0.320012] [G loss: 0.701758] time: 1:05:45.892222\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7184/12662], [Dloss: 0.133533] [G loss: 0.362351] time: 1:05:47.117913\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7185/12662], [Dloss: 0.076543] [G loss: 0.305697] time: 1:05:48.375550\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7186/12662], [Dloss: 0.067795] [G loss: 0.647253] time: 1:05:49.704994\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7187/12662], [Dloss: 0.618186] [G loss: 0.195642] time: 1:05:50.904785\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7188/12662], [Dloss: 0.377596] [G loss: 0.380515] time: 1:05:52.041745\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7189/12662], [Dloss: 0.025223] [G loss: 0.031435] time: 1:05:53.175712\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7190/12662], [Dloss: 0.158067] [G loss: 0.440975] time: 1:05:54.342592\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7191/12662], [Dloss: 0.355011] [G loss: 0.803426] time: 1:05:55.544377\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7192/12662], [Dloss: 0.321262] [G loss: 0.222089] time: 1:05:56.777080\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7193/12662], [Dloss: 0.231376] [G loss: 0.551743] time: 1:05:57.932989\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7194/12662], [Dloss: 0.620210] [G loss: 0.354420] time: 1:05:59.123805\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7195/12662], [Dloss: 0.405724] [G loss: 0.359560] time: 1:06:00.337558\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7196/12662], [Dloss: 0.575407] [G loss: 0.420830] time: 1:06:01.602176\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7197/12662], [Dloss: 0.491541] [G loss: 0.341901] time: 1:06:02.770053\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7198/12662], [Dloss: 0.141472] [G loss: 0.424247] time: 1:06:03.933940\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7199/12662], [Dloss: 0.756604] [G loss: 0.580828] time: 1:06:05.152682\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7200/12662], [Dloss: 0.336955] [G loss: 0.704253] time: 1:06:06.307592\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7201/12662], [Dloss: 0.220267] [G loss: 0.426748] time: 1:06:07.559245\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7202/12662], [Dloss: -0.071752] [G loss: 0.555274] time: 1:06:08.688226\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7203/12662], [Dloss: 0.315353] [G loss: 0.182653] time: 1:06:09.915943\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7204/12662], [Dloss: 0.644463] [G loss: -0.296343] time: 1:06:11.096784\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7205/12662], [Dloss: 0.183771] [G loss: -0.176240] time: 1:06:12.262666\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7206/12662], [Dloss: 0.465039] [G loss: 0.096698] time: 1:06:13.426554\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7207/12662], [Dloss: 0.635042] [G loss: -0.664846] time: 1:06:14.624350\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7208/12662], [Dloss: 0.513890] [G loss: -0.160526] time: 1:06:15.882984\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7209/12662], [Dloss: 0.433631] [G loss: -0.017677] time: 1:06:17.065821\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7210/12662], [Dloss: 0.472909] [G loss: -0.130810] time: 1:06:18.291543\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7211/12662], [Dloss: -0.065608] [G loss: 0.237700] time: 1:06:19.427505\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7212/12662], [Dloss: 0.283353] [G loss: -0.052807] time: 1:06:20.624304\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7213/12662], [Dloss: 0.142412] [G loss: 0.808022] time: 1:06:21.822101\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7214/12662], [Dloss: 0.399264] [G loss: 0.673413] time: 1:06:22.981002\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7215/12662], [Dloss: 0.316214] [G loss: 0.673911] time: 1:06:24.191764\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7216/12662], [Dloss: 0.363412] [G loss: 0.519202] time: 1:06:25.487299\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7217/12662], [Dloss: 0.414521] [G loss: 0.466225] time: 1:06:26.714018\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7218/12662], [Dloss: 0.562081] [G loss: 0.627394] time: 1:06:27.961682\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7219/12662], [Dloss: 0.413248] [G loss: 0.068512] time: 1:06:29.219318\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7220/12662], [Dloss: 0.564230] [G loss: -0.064725] time: 1:06:30.647499\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7221/12662], [Dloss: 0.190398] [G loss: -0.252807] time: 1:06:32.014842\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7222/12662], [Dloss: 0.422002] [G loss: -0.484368] time: 1:06:33.314367\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7223/12662], [Dloss: 0.463785] [G loss: -0.217730] time: 1:06:34.615886\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7224/12662], [Dloss: 0.346638] [G loss: 0.351300] time: 1:06:35.899453\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7225/12662], [Dloss: 0.316492] [G loss: 0.105524] time: 1:06:37.236877\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7226/12662], [Dloss: 0.124220] [G loss: 0.176753] time: 1:06:38.471574\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7227/12662], [Dloss: 0.116739] [G loss: 0.204262] time: 1:06:39.647430\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7228/12662], [Dloss: 0.150177] [G loss: 0.467508] time: 1:06:40.925013\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7229/12662], [Dloss: 0.291892] [G loss: 0.024716] time: 1:06:42.200122\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7230/12662], [Dloss: 0.209299] [G loss: 0.112140] time: 1:06:43.563476\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7231/12662], [Dloss: 0.473541] [G loss: 0.255577] time: 1:06:44.747310\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7232/12662], [Dloss: 0.270113] [G loss: 0.547235] time: 1:06:46.037858\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7233/12662], [Dloss: 0.037238] [G loss: 0.406039] time: 1:06:47.283528\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7234/12662], [Dloss: 0.525990] [G loss: 0.620121] time: 1:06:48.513238\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7235/12662], [Dloss: 0.395455] [G loss: 0.293702] time: 1:06:49.986299\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7236/12662], [Dloss: 0.563027] [G loss: -0.081218] time: 1:06:51.188085\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7237/12662], [Dloss: -0.019492] [G loss: 0.014514] time: 1:06:52.431759\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7238/12662], [Dloss: 0.423338] [G loss: 0.031621] time: 1:06:53.562734\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7239/12662], [Dloss: 0.258924] [G loss: -0.058635] time: 1:06:54.721635\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7240/12662], [Dloss: 0.392466] [G loss: -0.080724] time: 1:06:55.974285\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7241/12662], [Dloss: 0.244646] [G loss: 0.208471] time: 1:06:57.183064\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7242/12662], [Dloss: 0.459728] [G loss: -0.145280] time: 1:06:58.360914\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7243/12662], [Dloss: 0.257480] [G loss: 0.271050] time: 1:06:59.535772\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7244/12662], [Dloss: 0.166673] [G loss: -0.056357] time: 1:07:00.762492\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7245/12662], [Dloss: 0.427106] [G loss: -0.087672] time: 1:07:02.025115\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7246/12662], [Dloss: 0.345613] [G loss: 0.032107] time: 1:07:03.285743\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7247/12662], [Dloss: 0.478405] [G loss: -0.018537] time: 1:07:04.466585\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7248/12662], [Dloss: 0.481815] [G loss: 0.186744] time: 1:07:05.758292\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7249/12662], [Dloss: 0.334097] [G loss: 0.143752] time: 1:07:06.991993\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7250/12662], [Dloss: 0.157766] [G loss: 0.418668] time: 1:07:08.251624\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7251/12662], [Dloss: 0.205215] [G loss: -0.120169] time: 1:07:09.502279\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7252/12662], [Dloss: 0.361937] [G loss: 0.032683] time: 1:07:10.727004\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7253/12662], [Dloss: 0.534784] [G loss: -0.346414] time: 1:07:11.988630\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7254/12662], [Dloss: 0.397529] [G loss: -0.130859] time: 1:07:13.220349\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7255/12662], [Dloss: 0.196494] [G loss: -0.353669] time: 1:07:14.401191\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7256/12662], [Dloss: 0.232348] [G loss: -0.308346] time: 1:07:15.611954\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7257/12662], [Dloss: 0.158129] [G loss: -0.025906] time: 1:07:16.797782\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7258/12662], [Dloss: 0.079539] [G loss: -0.159553] time: 1:07:18.107280\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7259/12662], [Dloss: 0.125261] [G loss: -0.054894] time: 1:07:19.370901\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7260/12662], [Dloss: 0.314809] [G loss: -0.143926] time: 1:07:20.709321\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7261/12662], [Dloss: 0.758190] [G loss: 0.092748] time: 1:07:21.942025\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7262/12662], [Dloss: 0.244849] [G loss: -0.081661] time: 1:07:23.178717\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7263/12662], [Dloss: 0.356921] [G loss: 0.260463] time: 1:07:24.352578\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7264/12662], [Dloss: 0.411114] [G loss: 0.274487] time: 1:07:25.657089\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7265/12662], [Dloss: 0.198063] [G loss: 0.286617] time: 1:07:26.886800\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7266/12662], [Dloss: 0.501903] [G loss: 0.040596] time: 1:07:28.095568\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7267/12662], [Dloss: 0.376031] [G loss: -0.218384] time: 1:07:29.284388\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7268/12662], [Dloss: 0.382694] [G loss: 0.103308] time: 1:07:30.521081\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7269/12662], [Dloss: 0.115909] [G loss: -0.170806] time: 1:07:31.802654\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7270/12662], [Dloss: 0.242751] [G loss: 0.137928] time: 1:07:33.112152\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7271/12662], [Dloss: 0.345013] [G loss: -0.000053] time: 1:07:34.390239\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7272/12662], [Dloss: 0.330515] [G loss: 0.276643] time: 1:07:35.614963\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7273/12662], [Dloss: 0.465229] [G loss: -0.290284] time: 1:07:36.883571\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7274/12662], [Dloss: 0.278790] [G loss: 0.016754] time: 1:07:38.126248\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7275/12662], [Dloss: 0.337802] [G loss: 0.187077] time: 1:07:39.363937\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7276/12662], [Dloss: 0.297637] [G loss: 0.049229] time: 1:07:40.701361\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7277/12662], [Dloss: 0.272052] [G loss: 0.290514] time: 1:07:42.066720\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7278/12662], [Dloss: 0.339307] [G loss: 0.220983] time: 1:07:43.303501\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7279/12662], [Dloss: 0.204750] [G loss: 0.139687] time: 1:07:44.456429\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7280/12662], [Dloss: 0.386118] [G loss: 0.392776] time: 1:07:45.660209\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7281/12662], [Dloss: 0.177484] [G loss: 0.649638] time: 1:07:46.917352\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7282/12662], [Dloss: 0.176729] [G loss: 0.811715] time: 1:07:48.146571\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7283/12662], [Dloss: 0.598046] [G loss: 0.578322] time: 1:07:49.279058\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7284/12662], [Dloss: 0.499712] [G loss: 0.506068] time: 1:07:50.456919\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7285/12662], [Dloss: 0.226168] [G loss: 0.475817] time: 1:07:51.767415\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7286/12662], [Dloss: 0.427166] [G loss: 0.518655] time: 1:07:52.976182\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7287/12662], [Dloss: -0.123076] [G loss: 0.418523] time: 1:07:54.213872\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7288/12662], [Dloss: 0.372262] [G loss: 0.095672] time: 1:07:55.479487\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7289/12662], [Dloss: 0.532681] [G loss: 0.417337] time: 1:07:56.711193\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7290/12662], [Dloss: 0.250080] [G loss: 0.263454] time: 1:07:57.943897\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7291/12662], [Dloss: 0.425834] [G loss: 0.212504] time: 1:07:59.226466\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7292/12662], [Dloss: 0.437785] [G loss: 0.323814] time: 1:08:00.423266\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7293/12662], [Dloss: 0.223625] [G loss: 0.242106] time: 1:08:01.623057\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7294/12662], [Dloss: 0.654445] [G loss: -0.131166] time: 1:08:02.854763\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7295/12662], [Dloss: 0.476072] [G loss: -0.009850] time: 1:08:04.103424\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7296/12662], [Dloss: 0.207167] [G loss: 0.136928] time: 1:08:05.306207\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7297/12662], [Dloss: 0.256913] [G loss: 0.012235] time: 1:08:06.591769\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7298/12662], [Dloss: 0.426961] [G loss: 0.148440] time: 1:08:07.759646\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7299/12662], [Dloss: 0.255038] [G loss: -0.248666] time: 1:08:08.985871\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7300/12662], [Dloss: 0.719131] [G loss: 0.143058] time: 1:08:10.229558\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7301/12662], [Dloss: 0.297881] [G loss: 0.163171] time: 1:08:11.501157\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7302/12662], [Dloss: 0.481808] [G loss: 0.116717] time: 1:08:12.733861\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7303/12662], [Dloss: 0.509959] [G loss: 0.230240] time: 1:08:13.997481\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7304/12662], [Dloss: 0.019070] [G loss: 0.281480] time: 1:08:15.335422\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7305/12662], [Dloss: 0.548790] [G loss: 0.087926] time: 1:08:16.562142\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7306/12662], [Dloss: 0.424130] [G loss: 0.042983] time: 1:08:17.785869\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7307/12662], [Dloss: 0.113893] [G loss: 0.106185] time: 1:08:18.983724\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7308/12662], [Dloss: 0.339198] [G loss: 0.059332] time: 1:08:20.169553\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7309/12662], [Dloss: 0.491570] [G loss: -0.069008] time: 1:08:21.494011\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7310/12662], [Dloss: 0.097744] [G loss: 0.084605] time: 1:08:22.975050\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7311/12662], [Dloss: 0.283884] [G loss: 0.076257] time: 1:08:24.324441\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7312/12662], [Dloss: 0.101389] [G loss: 0.243627] time: 1:08:25.575097\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7313/12662], [Dloss: 0.135478] [G loss: 0.114730] time: 1:08:26.792840\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7314/12662], [Dloss: 0.435071] [G loss: -0.059347] time: 1:08:28.054466\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7315/12662], [Dloss: 0.282756] [G loss: -0.210180] time: 1:08:29.318086\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7316/12662], [Dloss: 0.405092] [G loss: -0.126023] time: 1:08:30.592678\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7317/12662], [Dloss: 0.025609] [G loss: -0.219182] time: 1:08:31.846325\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7318/12662], [Dloss: 0.513284] [G loss: 0.269304] time: 1:08:33.108948\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7319/12662], [Dloss: 0.409225] [G loss: 0.638651] time: 1:08:34.309737\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7320/12662], [Dloss: 0.425965] [G loss: 0.398412] time: 1:08:35.516510\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7321/12662], [Dloss: 0.279309] [G loss: 0.743373] time: 1:08:36.692365\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7322/12662], [Dloss: 0.156492] [G loss: 0.590901] time: 1:08:37.995879\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7323/12662], [Dloss: 0.159885] [G loss: 0.435779] time: 1:08:39.305377\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7324/12662], [Dloss: 0.023724] [G loss: 0.395992] time: 1:08:40.550048\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7325/12662], [Dloss: 0.154768] [G loss: 0.367219] time: 1:08:41.940330\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7326/12662], [Dloss: 0.164112] [G loss: 0.289637] time: 1:08:43.100733\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7327/12662], [Dloss: 0.424692] [G loss: 0.042727] time: 1:08:44.284567\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7328/12662], [Dloss: 0.231614] [G loss: 0.056201] time: 1:08:45.433495\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7329/12662], [Dloss: 0.235813] [G loss: -0.305302] time: 1:08:46.674177\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7330/12662], [Dloss: 0.395468] [G loss: 0.098753] time: 1:08:48.007611\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7331/12662], [Dloss: 0.609324] [G loss: 0.319564] time: 1:08:49.253279\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7332/12662], [Dloss: 0.443900] [G loss: -0.073601] time: 1:08:50.551807\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7333/12662], [Dloss: 0.118909] [G loss: 0.246046] time: 1:08:51.822409\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7334/12662], [Dloss: 0.430550] [G loss: 0.559848] time: 1:08:53.159832\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7335/12662], [Dloss: 0.250882] [G loss: 0.257863] time: 1:08:54.465341\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7336/12662], [Dloss: 0.313791] [G loss: 0.491195] time: 1:08:55.772844\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7337/12662], [Dloss: 0.475230] [G loss: 0.012848] time: 1:08:57.032475\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7338/12662], [Dloss: 0.320157] [G loss: 0.158822] time: 1:08:58.270165\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7339/12662], [Dloss: 0.029368] [G loss: 0.257800] time: 1:08:59.486911\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7340/12662], [Dloss: 0.486257] [G loss: 0.285528] time: 1:09:00.672740\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7341/12662], [Dloss: 0.281686] [G loss: 0.616777] time: 1:09:01.939352\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7342/12662], [Dloss: 0.126037] [G loss: 0.213923] time: 1:09:03.126178\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7343/12662], [Dloss: 0.139355] [G loss: 0.186765] time: 1:09:04.363868\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7344/12662], [Dloss: 0.399999] [G loss: -0.003429] time: 1:09:05.599563\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7345/12662], [Dloss: 0.352205] [G loss: -0.311852] time: 1:09:06.833264\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7346/12662], [Dloss: 0.814589] [G loss: -0.057980] time: 1:09:08.066965\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7347/12662], [Dloss: 0.494800] [G loss: -0.534403] time: 1:09:09.313631\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7348/12662], [Dloss: 0.365264] [G loss: -0.149035] time: 1:09:10.545336\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7349/12662], [Dloss: 0.319236] [G loss: -0.287572] time: 1:09:11.720195\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7350/12662], [Dloss: 0.545264] [G loss: -0.324943] time: 1:09:12.901037\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7351/12662], [Dloss: 0.231289] [G loss: 0.081720] time: 1:09:14.127748\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7352/12662], [Dloss: 0.415055] [G loss: -0.260860] time: 1:09:15.375411\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7353/12662], [Dloss: 0.261625] [G loss: -0.026309] time: 1:09:16.594657\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7354/12662], [Dloss: 0.468671] [G loss: 0.486023] time: 1:09:17.801429\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7355/12662], [Dloss: 0.759650] [G loss: 0.130600] time: 1:09:18.990250\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7356/12662], [Dloss: 0.092995] [G loss: 0.154114] time: 1:09:20.210986\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7357/12662], [Dloss: 0.220782] [G loss: 0.040748] time: 1:09:21.401801\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7358/12662], [Dloss: 0.102851] [G loss: 0.433420] time: 1:09:22.608573\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7359/12662], [Dloss: 0.771593] [G loss: -0.045166] time: 1:09:23.842274\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7360/12662], [Dloss: 0.107337] [G loss: 0.405187] time: 1:09:25.007159\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7361/12662], [Dloss: 0.242301] [G loss: 0.234502] time: 1:09:26.221910\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7362/12662], [Dloss: 0.373946] [G loss: 0.286630] time: 1:09:27.405744\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7363/12662], [Dloss: -0.159820] [G loss: 0.266768] time: 1:09:28.659391\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7364/12662], [Dloss: 0.506367] [G loss: -0.015434] time: 1:09:29.937972\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7365/12662], [Dloss: 0.447123] [G loss: 0.037016] time: 1:09:31.112830\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7366/12662], [Dloss: -0.016356] [G loss: 0.266742] time: 1:09:32.250787\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7367/12662], [Dloss: 0.185281] [G loss: 0.095879] time: 1:09:33.436615\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7368/12662], [Dloss: 0.175249] [G loss: 0.361413] time: 1:09:34.724172\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7369/12662], [Dloss: 0.161445] [G loss: 0.048669] time: 1:09:35.951888\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7370/12662], [Dloss: 0.458090] [G loss: 0.328368] time: 1:09:37.196560\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7371/12662], [Dloss: 0.261428] [G loss: 0.195686] time: 1:09:38.386379\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7372/12662], [Dloss: 0.138776] [G loss: -0.485215] time: 1:09:39.672444\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7373/12662], [Dloss: 0.487957] [G loss: -0.612939] time: 1:09:40.948032\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7374/12662], [Dloss: 0.190470] [G loss: -0.322912] time: 1:09:42.152316\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7375/12662], [Dloss: 0.132675] [G loss: -0.472145] time: 1:09:43.392024\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7376/12662], [Dloss: 0.299262] [G loss: 0.150221] time: 1:09:44.608812\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7377/12662], [Dloss: 0.018527] [G loss: 0.083030] time: 1:09:45.910856\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7378/12662], [Dloss: 0.353302] [G loss: 0.011071] time: 1:09:47.198494\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7379/12662], [Dloss: 0.540694] [G loss: 0.454427] time: 1:09:48.456635\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7380/12662], [Dloss: 0.158538] [G loss: 0.665279] time: 1:09:49.681360\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7381/12662], [Dloss: 0.034157] [G loss: 0.609114] time: 1:09:50.896702\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7382/12662], [Dloss: 0.411455] [G loss: 0.569382] time: 1:09:52.181266\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7383/12662], [Dloss: 0.424417] [G loss: 0.103986] time: 1:09:53.497253\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7384/12662], [Dloss: 0.323489] [G loss: 0.245794] time: 1:09:54.805754\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7385/12662], [Dloss: 0.349221] [G loss: 0.327057] time: 1:09:56.062393\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7386/12662], [Dloss: 0.287203] [G loss: 0.272149] time: 1:09:57.344963\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7387/12662], [Dloss: -0.077448] [G loss: 0.052519] time: 1:09:58.591629\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7388/12662], [Dloss: 0.651350] [G loss: 0.098196] time: 1:09:59.836300\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7389/12662], [Dloss: 0.220562] [G loss: 0.180957] time: 1:10:01.023127\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7390/12662], [Dloss: 0.243410] [G loss: 0.099206] time: 1:10:02.283755\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7391/12662], [Dloss: 0.343112] [G loss: -0.119538] time: 1:10:03.561338\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7392/12662], [Dloss: 0.391769] [G loss: 0.049730] time: 1:10:04.848897\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7393/12662], [Dloss: 0.351566] [G loss: 0.162121] time: 1:10:06.130972\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7394/12662], [Dloss: 0.393359] [G loss: 0.399048] time: 1:10:07.394593\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7395/12662], [Dloss: 0.770561] [G loss: 0.559083] time: 1:10:08.571446\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7396/12662], [Dloss: 0.368079] [G loss: 0.349832] time: 1:10:09.800160\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7397/12662], [Dloss: 0.343256] [G loss: 0.282174] time: 1:10:11.029871\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7398/12662], [Dloss: 0.149204] [G loss: 0.054734] time: 1:10:12.232654\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7399/12662], [Dloss: -0.088112] [G loss: -0.162566] time: 1:10:13.456382\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7400/12662], [Dloss: 0.283216] [G loss: -0.022190] time: 1:10:14.692077\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7401/12662], [Dloss: 0.246934] [G loss: -0.381941] time: 1:10:15.893863\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7402/12662], [Dloss: 0.214644] [G loss: -0.665545] time: 1:10:17.173441\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7403/12662], [Dloss: 0.034864] [G loss: -0.115780] time: 1:10:18.349296\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7404/12662], [Dloss: 0.583766] [G loss: -0.069516] time: 1:10:19.548090\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7405/12662], [Dloss: 0.364943] [G loss: 0.280612] time: 1:10:20.718959\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7406/12662], [Dloss: -0.037186] [G loss: 0.359571] time: 1:10:21.876862\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7407/12662], [Dloss: 0.020446] [G loss: 0.592336] time: 1:10:23.088622\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7408/12662], [Dloss: 0.203827] [G loss: 0.464372] time: 1:10:24.290407\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7409/12662], [Dloss: 0.407701] [G loss: 0.352142] time: 1:10:25.546049\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7410/12662], [Dloss: 0.363482] [G loss: 0.078357] time: 1:10:26.740854\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7411/12662], [Dloss: 0.141504] [G loss: 0.507489] time: 1:10:27.881803\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7412/12662], [Dloss: 0.194173] [G loss: 0.016084] time: 1:10:29.104533\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7413/12662], [Dloss: 0.142837] [G loss: 0.270228] time: 1:10:30.274404\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7414/12662], [Dloss: 0.018193] [G loss: 0.339976] time: 1:10:31.530046\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7415/12662], [Dloss: 0.835775] [G loss: 0.026543] time: 1:10:32.717869\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7416/12662], [Dloss: 0.274334] [G loss: 0.243853] time: 1:10:33.814936\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7417/12662], [Dloss: -0.016570] [G loss: -0.037452] time: 1:10:35.032679\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7418/12662], [Dloss: -0.152180] [G loss: 0.132035] time: 1:10:36.183601\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7419/12662], [Dloss: 0.323411] [G loss: -0.116142] time: 1:10:37.335521\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7420/12662], [Dloss: 0.515036] [G loss: -0.027930] time: 1:10:38.545285\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7421/12662], [Dloss: 0.167404] [G loss: 0.575883] time: 1:10:39.830847\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7422/12662], [Dloss: 0.300160] [G loss: 0.450359] time: 1:10:41.026655\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7423/12662], [Dloss: 0.855160] [G loss: 0.542080] time: 1:10:42.259358\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7424/12662], [Dloss: 0.599448] [G loss: 0.666708] time: 1:10:43.397315\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7425/12662], [Dloss: 0.577202] [G loss: 0.784226] time: 1:10:44.582146\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7426/12662], [Dloss: 0.189592] [G loss: 0.584820] time: 1:10:45.787922\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7427/12662], [Dloss: 0.636325] [G loss: 0.207206] time: 1:10:47.013644\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7428/12662], [Dloss: 0.211517] [G loss: 0.237088] time: 1:10:48.208449\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7429/12662], [Dloss: 0.114195] [G loss: -0.090051] time: 1:10:49.442654\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7430/12662], [Dloss: 0.373597] [G loss: 0.381155] time: 1:10:50.632473\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7431/12662], [Dloss: 0.562534] [G loss: 0.114874] time: 1:10:51.807330\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7432/12662], [Dloss: 0.157253] [G loss: -0.136255] time: 1:10:52.988173\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7433/12662], [Dloss: 0.290248] [G loss: -0.199926] time: 1:10:54.207911\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7434/12662], [Dloss: 0.358676] [G loss: 0.091806] time: 1:10:55.489483\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7435/12662], [Dloss: 0.431455] [G loss: -0.188739] time: 1:10:56.675312\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7436/12662], [Dloss: -0.110873] [G loss: 0.068405] time: 1:10:57.817258\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7437/12662], [Dloss: 0.047249] [G loss: 0.393971] time: 1:10:59.066916\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7438/12662], [Dloss: 0.482224] [G loss: -0.000891] time: 1:11:00.252744\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7439/12662], [Dloss: 0.534518] [G loss: 0.505668] time: 1:11:01.497416\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7440/12662], [Dloss: 0.419801] [G loss: 0.361143] time: 1:11:02.701196\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7441/12662], [Dloss: 0.263203] [G loss: 0.114622] time: 1:11:03.872065\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7442/12662], [Dloss: 0.094695] [G loss: 0.037560] time: 1:11:05.102765\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7443/12662], [Dloss: 0.411577] [G loss: 0.136670] time: 1:11:06.281613\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7444/12662], [Dloss: 0.356304] [G loss: 0.318972] time: 1:11:07.600086\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7445/12662], [Dloss: 0.068614] [G loss: 0.263384] time: 1:11:08.833787\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7446/12662], [Dloss: 0.306867] [G loss: -0.026931] time: 1:11:10.018618\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7447/12662], [Dloss: 0.296528] [G loss: -0.488256] time: 1:11:11.163556\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7448/12662], [Dloss: 0.484978] [G loss: -0.063652] time: 1:11:12.324452\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7449/12662], [Dloss: 0.299082] [G loss: -0.179706] time: 1:11:13.545187\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7450/12662], [Dloss: 0.225928] [G loss: 0.012158] time: 1:11:14.736002\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7451/12662], [Dloss: 0.219272] [G loss: -0.040662] time: 1:11:15.921831\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7452/12662], [Dloss: 0.338560] [G loss: 0.234655] time: 1:11:17.125612\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7453/12662], [Dloss: 0.251299] [G loss: 0.318263] time: 1:11:18.333382\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7454/12662], [Dloss: 0.337113] [G loss: 0.175130] time: 1:11:19.533173\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7455/12662], [Dloss: 0.425007] [G loss: 0.215857] time: 1:11:20.725983\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7456/12662], [Dloss: 0.442903] [G loss: 0.092062] time: 1:11:21.901838\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7457/12662], [Dloss: 0.250157] [G loss: 0.159075] time: 1:11:23.097640\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7458/12662], [Dloss: -0.156429] [G loss: 0.142702] time: 1:11:24.353282\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7459/12662], [Dloss: 0.180002] [G loss: 0.361265] time: 1:11:25.553074\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7460/12662], [Dloss: 0.223195] [G loss: 0.114084] time: 1:11:26.780790\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7461/12662], [Dloss: -0.011949] [G loss: -0.131089] time: 1:11:27.969611\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7462/12662], [Dloss: 0.325440] [G loss: -0.099387] time: 1:11:29.198325\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7463/12662], [Dloss: 0.434809] [G loss: -0.061568] time: 1:11:30.375178\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7464/12662], [Dloss: 0.233980] [G loss: -0.381444] time: 1:11:31.570980\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7465/12662], [Dloss: 0.197583] [G loss: -0.581300] time: 1:11:32.776755\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7466/12662], [Dloss: 0.500804] [G loss: -0.269224] time: 1:11:33.936653\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7467/12662], [Dloss: 0.003553] [G loss: 0.138915] time: 1:11:35.128466\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7468/12662], [Dloss: 0.567274] [G loss: 0.221526] time: 1:11:36.351195\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7469/12662], [Dloss: 0.102145] [G loss: 0.535656] time: 1:11:37.551984\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7470/12662], [Dloss: 0.454450] [G loss: 0.138933] time: 1:11:38.803637\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7471/12662], [Dloss: 0.157417] [G loss: 0.532665] time: 1:11:40.021380\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7472/12662], [Dloss: 0.035885] [G loss: 0.622042] time: 1:11:41.212195\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7473/12662], [Dloss: 0.451822] [G loss: 0.247055] time: 1:11:42.402013\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7474/12662], [Dloss: 0.294178] [G loss: 0.263715] time: 1:11:43.606298\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7475/12662], [Dloss: 0.055939] [G loss: 0.226524] time: 1:11:44.791129\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7476/12662], [Dloss: 0.232224] [G loss: -0.066597] time: 1:11:45.948035\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7477/12662], [Dloss: 0.770990] [G loss: 0.035093] time: 1:11:47.101949\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7478/12662], [Dloss: 0.323132] [G loss: 0.022245] time: 1:11:48.304732\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7479/12662], [Dloss: -0.056630] [G loss: -0.053761] time: 1:11:49.552396\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7480/12662], [Dloss: 0.146643] [G loss: -0.235622] time: 1:11:50.708305\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7481/12662], [Dloss: 0.382586] [G loss: -0.598635] time: 1:11:51.937019\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7482/12662], [Dloss: 0.539778] [G loss: -0.797596] time: 1:11:53.121850\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7483/12662], [Dloss: 0.685279] [G loss: -0.143258] time: 1:11:54.317652\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7484/12662], [Dloss: 0.505186] [G loss: 0.307851] time: 1:11:55.512456\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7485/12662], [Dloss: 0.289620] [G loss: 0.557317] time: 1:11:56.709256\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7486/12662], [Dloss: 0.360271] [G loss: 0.446810] time: 1:11:57.936972\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7487/12662], [Dloss: 0.438848] [G loss: 0.258144] time: 1:11:59.153719\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7488/12662], [Dloss: 0.353264] [G loss: 0.649088] time: 1:12:00.426315\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7489/12662], [Dloss: 0.506179] [G loss: 0.425184] time: 1:12:01.663008\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7490/12662], [Dloss: 0.465378] [G loss: 0.676965] time: 1:12:02.800964\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7491/12662], [Dloss: 0.467110] [G loss: 0.426819] time: 1:12:04.008734\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7492/12662], [Dloss: 0.421314] [G loss: 0.510334] time: 1:12:05.216505\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7493/12662], [Dloss: 0.215425] [G loss: -0.022653] time: 1:12:06.422280\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7494/12662], [Dloss: 0.373724] [G loss: -0.285784] time: 1:12:07.574199\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7495/12662], [Dloss: 0.206229] [G loss: 0.305675] time: 1:12:08.763020\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7496/12662], [Dloss: 0.531212] [G loss: 0.014444] time: 1:12:09.980764\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7497/12662], [Dloss: 0.644656] [G loss: -0.121189] time: 1:12:11.225434\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7498/12662], [Dloss: 0.090808] [G loss: 0.153009] time: 1:12:12.413258\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7499/12662], [Dloss: 0.039902] [G loss: 0.233771] time: 1:12:13.607065\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7500/12662], [Dloss: 0.075057] [G loss: 0.435148] time: 1:12:14.776936\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7500/12662], [Dloss: 0.075057] [G loss: 0.435148] time: 1:12:14.776936\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7501/12662], [Dloss: 0.414067] [G loss: 0.344519] time: 1:12:16.910244\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7502/12662], [Dloss: 0.348986] [G loss: 0.638225] time: 1:12:18.151943\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7503/12662], [Dloss: 0.332238] [G loss: 0.743410] time: 1:12:19.412088\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7504/12662], [Dloss: 0.489493] [G loss: 0.367516] time: 1:12:20.629348\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7505/12662], [Dloss: 0.401401] [G loss: 0.298928] time: 1:12:21.877041\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7506/12662], [Dloss: 0.194277] [G loss: 0.321729] time: 1:12:23.051417\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7507/12662], [Dloss: 0.095117] [G loss: 0.157167] time: 1:12:24.308056\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7508/12662], [Dloss: -0.053894] [G loss: -0.334680] time: 1:12:25.555720\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7509/12662], [Dloss: 0.495140] [G loss: -0.265529] time: 1:12:26.686695\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7510/12662], [Dloss: 0.396791] [G loss: -0.654670] time: 1:12:27.896459\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7511/12662], [Dloss: 0.368129] [G loss: -0.318105] time: 1:12:29.104229\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7512/12662], [Dloss: 0.800330] [G loss: -0.203644] time: 1:12:30.258143\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7513/12662], [Dloss: 0.219450] [G loss: -0.247477] time: 1:12:31.528252\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7514/12662], [Dloss: 0.326102] [G loss: -0.004001] time: 1:12:32.687656\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7515/12662], [Dloss: 0.471012] [G loss: 0.194225] time: 1:12:33.901410\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7516/12662], [Dloss: 0.476176] [G loss: 0.196341] time: 1:12:35.137608\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7517/12662], [Dloss: 0.356244] [G loss: 0.246264] time: 1:12:36.313463\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7518/12662], [Dloss: 0.191399] [G loss: 0.395603] time: 1:12:37.572097\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7519/12662], [Dloss: 0.380428] [G loss: 0.517733] time: 1:12:38.781862\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7520/12662], [Dloss: 0.405721] [G loss: 0.428917] time: 1:12:39.978661\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7521/12662], [Dloss: 0.390636] [G loss: 0.226304] time: 1:12:41.131578\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7522/12662], [Dloss: 0.343739] [G loss: 0.412387] time: 1:12:42.345332\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7523/12662], [Dloss: -0.013370] [G loss: 0.359888] time: 1:12:43.551107\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7524/12662], [Dloss: 0.421697] [G loss: -0.120857] time: 1:12:44.727960\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7525/12662], [Dloss: 0.224323] [G loss: 0.260871] time: 1:12:45.892845\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7526/12662], [Dloss: 0.067564] [G loss: 0.358670] time: 1:12:47.127049\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7527/12662], [Dloss: 0.545958] [G loss: -0.142761] time: 1:12:48.274979\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7528/12662], [Dloss: 0.151500] [G loss: -0.068491] time: 1:12:49.537602\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7529/12662], [Dloss: 0.142171] [G loss: 0.520485] time: 1:12:50.687527\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7530/12662], [Dloss: 0.292230] [G loss: -0.155498] time: 1:12:51.885323\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7531/12662], [Dloss: 0.486860] [G loss: 0.095333] time: 1:12:53.059184\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7532/12662], [Dloss: 0.549487] [G loss: 0.284507] time: 1:12:54.216594\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7533/12662], [Dloss: 0.398061] [G loss: 0.475027] time: 1:12:55.441319\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7534/12662], [Dloss: 0.138509] [G loss: 0.317896] time: 1:12:56.668038\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7535/12662], [Dloss: 0.102880] [G loss: 0.436036] time: 1:12:57.875808\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7536/12662], [Dloss: 0.140560] [G loss: 0.201075] time: 1:12:59.090559\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7537/12662], [Dloss: 0.409769] [G loss: -0.080735] time: 1:13:00.340217\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7538/12662], [Dloss: 0.242068] [G loss: -0.065630] time: 1:13:01.549992\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7539/12662], [Dloss: 0.571169] [G loss: -0.141459] time: 1:13:02.804144\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7540/12662], [Dloss: -0.105994] [G loss: 0.166808] time: 1:13:04.001448\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7541/12662], [Dloss: 0.575333] [G loss: -0.127661] time: 1:13:05.238141\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7542/12662], [Dloss: 0.393676] [G loss: 0.206293] time: 1:13:06.429967\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7543/12662], [Dloss: 0.130813] [G loss: -0.050446] time: 1:13:07.593866\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7544/12662], [Dloss: 0.707531] [G loss: 0.262311] time: 1:13:08.887406\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7545/12662], [Dloss: 0.523923] [G loss: 0.316357] time: 1:13:10.045310\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7546/12662], [Dloss: 0.286454] [G loss: -0.022722] time: 1:13:11.316909\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7547/12662], [Dloss: 0.220417] [G loss: 0.142298] time: 1:13:12.517204\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7548/12662], [Dloss: 0.578072] [G loss: 0.218391] time: 1:13:13.688073\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7549/12662], [Dloss: 0.470400] [G loss: 0.390283] time: 1:13:14.964658\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7550/12662], [Dloss: 0.238644] [G loss: 0.112699] time: 1:13:16.151484\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7551/12662], [Dloss: 0.519584] [G loss: 0.241724] time: 1:13:17.333324\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7552/12662], [Dloss: 0.317874] [G loss: 0.382131] time: 1:13:18.506691\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7553/12662], [Dloss: 0.016656] [G loss: 0.561607] time: 1:13:19.725431\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7554/12662], [Dloss: 0.214901] [G loss: 0.360454] time: 1:13:20.910262\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7555/12662], [Dloss: 0.358292] [G loss: 0.104362] time: 1:13:22.193337\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7556/12662], [Dloss: 0.073363] [G loss: 0.292595] time: 1:13:23.580145\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7557/12662], [Dloss: 0.384574] [G loss: 0.512336] time: 1:13:24.783432\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7558/12662], [Dloss: 0.474889] [G loss: 0.250228] time: 1:13:26.008730\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7559/12662], [Dloss: -0.051578] [G loss: 0.266404] time: 1:13:27.145689\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7560/12662], [Dloss: -0.005023] [G loss: 0.292344] time: 1:13:28.387872\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7561/12662], [Dloss: 0.338006] [G loss: 0.006071] time: 1:13:29.717316\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7562/12662], [Dloss: 0.063717] [G loss: -0.331533] time: 1:13:30.958996\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7563/12662], [Dloss: 0.367550] [G loss: 0.036671] time: 1:13:32.230595\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7564/12662], [Dloss: 0.533612] [G loss: -0.302394] time: 1:13:33.474774\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7565/12662], [Dloss: -0.000094] [G loss: -0.350591] time: 1:13:34.772303\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7566/12662], [Dloss: 0.273783] [G loss: -0.244872] time: 1:13:36.136164\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7567/12662], [Dloss: 0.076550] [G loss: 0.019639] time: 1:13:37.414756\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7568/12662], [Dloss: 0.291550] [G loss: -0.012557] time: 1:13:38.606569\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7569/12662], [Dloss: 0.440963] [G loss: 0.425822] time: 1:13:39.924045\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7570/12662], [Dloss: -0.156089] [G loss: 0.353851] time: 1:13:41.207613\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7571/12662], [Dloss: 0.259442] [G loss: 0.413442] time: 1:13:42.427855\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7572/12662], [Dloss: 0.591774] [G loss: 0.019145] time: 1:13:43.741375\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7573/12662], [Dloss: 0.164944] [G loss: -0.228009] time: 1:13:44.967604\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7574/12662], [Dloss: 0.329342] [G loss: -0.078097] time: 1:13:46.197315\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7575/12662], [Dloss: 0.461051] [G loss: -0.236906] time: 1:13:47.494855\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7576/12662], [Dloss: 0.178309] [G loss: 0.095156] time: 1:13:48.664740\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7577/12662], [Dloss: 0.329121] [G loss: 0.137585] time: 1:13:49.892482\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7578/12662], [Dloss: 0.475124] [G loss: -0.009595] time: 1:13:51.094268\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7579/12662], [Dloss: 0.269526] [G loss: -0.380110] time: 1:13:52.335453\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7580/12662], [Dloss: 0.527943] [G loss: 0.362690] time: 1:13:53.578130\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7581/12662], [Dloss: 0.664903] [G loss: -0.050038] time: 1:13:54.800860\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7582/12662], [Dloss: 0.493912] [G loss: 0.069576] time: 1:13:56.002646\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7583/12662], [Dloss: 0.317232] [G loss: 0.475857] time: 1:13:57.279232\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7584/12662], [Dloss: 0.404221] [G loss: 0.072374] time: 1:13:58.633611\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7585/12662], [Dloss: -0.057321] [G loss: 0.235934] time: 1:13:59.972031\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7586/12662], [Dloss: 0.089095] [G loss: 0.159404] time: 1:14:01.193763\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7587/12662], [Dloss: 0.006719] [G loss: 0.648344] time: 1:14:02.360642\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7588/12662], [Dloss: 0.408509] [G loss: -0.014102] time: 1:14:03.657175\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7589/12662], [Dloss: 0.550208] [G loss: 0.040707] time: 1:14:04.922790\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7590/12662], [Dloss: 0.052325] [G loss: -0.233531] time: 1:14:06.121585\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7591/12662], [Dloss: 0.490520] [G loss: -0.243895] time: 1:14:07.374234\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7592/12662], [Dloss: 0.275133] [G loss: -0.807336] time: 1:14:08.576020\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7593/12662], [Dloss: 0.317541] [G loss: -0.872304] time: 1:14:09.758857\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7594/12662], [Dloss: 0.410026] [G loss: -0.320159] time: 1:14:11.069857\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7595/12662], [Dloss: 0.497216] [G loss: 0.175049] time: 1:14:12.456654\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7596/12662], [Dloss: 0.210448] [G loss: 0.167106] time: 1:14:13.677389\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7597/12662], [Dloss: 0.586256] [G loss: 0.239657] time: 1:14:14.900107\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7598/12662], [Dloss: 0.588260] [G loss: 0.873306] time: 1:14:16.059008\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7599/12662], [Dloss: 0.024051] [G loss: 0.465791] time: 1:14:17.285727\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7600/12662], [Dloss: 0.097897] [G loss: 0.434480] time: 1:14:18.389774\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7601/12662], [Dloss: 0.130418] [G loss: 0.387935] time: 1:14:19.589566\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7602/12662], [Dloss: 0.379398] [G loss: 0.246674] time: 1:14:20.780381\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7603/12662], [Dloss: 0.323843] [G loss: 0.353856] time: 1:14:21.952247\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7604/12662], [Dloss: 0.033717] [G loss: 0.035399] time: 1:14:23.112145\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7605/12662], [Dloss: 0.378225] [G loss: -0.104149] time: 1:14:24.325899\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7606/12662], [Dloss: -0.082424] [G loss: 0.089568] time: 1:14:25.561595\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7607/12662], [Dloss: 0.188145] [G loss: -0.261946] time: 1:14:26.736452\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7608/12662], [Dloss: 0.026645] [G loss: 0.066030] time: 1:14:27.899343\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7609/12662], [Dloss: 0.318516] [G loss: -0.439918] time: 1:14:29.096100\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7610/12662], [Dloss: -0.071951] [G loss: 0.449649] time: 1:14:30.255998\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7611/12662], [Dloss: 0.362367] [G loss: 0.182167] time: 1:14:31.471747\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7612/12662], [Dloss: 0.089801] [G loss: 0.570141] time: 1:14:32.638627\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7613/12662], [Dloss: 0.115017] [G loss: 0.767038] time: 1:14:33.857367\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7614/12662], [Dloss: 0.140467] [G loss: 0.334646] time: 1:14:35.050178\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7615/12662], [Dloss: 0.358172] [G loss: 0.554079] time: 1:14:36.294860\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7616/12662], [Dloss: 0.458031] [G loss: 0.080594] time: 1:14:37.543520\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7617/12662], [Dloss: 0.414125] [G loss: 0.114287] time: 1:14:38.805146\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7618/12662], [Dloss: 0.141287] [G loss: 0.134725] time: 1:14:39.916175\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7619/12662], [Dloss: 0.456814] [G loss: -0.123527] time: 1:14:41.146817\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7620/12662], [Dloss: 0.601119] [G loss: 0.014513] time: 1:14:42.347605\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7621/12662], [Dloss: 0.789225] [G loss: -0.396050] time: 1:14:43.534431\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7622/12662], [Dloss: 0.529115] [G loss: -0.111020] time: 1:14:44.788078\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7623/12662], [Dloss: 0.437483] [G loss: -0.218392] time: 1:14:46.064665\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7624/12662], [Dloss: 0.315123] [G loss: -0.046110] time: 1:14:47.239522\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7625/12662], [Dloss: 0.542655] [G loss: -0.075670] time: 1:14:48.575948\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7626/12662], [Dloss: 0.158699] [G loss: 0.263314] time: 1:14:49.799676\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7627/12662], [Dloss: 0.126510] [G loss: 0.132949] time: 1:14:51.091221\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7628/12662], [Dloss: -0.034555] [G loss: 0.462965] time: 1:14:52.318938\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7629/12662], [Dloss: 0.009816] [G loss: 0.485403] time: 1:14:53.487813\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7630/12662], [Dloss: 0.389534] [G loss: 0.305854] time: 1:14:54.695582\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7631/12662], [Dloss: 0.658253] [G loss: 0.208140] time: 1:14:55.986646\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7632/12662], [Dloss: 0.257859] [G loss: 0.128520] time: 1:14:57.204389\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7633/12662], [Dloss: 0.420896] [G loss: 0.094920] time: 1:14:58.419141\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7634/12662], [Dloss: 0.239536] [G loss: -0.454198] time: 1:14:59.664809\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7635/12662], [Dloss: 0.236742] [G loss: -0.030873] time: 1:15:00.983298\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7636/12662], [Dloss: 0.111237] [G loss: -0.599976] time: 1:15:02.192065\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7637/12662], [Dloss: 0.295719] [G loss: -0.449880] time: 1:15:03.550432\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7638/12662], [Dloss: 0.396003] [G loss: -0.174247] time: 1:15:04.794106\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7639/12662], [Dloss: 0.462765] [G loss: -0.252084] time: 1:15:05.957995\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7640/12662], [Dloss: 0.390066] [G loss: 0.013211] time: 1:15:07.347278\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7641/12662], [Dloss: 0.005910] [G loss: 0.346996] time: 1:15:08.613891\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7642/12662], [Dloss: 0.092776] [G loss: 0.164053] time: 1:15:09.938349\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7643/12662], [Dloss: 0.664491] [G loss: -0.107181] time: 1:15:11.197980\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7644/12662], [Dloss: -0.222992] [G loss: 0.268915] time: 1:15:12.408742\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7645/12662], [Dloss: 0.044676] [G loss: 0.706157] time: 1:15:13.728719\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7646/12662], [Dloss: 0.334719] [G loss: 0.278309] time: 1:15:15.018270\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7647/12662], [Dloss: 0.078773] [G loss: 0.711248] time: 1:15:16.292861\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7648/12662], [Dloss: 0.395195] [G loss: -0.011630] time: 1:15:17.576429\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7649/12662], [Dloss: -0.139509] [G loss: -0.154757] time: 1:15:18.781206\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7650/12662], [Dloss: 0.368581] [G loss: -0.147409] time: 1:15:20.051808\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7651/12662], [Dloss: 0.704083] [G loss: -0.087692] time: 1:15:21.236640\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7652/12662], [Dloss: 0.382245] [G loss: -0.149321] time: 1:15:22.409503\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7653/12662], [Dloss: 0.323076] [G loss: -0.037175] time: 1:15:23.638217\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7654/12662], [Dloss: 0.595825] [G loss: -0.046732] time: 1:15:24.865933\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7655/12662], [Dloss: 0.501837] [G loss: 0.105583] time: 1:15:26.222345\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7656/12662], [Dloss: 0.288853] [G loss: -0.081901] time: 1:15:27.509902\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7657/12662], [Dloss: 0.478545] [G loss: -0.109423] time: 1:15:28.748589\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7658/12662], [Dloss: 0.325731] [G loss: 0.259639] time: 1:15:29.941399\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7659/12662], [Dloss: 0.398069] [G loss: 0.019429] time: 1:15:31.206016\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7660/12662], [Dloss: 0.182747] [G loss: 0.052178] time: 1:15:32.448693\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7661/12662], [Dloss: 0.034281] [G loss: 0.226066] time: 1:15:33.659455\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7662/12662], [Dloss: 0.622171] [G loss: 0.087024] time: 1:15:35.014830\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7663/12662], [Dloss: 0.431977] [G loss: -0.026152] time: 1:15:36.399128\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7664/12662], [Dloss: 0.465083] [G loss: -0.206595] time: 1:15:37.658759\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7665/12662], [Dloss: 0.446636] [G loss: 0.162550] time: 1:15:38.927367\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7666/12662], [Dloss: 0.359561] [G loss: 0.080348] time: 1:15:40.156081\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7667/12662], [Dloss: 0.178701] [G loss: -0.263108] time: 1:15:41.543370\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7668/12662], [Dloss: 0.142068] [G loss: -0.071147] time: 1:15:42.767098\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7669/12662], [Dloss: -0.077147] [G loss: 0.066424] time: 1:15:44.042699\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7670/12662], [Dloss: 0.008847] [G loss: 0.004949] time: 1:15:45.375171\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7671/12662], [Dloss: 0.276637] [G loss: -0.054075] time: 1:15:46.648282\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7672/12662], [Dloss: 0.266250] [G loss: -0.213788] time: 1:15:47.841092\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7673/12662], [Dloss: 0.311085] [G loss: -0.499491] time: 1:15:49.107717\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7674/12662], [Dloss: 0.703384] [G loss: -0.126046] time: 1:15:50.345910\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7675/12662], [Dloss: 0.282563] [G loss: -0.235239] time: 1:15:51.574130\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7676/12662], [Dloss: 0.057060] [G loss: 0.066689] time: 1:15:52.822791\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7677/12662], [Dloss: -0.099252] [G loss: 0.645725] time: 1:15:54.059484\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7678/12662], [Dloss: 0.760483] [G loss: 0.032617] time: 1:15:55.263267\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7679/12662], [Dloss: 0.237915] [G loss: 0.299402] time: 1:15:56.440620\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7680/12662], [Dloss: 0.476261] [G loss: 0.446352] time: 1:15:57.774560\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7681/12662], [Dloss: 0.416337] [G loss: 0.191366] time: 1:15:59.012250\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7682/12662], [Dloss: -0.277367] [G loss: 0.290849] time: 1:16:00.219023\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7683/12662], [Dloss: 0.233373] [G loss: 0.070583] time: 1:16:01.435769\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7684/12662], [Dloss: 0.353835] [G loss: 0.027061] time: 1:16:02.663485\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7685/12662], [Dloss: 0.537085] [G loss: -0.065111] time: 1:16:03.906162\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7686/12662], [Dloss: 0.315027] [G loss: -0.097108] time: 1:16:05.092988\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7687/12662], [Dloss: 0.546645] [G loss: -0.107198] time: 1:16:06.345638\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7688/12662], [Dloss: 0.390068] [G loss: 0.008581] time: 1:16:07.533461\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7689/12662], [Dloss: 0.291019] [G loss: 0.167746] time: 1:16:08.799076\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7690/12662], [Dloss: 0.161777] [G loss: 0.364352] time: 1:16:10.012831\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7691/12662], [Dloss: -0.169473] [G loss: 0.243678] time: 1:16:11.226585\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7692/12662], [Dloss: -0.010898] [G loss: 0.108217] time: 1:16:12.447320\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7693/12662], [Dloss: 0.454024] [G loss: -0.433629] time: 1:16:13.570317\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7694/12662], [Dloss: 0.348212] [G loss: -0.238771] time: 1:16:14.779084\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7695/12662], [Dloss: 0.319736] [G loss: -0.207348] time: 1:16:15.999819\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7696/12662], [Dloss: 0.356165] [G loss: -0.557565] time: 1:16:17.185662\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7697/12662], [Dloss: 0.540423] [G loss: -0.411296] time: 1:16:18.388465\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7698/12662], [Dloss: 0.397244] [G loss: -0.482811] time: 1:16:19.656099\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7699/12662], [Dloss: 0.303466] [G loss: 0.126553] time: 1:16:20.902765\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7700/12662], [Dloss: 0.741693] [G loss: -0.165875] time: 1:16:22.248167\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7701/12662], [Dloss: 0.010120] [G loss: 0.177126] time: 1:16:23.473410\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7702/12662], [Dloss: -0.005850] [G loss: 0.252631] time: 1:16:24.731047\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7703/12662], [Dloss: 0.242152] [G loss: 0.428739] time: 1:16:26.049520\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7704/12662], [Dloss: 0.191217] [G loss: 0.367537] time: 1:16:27.262277\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7705/12662], [Dloss: 0.270400] [G loss: 0.408648] time: 1:16:28.438637\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7706/12662], [Dloss: 0.389902] [G loss: 0.224825] time: 1:16:29.718718\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7707/12662], [Dloss: 0.286270] [G loss: 0.239111] time: 1:16:30.890584\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7708/12662], [Dloss: 0.344301] [G loss: -0.016777] time: 1:16:32.159191\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7709/12662], [Dloss: 0.318532] [G loss: 0.140388] time: 1:16:33.422321\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7710/12662], [Dloss: 0.143281] [G loss: -0.268695] time: 1:16:34.567288\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7711/12662], [Dloss: 0.341017] [G loss: -0.334770] time: 1:16:35.748161\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7712/12662], [Dloss: 0.251589] [G loss: -0.179159] time: 1:16:36.935500\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7713/12662], [Dloss: 0.348028] [G loss: 0.348607] time: 1:16:38.049027\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7714/12662], [Dloss: 0.441754] [G loss: 0.449775] time: 1:16:39.198459\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7715/12662], [Dloss: 0.440940] [G loss: 0.465339] time: 1:16:40.377339\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7716/12662], [Dloss: 0.407797] [G loss: 0.723577] time: 1:16:41.586136\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7717/12662], [Dloss: 0.010849] [G loss: 0.599338] time: 1:16:42.776468\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7718/12662], [Dloss: 0.438702] [G loss: 0.257585] time: 1:16:43.954318\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7719/12662], [Dloss: 0.174542] [G loss: 0.403771] time: 1:16:45.175053\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7720/12662], [Dloss: 0.095693] [G loss: 0.191571] time: 1:16:46.365879\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7721/12662], [Dloss: 0.291578] [G loss: -0.087021] time: 1:16:47.643462\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7722/12662], [Dloss: 0.548063] [G loss: -0.417199] time: 1:16:48.844767\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7723/12662], [Dloss: 0.321859] [G loss: -0.783738] time: 1:16:50.054039\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7724/12662], [Dloss: 0.290649] [G loss: -0.676164] time: 1:16:51.231395\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7725/12662], [Dloss: 0.332407] [G loss: -0.372103] time: 1:16:52.542887\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7726/12662], [Dloss: 0.518977] [G loss: -0.470430] time: 1:16:53.790561\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7727/12662], [Dloss: 0.471890] [G loss: -0.189006] time: 1:16:55.181348\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7728/12662], [Dloss: 0.256450] [G loss: -0.229328] time: 1:16:56.458931\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7729/12662], [Dloss: 0.436762] [G loss: -0.000837] time: 1:16:57.699624\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7730/12662], [Dloss: 0.338589] [G loss: 0.048936] time: 1:16:58.922365\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7731/12662], [Dloss: 0.183882] [G loss: 0.409681] time: 1:17:00.192978\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7732/12662], [Dloss: 0.531761] [G loss: -0.143143] time: 1:17:01.448127\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7733/12662], [Dloss: 0.274872] [G loss: 0.045815] time: 1:17:02.712697\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7734/12662], [Dloss: 0.227369] [G loss: 0.138457] time: 1:17:03.904030\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7735/12662], [Dloss: 0.120818] [G loss: 0.198020] time: 1:17:05.120785\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7736/12662], [Dloss: 0.192218] [G loss: -0.085796] time: 1:17:06.365456\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7737/12662], [Dloss: 0.158763] [G loss: -0.120743] time: 1:17:07.667482\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7738/12662], [Dloss: 0.292644] [G loss: -0.751899] time: 1:17:08.820396\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7739/12662], [Dloss: 0.330074] [G loss: -0.491593] time: 1:17:10.047116\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7740/12662], [Dloss: 0.477974] [G loss: -0.481615] time: 1:17:11.254886\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7741/12662], [Dloss: 0.374305] [G loss: -0.382776] time: 1:17:12.418773\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7742/12662], [Dloss: 0.502225] [G loss: -0.257548] time: 1:17:13.519829\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7743/12662], [Dloss: 0.407157] [G loss: -0.037862] time: 1:17:14.756521\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7744/12662], [Dloss: 0.158441] [G loss: 0.387299] time: 1:17:15.913427\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7745/12662], [Dloss: 0.408156] [G loss: 0.000271] time: 1:17:17.100265\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7746/12662], [Dloss: 0.489873] [G loss: 0.341859] time: 1:17:18.244206\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7747/12662], [Dloss: 0.283039] [G loss: -0.134331] time: 1:17:19.542733\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7748/12662], [Dloss: 0.507838] [G loss: -0.292099] time: 1:17:20.781924\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7749/12662], [Dloss: 0.420407] [G loss: 0.004482] time: 1:17:21.997673\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7750/12662], [Dloss: 0.223162] [G loss: -0.075518] time: 1:17:23.205443\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7751/12662], [Dloss: 0.505080] [G loss: 0.070636] time: 1:17:24.425181\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7752/12662], [Dloss: 0.048404] [G loss: 0.087112] time: 1:17:25.661874\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7753/12662], [Dloss: 0.414064] [G loss: 0.408112] time: 1:17:26.858180\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7754/12662], [Dloss: 0.234685] [G loss: 0.796976] time: 1:17:28.022117\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7755/12662], [Dloss: 0.405093] [G loss: 0.617840] time: 1:17:29.197972\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7756/12662], [Dloss: 0.261621] [G loss: 0.392061] time: 1:17:30.419704\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7757/12662], [Dloss: 0.571966] [G loss: 0.123244] time: 1:17:31.609028\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7758/12662], [Dloss: 0.573973] [G loss: 0.361104] time: 1:17:32.792368\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7759/12662], [Dloss: 0.603078] [G loss: 0.575498] time: 1:17:33.983686\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7760/12662], [Dloss: 0.292596] [G loss: 0.507617] time: 1:17:35.129129\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7761/12662], [Dloss: 0.104180] [G loss: 0.708121] time: 1:17:36.296511\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7762/12662], [Dloss: 0.438741] [G loss: 0.320639] time: 1:17:37.449428\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7763/12662], [Dloss: 0.030277] [G loss: 0.009961] time: 1:17:38.606334\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7764/12662], [Dloss: 0.162821] [G loss: 0.243847] time: 1:17:39.818093\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7765/12662], [Dloss: 0.331137] [G loss: 0.220031] time: 1:17:40.959043\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7766/12662], [Dloss: 0.468415] [G loss: -0.166795] time: 1:17:42.148387\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7767/12662], [Dloss: 0.565610] [G loss: -0.226428] time: 1:17:43.439943\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7768/12662], [Dloss: 0.290979] [G loss: -0.367311] time: 1:17:44.603830\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7769/12662], [Dloss: 0.329638] [G loss: -0.179693] time: 1:17:45.801133\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7770/12662], [Dloss: 0.551427] [G loss: 0.041096] time: 1:17:46.993943\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7771/12662], [Dloss: 0.282117] [G loss: 0.159160] time: 1:17:48.178775\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7772/12662], [Dloss: -0.261341] [G loss: 0.188542] time: 1:17:49.361117\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7773/12662], [Dloss: 0.377777] [G loss: 0.212600] time: 1:17:50.576371\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7774/12662], [Dloss: 0.061616] [G loss: -0.036529] time: 1:17:51.783155\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7775/12662], [Dloss: -0.276188] [G loss: 0.155781] time: 1:17:52.956019\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7776/12662], [Dloss: 0.131799] [G loss: -0.043642] time: 1:17:54.180743\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7777/12662], [Dloss: 0.404488] [G loss: -0.547313] time: 1:17:55.435388\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7778/12662], [Dloss: 0.782215] [G loss: 0.093466] time: 1:17:56.612241\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7779/12662], [Dloss: 0.667962] [G loss: 0.261847] time: 1:17:57.857909\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7780/12662], [Dloss: 0.161566] [G loss: -0.220555] time: 1:17:59.040746\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7781/12662], [Dloss: 0.412620] [G loss: -0.322335] time: 1:18:00.129834\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7782/12662], [Dloss: 0.304725] [G loss: -0.129557] time: 1:18:01.383481\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7783/12662], [Dloss: 0.363921] [G loss: 0.322803] time: 1:18:02.555347\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7784/12662], [Dloss: 0.400485] [G loss: -0.031702] time: 1:18:03.753144\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7785/12662], [Dloss: 0.163690] [G loss: -0.090648] time: 1:18:04.896087\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7786/12662], [Dloss: 0.676240] [G loss: 0.250339] time: 1:18:06.097873\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7787/12662], [Dloss: 0.580549] [G loss: 0.373863] time: 1:18:07.294672\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7788/12662], [Dloss: 0.348802] [G loss: 0.239056] time: 1:18:08.477509\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7789/12662], [Dloss: 0.225824] [G loss: 0.599318] time: 1:18:09.646383\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7790/12662], [Dloss: 0.572454] [G loss: 0.078342] time: 1:18:10.874099\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7791/12662], [Dloss: 0.557699] [G loss: 0.194938] time: 1:18:12.016045\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7792/12662], [Dloss: 0.273747] [G loss: 0.162777] time: 1:18:13.193895\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7793/12662], [Dloss: 0.509265] [G loss: -0.100150] time: 1:18:14.377730\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7794/12662], [Dloss: 0.122377] [G loss: -0.161705] time: 1:18:15.572534\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7795/12662], [Dloss: 0.144356] [G loss: -0.491542] time: 1:18:16.762352\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7796/12662], [Dloss: 0.155464] [G loss: -0.478019] time: 1:18:17.897317\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7797/12662], [Dloss: 0.752783] [G loss: -0.260055] time: 1:18:19.134987\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7798/12662], [Dloss: 0.312406] [G loss: -0.176623] time: 1:18:20.246016\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7799/12662], [Dloss: 0.557680] [G loss: 0.056662] time: 1:18:21.464757\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7800/12662], [Dloss: 0.329720] [G loss: 0.481006] time: 1:18:22.677513\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7801/12662], [Dloss: 0.133849] [G loss: 0.584066] time: 1:18:23.863342\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7802/12662], [Dloss: 0.194704] [G loss: 0.595484] time: 1:18:24.998307\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7803/12662], [Dloss: 0.270322] [G loss: 0.121465] time: 1:18:26.279879\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7804/12662], [Dloss: 0.494813] [G loss: 0.408688] time: 1:18:27.473687\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7805/12662], [Dloss: 0.465503] [G loss: -0.060138] time: 1:18:28.644555\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7806/12662], [Dloss: 0.247207] [G loss: 0.058562] time: 1:18:29.832379\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7807/12662], [Dloss: 0.146086] [G loss: -0.331453] time: 1:18:31.021199\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7808/12662], [Dloss: 0.558753] [G loss: -0.532770] time: 1:18:32.186084\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7809/12662], [Dloss: 0.158038] [G loss: -0.190816] time: 1:18:33.365929\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7810/12662], [Dloss: 0.372410] [G loss: -0.202001] time: 1:18:34.553752\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7811/12662], [Dloss: 0.333961] [G loss: -0.199715] time: 1:18:35.765511\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7812/12662], [Dloss: 0.276143] [G loss: -0.212222] time: 1:18:36.952337\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7813/12662], [Dloss: 0.497164] [G loss: 0.425786] time: 1:18:38.181051\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7814/12662], [Dloss: 0.055728] [G loss: 1.032276] time: 1:18:39.368874\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7815/12662], [Dloss: 0.076535] [G loss: 0.734617] time: 1:18:40.581631\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7816/12662], [Dloss: 0.200757] [G loss: 1.130644] time: 1:18:41.766463\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7817/12662], [Dloss: 0.192960] [G loss: 0.764399] time: 1:18:42.876495\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7818/12662], [Dloss: 0.299133] [G loss: 0.475895] time: 1:18:44.101219\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7819/12662], [Dloss: 0.198054] [G loss: 0.245358] time: 1:18:45.239175\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7820/12662], [Dloss: 0.169577] [G loss: 0.296491] time: 1:18:46.442956\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7821/12662], [Dloss: 0.516001] [G loss: 0.378259] time: 1:18:47.638758\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7822/12662], [Dloss: 0.595493] [G loss: 0.068698] time: 1:18:48.809626\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7823/12662], [Dloss: 0.177686] [G loss: 0.203115] time: 1:18:50.052303\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7824/12662], [Dloss: 0.293972] [G loss: 0.216827] time: 1:18:51.248105\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7825/12662], [Dloss: 0.576253] [G loss: -0.324796] time: 1:18:52.473827\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7826/12662], [Dloss: 0.293257] [G loss: -0.077108] time: 1:18:53.678605\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7827/12662], [Dloss: -0.175806] [G loss: -0.143953] time: 1:18:54.843490\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7828/12662], [Dloss: 0.204807] [G loss: 0.000430] time: 1:18:56.063228\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7829/12662], [Dloss: 0.356487] [G loss: 0.113639] time: 1:18:57.375718\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7830/12662], [Dloss: 0.178662] [G loss: 0.334380] time: 1:18:58.573514\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7831/12662], [Dloss: 0.359812] [G loss: 0.316099] time: 1:18:59.772309\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7832/12662], [Dloss: 0.193564] [G loss: 0.265443] time: 1:19:00.993044\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7833/12662], [Dloss: 0.319559] [G loss: 0.026820] time: 1:19:02.231731\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7834/12662], [Dloss: 0.308800] [G loss: 0.308147] time: 1:19:03.424048\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7835/12662], [Dloss: 0.593498] [G loss: 0.040997] time: 1:19:04.628826\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7836/12662], [Dloss: 0.353068] [G loss: -0.246049] time: 1:19:05.875492\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7837/12662], [Dloss: 0.524994] [G loss: 0.387555] time: 1:19:07.118169\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7838/12662], [Dloss: 0.491781] [G loss: -0.245880] time: 1:19:08.450606\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7839/12662], [Dloss: 0.306038] [G loss: 0.003809] time: 1:19:09.693786\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7840/12662], [Dloss: 0.229087] [G loss: -0.092040] time: 1:19:10.939455\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7841/12662], [Dloss: -0.137524] [G loss: -0.079409] time: 1:19:12.248953\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7842/12662], [Dloss: 0.141004] [G loss: 0.097652] time: 1:19:13.506590\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7843/12662], [Dloss: 0.063941] [G loss: 0.619552] time: 1:19:14.785674\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7844/12662], [Dloss: 0.505234] [G loss: -0.051216] time: 1:19:16.048811\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7845/12662], [Dloss: 0.220626] [G loss: 0.316290] time: 1:19:17.353323\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7846/12662], [Dloss: 0.209902] [G loss: -0.152525] time: 1:19:18.639389\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7847/12662], [Dloss: 0.201155] [G loss: 0.510715] time: 1:19:19.996758\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7848/12662], [Dloss: 0.369679] [G loss: 0.393189] time: 1:19:21.238438\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7849/12662], [Dloss: 0.113432] [G loss: 0.194805] time: 1:19:22.375397\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7850/12662], [Dloss: -0.382322] [G loss: 0.574870] time: 1:19:23.609097\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7851/12662], [Dloss: 0.213164] [G loss: 0.268614] time: 1:19:24.902145\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7852/12662], [Dloss: -0.005711] [G loss: 0.334192] time: 1:19:26.187707\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7853/12662], [Dloss: 0.519371] [G loss: 0.126859] time: 1:19:27.441354\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7854/12662], [Dloss: 0.275574] [G loss: 0.333350] time: 1:19:28.647130\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7855/12662], [Dloss: 0.383676] [G loss: 0.058560] time: 1:19:29.925710\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7856/12662], [Dloss: 0.477331] [G loss: 0.227545] time: 1:19:31.213267\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7857/12662], [Dloss: 0.453920] [G loss: -0.038479] time: 1:19:32.369176\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7858/12662], [Dloss: 0.444642] [G loss: 0.307493] time: 1:19:33.568967\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7859/12662], [Dloss: 0.380547] [G loss: 0.280321] time: 1:19:34.737841\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7860/12662], [Dloss: 0.238787] [G loss: 0.385304] time: 1:19:35.928656\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7861/12662], [Dloss: -0.093541] [G loss: 0.296167] time: 1:19:37.175322\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7862/12662], [Dloss: 0.536089] [G loss: -0.137991] time: 1:19:38.371124\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7863/12662], [Dloss: 0.144559] [G loss: 0.523071] time: 1:19:39.550969\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7864/12662], [Dloss: 0.178153] [G loss: 0.481421] time: 1:19:40.746771\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7865/12662], [Dloss: 0.498634] [G loss: 0.327845] time: 1:19:41.904674\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7866/12662], [Dloss: -0.055138] [G loss: 0.191528] time: 1:19:43.086513\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7867/12662], [Dloss: 0.244875] [G loss: 0.070074] time: 1:19:44.202529\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7868/12662], [Dloss: 0.306568] [G loss: 0.331737] time: 1:19:45.421270\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7869/12662], [Dloss: 0.238102] [G loss: 0.019070] time: 1:19:46.555237\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7870/12662], [Dloss: -0.013837] [G loss: 0.245643] time: 1:19:47.723114\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7871/12662], [Dloss: 0.379989] [G loss: 0.210160] time: 1:19:48.944847\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7872/12662], [Dloss: 0.228019] [G loss: 0.121109] time: 1:19:50.216446\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7873/12662], [Dloss: 0.156827] [G loss: -0.053735] time: 1:19:51.570342\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7874/12662], [Dloss: 0.247636] [G loss: -0.086379] time: 1:19:52.820998\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7875/12662], [Dloss: 0.378930] [G loss: -0.239458] time: 1:19:54.112544\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7876/12662], [Dloss: 0.002221] [G loss: -0.030815] time: 1:19:55.320314\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7877/12662], [Dloss: -0.115892] [G loss: 0.043730] time: 1:19:56.485198\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7878/12662], [Dloss: -0.140413] [G loss: -0.001664] time: 1:19:57.770761\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7879/12662], [Dloss: 0.141292] [G loss: 0.199814] time: 1:19:58.999978\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7880/12662], [Dloss: -0.102138] [G loss: -0.002458] time: 1:20:00.245647\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7881/12662], [Dloss: 0.506429] [G loss: 0.210775] time: 1:20:01.502286\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7882/12662], [Dloss: 0.373837] [G loss: -0.141647] time: 1:20:02.749962\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7883/12662], [Dloss: 0.204748] [G loss: 0.345975] time: 1:20:03.984661\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7884/12662], [Dloss: 0.343129] [G loss: 0.401323] time: 1:20:05.129598\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7885/12662], [Dloss: 0.315891] [G loss: 0.435739] time: 1:20:06.496942\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7886/12662], [Dloss: 0.301775] [G loss: 0.168040] time: 1:20:07.874258\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7887/12662], [Dloss: -0.061798] [G loss: -0.058953] time: 1:20:09.110951\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7888/12662], [Dloss: 0.306777] [G loss: 0.030869] time: 1:20:10.291793\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7889/12662], [Dloss: 0.322911] [G loss: -0.411294] time: 1:20:11.574363\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7890/12662], [Dloss: 0.149506] [G loss: 0.185469] time: 1:20:12.865909\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7891/12662], [Dloss: 0.086217] [G loss: 0.383091] time: 1:20:14.136511\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7892/12662], [Dloss: 0.279783] [G loss: 0.302863] time: 1:20:15.428057\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7893/12662], [Dloss: 0.324071] [G loss: -0.141186] time: 1:20:16.744536\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7894/12662], [Dloss: -0.182854] [G loss: -0.005616] time: 1:20:18.044061\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7895/12662], [Dloss: 0.431612] [G loss: 0.010590] time: 1:20:19.289730\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7896/12662], [Dloss: 0.193452] [G loss: 0.568646] time: 1:20:20.459601\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7897/12662], [Dloss: 0.005476] [G loss: 0.283147] time: 1:20:21.756133\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7898/12662], [Dloss: 0.065601] [G loss: 0.096761] time: 1:20:22.881124\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7899/12662], [Dloss: 0.087664] [G loss: 0.266162] time: 1:20:24.168189\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7900/12662], [Dloss: 0.473304] [G loss: 0.142129] time: 1:20:25.379948\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7901/12662], [Dloss: 0.283723] [G loss: -0.078079] time: 1:20:26.545830\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7902/12662], [Dloss: 0.144083] [G loss: 0.252602] time: 1:20:27.782522\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7903/12662], [Dloss: 0.667253] [G loss: -0.137238] time: 1:20:29.069082\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7904/12662], [Dloss: 0.260286] [G loss: -0.135785] time: 1:20:30.319737\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7905/12662], [Dloss: 0.022988] [G loss: 0.063417] time: 1:20:31.514542\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7906/12662], [Dloss: 0.286420] [G loss: 0.344021] time: 1:20:32.724306\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7907/12662], [Dloss: 0.140926] [G loss: -0.005286] time: 1:20:33.976967\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7908/12662], [Dloss: 0.015898] [G loss: -0.137752] time: 1:20:35.193713\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7909/12662], [Dloss: 0.335884] [G loss: -0.052493] time: 1:20:36.448358\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7910/12662], [Dloss: 0.429303] [G loss: 0.530387] time: 1:20:37.674080\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7911/12662], [Dloss: 0.197793] [G loss: 0.096153] time: 1:20:38.924736\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7912/12662], [Dloss: 0.121314] [G loss: 0.321178] time: 1:20:40.132505\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7913/12662], [Dloss: 0.053721] [G loss: 0.619511] time: 1:20:41.338281\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7914/12662], [Dloss: 0.127759] [G loss: 0.090408] time: 1:20:42.553032\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7915/12662], [Dloss: 0.335181] [G loss: 0.030190] time: 1:20:43.707944\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7916/12662], [Dloss: 0.406623] [G loss: 0.161976] time: 1:20:44.903745\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7917/12662], [Dloss: 0.338059] [G loss: 0.282262] time: 1:20:46.138948\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7918/12662], [Dloss: 0.424432] [G loss: -0.033010] time: 1:20:47.380628\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7919/12662], [Dloss: 0.181053] [G loss: 0.207878] time: 1:20:48.596376\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7920/12662], [Dloss: 0.420080] [G loss: -0.266174] time: 1:20:49.768242\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7921/12662], [Dloss: 0.240163] [G loss: 0.014213] time: 1:20:50.922156\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7922/12662], [Dloss: -0.213517] [G loss: 0.071654] time: 1:20:52.103995\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7923/12662], [Dloss: 0.675476] [G loss: 0.078483] time: 1:20:53.306779\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7924/12662], [Dloss: 0.437603] [G loss: 0.268836] time: 1:20:54.422794\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7925/12662], [Dloss: 0.603010] [G loss: 0.235001] time: 1:20:55.639540\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7926/12662], [Dloss: 0.380924] [G loss: 0.081807] time: 1:20:56.856286\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7927/12662], [Dloss: 0.327441] [G loss: 0.489192] time: 1:20:58.125891\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7928/12662], [Dloss: 0.252963] [G loss: 0.451950] time: 1:20:59.495229\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7929/12662], [Dloss: 0.371013] [G loss: 0.379240] time: 1:21:00.688038\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7930/12662], [Dloss: 0.443496] [G loss: 0.418243] time: 1:21:01.846939\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7931/12662], [Dloss: 0.204794] [G loss: 0.087604] time: 1:21:03.068673\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7932/12662], [Dloss: 0.669282] [G loss: -0.157772] time: 1:21:04.308861\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7933/12662], [Dloss: 0.347887] [G loss: 0.310891] time: 1:21:05.431858\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7934/12662], [Dloss: 0.486660] [G loss: 0.453414] time: 1:21:06.627660\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7935/12662], [Dloss: -0.063783] [G loss: 0.253935] time: 1:21:07.800523\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7936/12662], [Dloss: 0.207109] [G loss: 0.333436] time: 1:21:09.003307\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7937/12662], [Dloss: 0.417127] [G loss: 0.181795] time: 1:21:10.257951\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7938/12662], [Dloss: 0.555678] [G loss: 0.201738] time: 1:21:11.463727\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7939/12662], [Dloss: -0.035145] [G loss: -0.374033] time: 1:21:12.659529\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7940/12662], [Dloss: 0.416021] [G loss: -0.606418] time: 1:21:13.886248\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7941/12662], [Dloss: 0.492399] [G loss: -0.078981] time: 1:21:15.126930\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7942/12662], [Dloss: 0.517868] [G loss: -0.114879] time: 1:21:16.355644\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7943/12662], [Dloss: 0.241866] [G loss: 0.086849] time: 1:21:17.612283\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7944/12662], [Dloss: 0.389568] [G loss: -0.236996] time: 1:21:18.838005\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 7945/12662], [Dloss: 0.200157] [G loss: 0.069263] time: 1:21:20.037797\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7946/12662], [Dloss: 0.086801] [G loss: 0.190459] time: 1:21:21.245567\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7947/12662], [Dloss: 0.104855] [G loss: 0.422650] time: 1:21:22.540104\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7948/12662], [Dloss: 0.219989] [G loss: 0.519397] time: 1:21:23.679059\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7949/12662], [Dloss: 0.377626] [G loss: 0.201647] time: 1:21:24.905778\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7950/12662], [Dloss: 0.334798] [G loss: 0.088907] time: 1:21:26.022791\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7951/12662], [Dloss: 0.020254] [G loss: 0.248746] time: 1:21:27.221585\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7952/12662], [Dloss: 0.657813] [G loss: -0.205799] time: 1:21:28.429354\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7953/12662], [Dloss: 0.260258] [G loss: 0.008379] time: 1:21:29.568309\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7954/12662], [Dloss: 0.476975] [G loss: -0.153929] time: 1:21:30.808991\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7955/12662], [Dloss: 0.241865] [G loss: 0.051100] time: 1:21:31.934979\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7956/12662], [Dloss: 0.397761] [G loss: -0.250262] time: 1:21:33.067949\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7957/12662], [Dloss: 0.372770] [G loss: 0.052595] time: 1:21:34.251784\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7958/12662], [Dloss: 0.294207] [G loss: 0.094831] time: 1:21:35.447585\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7959/12662], [Dloss: 0.625650] [G loss: -0.417210] time: 1:21:36.616459\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7960/12662], [Dloss: 0.032965] [G loss: -0.350823] time: 1:21:37.820240\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7961/12662], [Dloss: 0.555881] [G loss: 0.107837] time: 1:21:38.956202\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7962/12662], [Dloss: 0.210719] [G loss: 0.111790] time: 1:21:40.161977\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7963/12662], [Dloss: -0.033319] [G loss: -0.162893] time: 1:21:41.352802\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7964/12662], [Dloss: 0.009015] [G loss: 0.255964] time: 1:21:42.581536\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7965/12662], [Dloss: 0.003091] [G loss: 0.038118] time: 1:21:43.770422\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7966/12662], [Dloss: 0.128611] [G loss: -0.005209] time: 1:21:45.030064\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7967/12662], [Dloss: 0.581789] [G loss: 0.023395] time: 1:21:46.225373\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7968/12662], [Dloss: 0.572635] [G loss: -0.008316] time: 1:21:47.454087\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7969/12662], [Dloss: 0.495389] [G loss: 0.221724] time: 1:21:48.623970\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7970/12662], [Dloss: 0.360058] [G loss: -0.188364] time: 1:21:49.834732\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7971/12662], [Dloss: 0.704636] [G loss: 0.039824] time: 1:21:51.017569\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7972/12662], [Dloss: 0.038772] [G loss: 0.072563] time: 1:21:52.157026\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7973/12662], [Dloss: 0.180010] [G loss: 0.421055] time: 1:21:53.298972\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7974/12662], [Dloss: 0.261673] [G loss: 0.293981] time: 1:21:54.554614\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7975/12662], [Dloss: -0.178322] [G loss: 0.804423] time: 1:21:55.699552\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7976/12662], [Dloss: 0.194197] [G loss: 0.528801] time: 1:21:56.893359\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7977/12662], [Dloss: 0.342952] [G loss: 0.276698] time: 1:21:58.022340\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7978/12662], [Dloss: 0.073418] [G loss: 0.095193] time: 1:21:59.140350\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7979/12662], [Dloss: 0.667676] [G loss: -0.292406] time: 1:22:00.317203\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7980/12662], [Dloss: 0.466363] [G loss: -0.144128] time: 1:22:01.484082\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7981/12662], [Dloss: 0.464431] [G loss: -0.113770] time: 1:22:02.667916\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7982/12662], [Dloss: 0.176951] [G loss: -0.003021] time: 1:22:03.783931\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7983/12662], [Dloss: 0.447107] [G loss: -0.144638] time: 1:22:04.987712\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7984/12662], [Dloss: 0.118016] [G loss: 0.297453] time: 1:22:06.197477\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7985/12662], [Dloss: 0.150906] [G loss: 0.383606] time: 1:22:07.405247\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7986/12662], [Dloss: 0.244721] [G loss: 0.319705] time: 1:22:08.632964\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7987/12662], [Dloss: 0.174291] [G loss: 0.450132] time: 1:22:09.806824\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7988/12662], [Dloss: 0.194902] [G loss: 0.449861] time: 1:22:11.034541\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7989/12662], [Dloss: -0.026220] [G loss: 0.674644] time: 1:22:12.251287\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7990/12662], [Dloss: 0.162121] [G loss: 0.878878] time: 1:22:13.368299\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7991/12662], [Dloss: 0.484018] [G loss: 0.058132] time: 1:22:14.537173\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7992/12662], [Dloss: 0.038454] [G loss: 0.208804] time: 1:22:15.738959\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7993/12662], [Dloss: 0.670515] [G loss: -0.115454] time: 1:22:16.893871\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7994/12662], [Dloss: 0.382855] [G loss: 0.207601] time: 1:22:18.091667\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7995/12662], [Dloss: 0.643757] [G loss: 0.400429] time: 1:22:19.213667\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7996/12662], [Dloss: 0.265923] [G loss: 0.206885] time: 1:22:20.422434\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7997/12662], [Dloss: 0.425793] [G loss: -0.167013] time: 1:22:21.580338\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7998/12662], [Dloss: 0.177874] [G loss: -0.053239] time: 1:22:22.703334\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 7999/12662], [Dloss: 0.535709] [G loss: -0.006383] time: 1:22:23.857248\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8000/12662], [Dloss: 0.368488] [G loss: 0.064529] time: 1:22:25.046069\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8000/12662], [Dloss: 0.368488] [G loss: 0.064529] time: 1:22:25.046069\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8001/12662], [Dloss: 0.186763] [G loss: 0.331912] time: 1:22:29.469324\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8002/12662], [Dloss: 0.391096] [G loss: 0.213427] time: 1:22:30.698038\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8003/12662], [Dloss: 0.028420] [G loss: 0.563294] time: 1:22:31.953680\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8004/12662], [Dloss: 0.363558] [G loss: 0.065760] time: 1:22:33.165439\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8005/12662], [Dloss: 0.613180] [G loss: 0.069524] time: 1:22:34.325347\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8006/12662], [Dloss: 0.618682] [G loss: 0.063705] time: 1:22:35.506189\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8007/12662], [Dloss: 0.689013] [G loss: -0.193842] time: 1:22:36.749863\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8008/12662], [Dloss: -0.105944] [G loss: -0.113054] time: 1:22:37.923724\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8009/12662], [Dloss: 0.062727] [G loss: 0.275516] time: 1:22:39.127504\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8010/12662], [Dloss: 0.477002] [G loss: 0.161795] time: 1:22:40.337269\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6/11], [batch 8011/12662], [Dloss: 0.203490] [G loss: 0.159177] time: 1:22:41.538068\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8012/12662], [Dloss: 0.227520] [G loss: 0.411917] time: 1:22:42.731923\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8013/12662], [Dloss: 0.288433] [G loss: 0.057253] time: 1:22:43.926245\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8014/12662], [Dloss: 0.347894] [G loss: 0.100944] time: 1:22:45.204000\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8015/12662], [Dloss: 0.431689] [G loss: 0.133815] time: 1:22:46.347940\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8016/12662], [Dloss: 0.383485] [G loss: 0.120737] time: 1:22:47.592611\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8017/12662], [Dloss: 0.314832] [G loss: -0.003706] time: 1:22:48.701645\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8018/12662], [Dloss: 0.552809] [G loss: 0.164344] time: 1:22:49.858551\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8019/12662], [Dloss: 0.248470] [G loss: 0.189416] time: 1:22:51.026428\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8020/12662], [Dloss: 0.189980] [G loss: 0.329968] time: 1:22:52.208771\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8021/12662], [Dloss: -0.141200] [G loss: 0.445530] time: 1:22:53.392121\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8022/12662], [Dloss: 0.210081] [G loss: 0.182697] time: 1:22:54.535568\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8023/12662], [Dloss: 0.153442] [G loss: 0.438662] time: 1:22:55.730404\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8024/12662], [Dloss: 0.345113] [G loss: 0.417136] time: 1:22:56.895289\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8025/12662], [Dloss: 0.264885] [G loss: -0.136796] time: 1:22:58.027262\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8026/12662], [Dloss: 0.343517] [G loss: 0.030645] time: 1:22:59.195138\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8027/12662], [Dloss: 0.300594] [G loss: 0.269908] time: 1:23:00.397438\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8028/12662], [Dloss: 0.103480] [G loss: 0.518512] time: 1:23:01.539887\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8029/12662], [Dloss: -0.059653] [G loss: 0.683059] time: 1:23:02.723721\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8030/12662], [Dloss: 0.279874] [G loss: 0.203397] time: 1:23:03.891105\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8031/12662], [Dloss: 0.248067] [G loss: -0.219920] time: 1:23:05.140270\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8032/12662], [Dloss: 0.525407] [G loss: -0.389669] time: 1:23:06.363000\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8033/12662], [Dloss: 0.379188] [G loss: -0.084776] time: 1:23:07.530877\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8034/12662], [Dloss: 0.368933] [G loss: 0.371566] time: 1:23:08.702743\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8035/12662], [Dloss: 0.431780] [G loss: 0.632734] time: 1:23:09.870619\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8036/12662], [Dloss: 0.383910] [G loss: 0.763304] time: 1:23:11.126261\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8037/12662], [Dloss: 0.130206] [G loss: 0.821383] time: 1:23:12.338020\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8038/12662], [Dloss: 0.121334] [G loss: 1.024943] time: 1:23:13.503903\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8039/12662], [Dloss: -0.017317] [G loss: 0.659097] time: 1:23:14.606952\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8040/12662], [Dloss: 0.087566] [G loss: 0.899168] time: 1:23:15.820707\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8041/12662], [Dloss: -0.018154] [G loss: 0.708481] time: 1:23:16.971629\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8042/12662], [Dloss: 0.553877] [G loss: 0.416863] time: 1:23:18.275143\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8043/12662], [Dloss: 0.596220] [G loss: 0.286941] time: 1:23:19.629521\n",
      "current learning rate:0.000045\n",
      "[epoch 6/11], [batch 8044/12662], [Dloss: -0.215326] [G loss: 0.251728] time: 1:23:20.928059\n",
      "current learning rate:0.000045\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-82c0944cda5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mstargan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStarGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontinue_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinear_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#resume_epoch & resume_batch: set in case of resume of the training stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mstargan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresume_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresume_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-ad01b07a8ee9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, batch_size, sample_interval, resume_epoch, resume_batch)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m                 \u001b[0mimgs_reconstruct\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimgs_generated\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0morigin_label\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m                 \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mG_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0morigin_label\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                 \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1449\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1450\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':    \n",
    "    #set continue_training to True to load the weights,otherwise it will trains from scratch\n",
    "    stargan = StarGAN(continue_training=True,linear_decay=True)\n",
    "    #resume_epoch & resume_batch: set in case of resume of the training stopped\n",
    "    stargan.train(epochs=11,resume_epoch=6,resume_batch=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4,10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - 40 labels\n",
    "\n",
    "# 5_o_Clock_Shadow  Arched_Eyebrows Attractive      Bags_Under_Eyes    Bald\n",
    "# Bangs             Big_Lips        Big_Nose        Black_Hair         Blond_Hair\n",
    "# Blurry            Brown_Hair      Bushy_Eyebrows  Chubby             Double_Chin\n",
    "# Eyeglasses        Goatee          Gray_Hair       Heavy_Makeup       High_Cheekbones\n",
    "# Male              Mouth_Slightly_Open Mustache    Narrow_Eyes        No_Beard \n",
    "# Oval_Face         Pale_Skin       Pointy_Nose     Receding_Hairline  Rosy_Cheeks\n",
    "# Sideburns         Smiling         Straight_Hair   Wavy_Hair          Wearing_Earrings\n",
    "# Wearing_Hat       Wearing_Lipstick Wearing_Necklace Wearing_Necktie Young "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_labels.columns.get_loc('Young')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1] blond\n",
    "# [0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1] blond\n",
    "\n",
    "# [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0] eyeglass\n",
    "\n",
    "# [1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0] mustache\n",
    "\n",
    "#[0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1] smile\n",
    "\n",
    "# [0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1] young"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blondy: 476  [0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1]\n",
    "#glass: 676   [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0]\n",
    "#mustache:689 [1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1]\n",
    "#smile:718    [0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1]\n",
    "# young :459  [0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
